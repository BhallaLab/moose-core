06 July 2008

This document deals with the parallel organization of MOOSE. There are
several aspects to this:

- Multithreaded vs. MPI based parallelization
- Managing objects across nodes: See the IDS chapter
- Sending commands across nodes.
- Sending message data across nodes.
- Setting up messages across nodes.
- Scheduling data transfer.
- Moving objects between nodes.
- Load balancing and monitoring traffic

............................................................................
- Multithreaded vs. MPI based parallelization

Multithreaded parallelization is currently the purview of solvers. For 
example, the kinetic solver has one key line where calculation of rate updates
could be done in a multithreaded way. Key issues:
	- Specifying environment to solver: # of threads to use
In principle I could to extensive message traversal to work out multithreading
of regular messages. This is probably not so valuable right now. In due course
(>4 CPUs, many neurons per node) it will be necessary to do some more 
intelligent multithreading, possibly by putting several solvers to work at
once. 

MPI parallelization
This is slow and expensive in terms of memory, but can scale well
and is pretty general. See below. 

The plan is to have MPI code handle internode communications for now, and
use multithreading as an option for solvers because they need very compact
thread operations. For example, in the kinetic solver there is a very tight
inner loop which could be separated into multiple threads. For the 
neuronal solver the channels could be farmed out.

Postmasters:

............................................................................
- Sending commands across nodes.
At startup, the shells set up async messages to each other, across nodes.
We need pre-simulation scheduling set up to let them send info around.

............................................................................
- Sending message data across nodes.

General idea: Each src object has a proxy on the remote node. 
This proxy handles messaging on the remote node.
Each node is represented by a postmaster, on each of the other nodes.
The src postmaster organizes data transfer from src to dest node.
The dest postmaster organizes data transfer from buffer to proxy.

There are two kinds of messages: 
Async messages: go at unpredictable times and may have unpredictable sizes.
	Examples: Synaptic messages, shell commands.
Sync messages: Known schedule, known order, known size.
	Examples: Diffusion messages.

............................................................................
Async messages: Use a synapse as a prototype as it is the most exacting
and common situation.

At src:
As far as the sending element is concerned, nothing changes. It uses the same
Send function as always. 
- The RecvFunc now is a templated variant of 
srcType->asyncFuncId(), and is provided by the AsyncDestFinfo on the
PostMaster in respondToAdd(). The asyncFunc takes the arguments, serializes 
them, and
stuffs them into the buffer location provided by PostMaster::getAsyncParBuf.
- getAsyncParBuf manages data buffer positioning, and stuffs additional info 
from the Conn into the AsyncStruct which is placed at the start of each 
async data packet.
This additional info is
	- Id of the source element: Used to look up destination proxy
	- Message size.

Once send buffer is full, the postmaster does its send. Usual juggling of
sends, irecvs and scheduling so that local operations are done while waiting.
Note that we can schedule certain categories of message to go less often,
such as spike activity and possibly script calls.

At dest: 
- On Postmaster: March through recv buffer. This is done 'blind', not knowing
	contents of data, but using the AsyncStruct info for size of data
	packet to advance buffer.
	- Call proxy2tgt( AsyncStruct&, char* data );
		- Looks up proxy element from Id stored in AsyncStruct.
		- Calls ProxyElement::sendData( data )
			- Uses ProxyFunc stored on the ProxyElement to 
			convert data and send out message to final targets.

Proxy: Looked up by Id of src. This has to come through buffer. This is just
	a subclass of Element. I will need a default simple version that
	handles just a single Msg. This should do for almost all cases.
	Later I'll need a more expensive/complete version for cases
	where there are multiple messages. Fields of ProxyElement class:
	- Id (from base class)
	- Msg entry
	- unsigned int node_: Place where original of Proxy really lives.
	- ProxyFunc: Function that converts and sends out data.
	
Handling array msgs
	- Single proxy Element for entire array, as usual.
		- Proxy may represent only part of a total array. Hm.
	- The incoming msg has src eIndex as part of src Id.
	- Usual send function.
Handling reverse traversal
	- Use regular message traversal code to get to proxy
	- Proxy will provide suitable virtual functions to refer all
		questions back to original element via postmaster.

.........................................................................
This looks good so far. Now let's see how this would work with sync msgs.

At src:
- Use Conn idx to position data into buffer
- RecvFunc for this comes from the Ftype, just a matter of dumping data values.

Every clock tick (see parallel scheduling) the postmaster does its send.

At dest: March through already set-up vector of ParSyncMsgInfo.
- Each ParSyncMsgFinfo has the following fields:
	- TransferFunc: Extracted from Ftype of data. Used to take buffer data
		and call proxy 'send'
	- vector< Eref > of the proxies. Should it be vector< Id > ?
		- Cannot be vector< Element* >, because of eIndices.

This is pretty streamlined, but setup will be more involved.

==============================================================================
Shared messages in parallel. Shell messages are async, axial are sync, so
we need to deal with both cases.
Option 1: Do all of the above both ways. Entails a proxy at either end.
	More symmetric perhaps but doesn't work like shared messages at the
	local level, because src->srcpost->destpost->proxy->dest and then
	dest->destpost->srcpost->proxy->src are not mirror images. Better to
	retain full compatibility with shared message behaviour.
Option 2: Have proxy at both ends, but pass through it on the sending as
	well as receiving node. Proxy element has to have tools to handle
	return functions. It will 
	still use the same Msg data struct, but will now forward operations
	to the postmaster. Would need a proxy at both ends of the
	message to acheive symmetry:
	src<->srcproxy<->srcpost<->destpost<->destproxy<->dest

==============================================================================

Setting up messages across nodes.

1. Identifying sync and async messages
This should be known to the SrcFinfo. By default msgs should be sync.

2. Setting up async messages
- At src:
	- Check if this msg already goes to postmaster. If not:
		- Make local msg to postmaster.
	- Send/accumulate msg request info:
		src Id
		dest Id
		dest/src funcIndex so we can figure out type info.
		dest msgIndex
- At dest:
	This is done by PostMaster::setupProxyMsg( srcNode, proxyId, destId, 			destMsg, post )
	- Create proxy element with specified msg type. If proxy exists, use it.
		Should be easy to using lookup of ElementList by Id.
	- Figure out ProxyFunc from destMsg.
	- Send message from proxy to true target[s]

3. Setting up sync messages.
- At src:
	- Have a map< const Ftype*, num > to build up conn indexing into buffer,
		one message at a time.
	- Check if message already exists to postmaster. If not:
		- Make local msg to postmaster.
	- Send/accumulate msg request info:
		src Id
		dest Id
		dest/src funcIndex so we can figure out type info.
		dest msgIndex

- At dest:
	- Have a map< const Ftype*, ParSyncMsgInfoIndex > to keep track
		of how incoming msg requests map onto new or existing 
		ParSyncMsgInfos
	- Create/look for proxy element as needed.
	- Each msg request puts the proxy into the ParSyncMsgInfo.
	- Make local msg.



==============================================================================
