01 June 2008

This document deals with the parallel organization of MOOSE. There are
several aspects to this:

- Multithreaded vs. MPI based parallelization
- Managing objects across nodes: See the IDS chapter
- Sending commands across nodes.
- Sending message data across nodes.
- Setting up messages across nodes.
- Scheduling data transfer.
- Moving objects between nodes.
- Load balancing and monitoring traffic

............................................................................
- Multithreaded vs. MPI based parallelization

Multithreaded parallelization is currently the purview of solvers. For 
example, the kinetic solver has one key line where calculation of rate updates
could be done in a multithreaded way. Key issues:
	- Specifying environment to solver: # of threads to use
In principle I could to extensive message traversal to work out multithreading
of regular messages. This is probably not so valuable right now.

MPI parallelization
This is slow and expensive in terms of memory, but can scale well
and is pretty general. See below. If we keep the multithreaded and MPI
parallization code separate, we may have a good mix.
Postmasters:

............................................................................
- Sending commands across nodes.
At startup, the shells set up async messages to each other, across nodes.
We need pre-simulation scheduling set up to let them send info around.

............................................................................
- Sending message data across nodes.

General idea: Each src object has a proxy on the remote node. 
This proxy handles messaging on the remote node.
Each node is represented by a postmaster, on each of the other nodes.
The src postmaster organizes data transfer from src to dest node.
The dest postmaster organizes data transfer from buffer to proxy.

There are two kinds of messages: 
Async messages: go at unpredictable times and may have unpredictable sizes.
	Examples: Synaptic messages, shell commands.
Sync messages: Known schedule, known order, known size.
	Examples: Diffusion messages.

............................................................................
Async messages: Use a synapse as a prototype as it is the most exacting
and common situation.

At src:
Conn provides 4 pieces of info: to the destination RecvFunc on the PostMaster:
- data: e.g., the time stamp. This is passed in as the arg of the RecvFunc.
- Id of src: Passed in from Conn.
- Target message#: This identifies the target synapse Finfo. Passed from Conn.
- Source Message#: The Message# is a way to look up Ftype. Passed in from Conn.

This goes into a send buffer on the originating node PostMaster. This is just
filled up sequentially as the send requests come in. As we know the data
size, we can keep track of location in buffer.
Q: Would it speed up matters significantly to store size separately? Or send
it along with the data? Actually we need to be smarter here. How do we
send arrays and strings?
A: Use instead the recvFunc provided by the Ftype of the src message. This
templated operation fills the buffer and adds whatever other info is needed,
such as vector size.

Once send buffer is full, the postmaster does its send. Usual juggling of
sends, irecvs and scheduling so that local operations are done while waiting.
Note that we can schedule certain categories of message to go less often,
such as spike activity and possibly script calls.

At dest: March through recv buffer.
	- Look up proxy element from Id
	- Identify Ftype from tgt Message#
	- Ftype provides a func to call outgoing message from proxy, using
		data in buffer. Id::eIndex used to identify specific tgts.
	- Ftype provides increment to go on to next message in buffer.
	- For now, slot is always zero: there is only one outgoing msg.

Proxy: Looked up by Id of src. This has to come through buffer. This is just
	a subclass of Element. I will need a default simple version that
	handles just a single Msg. This should do for almost all cases.
	Later I'll need a more expensive/complete version for cases
	where there are multiple messages. Fields of ProxyElement class:
	Msg entry
	Single Finfo ptr.
	Id (from base class)
	postmaster for node where src actually lives.
	
Handling array msgs
	- Single proxy Element for entire array, as usual.
		- Proxy may represent only part of a total array. Hm.
	- The incoming msg has src eIndex as part of src Id.
	- Usual send function.
Handling reverse traversal
	- Use regular message traversal code to get to proxy
	- Proxy will provide suitable virtual functions to refer all
		questions back to original element via postmaster.

.........................................................................
This looks good so far. Now let's see how this would work with sync msgs.

At src:
- Use Conn idx to position data into buffer
- RecvFunc for this comes from the Ftype, just a matter of dumping data values.

Every clock tick (see parallel scheduling) the postmaster does its send.

At dest: March through already set-up vector of ParSyncMsgInfo.
- Each ParSyncMsgFinfo has the following fields:
	- TransferFunc: Extracted from Ftype of data. Used to take buffer data
		and call proxy 'send'
	- vector< Eref > of the proxies. Should it be vector< Id > ?
		- Cannot be vector< Element* >, because of eIndices.

This is pretty streamlined, but setup will be more involved.

==============================================================================
Shared messages in parallel. Shell messages are async, axial are sync, so
we need to deal with both cases.
Option 1: Do all of the above both ways. Entails a proxy at either end.
	More symmetric perhaps but doesn't work like shared messages at the
	local level, because src->srcpost->destpost->proxy->dest and then
	dest->destpost->srcpost->proxy->src are not mirror images. Better to
	retain full compatibility with shared message behaviour.
Option 2: Have proxy at both ends, but pass through it on the sending as
	well as receiving node. Proxy element has to have tools to handle
	return functions. It will 
	still use the same Msg data struct, but will now forward operations
	to the postmaster. Would need a proxy at both ends of the
	message to acheive symmetry:
	src<->srcproxy<->srcpost<->destpost<->destproxy<->dest

==============================================================================

Setting up messages across nodes.

1. Identifying sync and async messages
This should be known to the SrcFinfo. By default msgs should be sync.

2. Setting up async messages
- At src:
	- Check if this msg already goes to postmaster. If not:
		- Make local msg to postmaster.
	- Send/accumulate msg request info:
		src Id
		dest Id
		dest/src funcIndex so we can figure out type info.
		dest msgIndex
- At dest:
	- Create proxy element with specified msg type. If proxy exists, use it.
		Should be easy to using lookup of ElementList by Id.
	- Send message from proxy to true target[s]

3. Setting up sync messages.
- At src:
	- Have a map< const Ftype*, num > to build up conn indexing into buffer,
		one message at a time.
	- Check if message already exists to postmaster. If not:
		- Make local msg to postmaster.
	- Send/accumulate msg request info:
		src Id
		dest Id
		dest/src funcIndex so we can figure out type info.
		dest msgIndex

- At dest:
	- Have a map< const Ftype*, ParSyncMsgInfoIndex > to keep track
		of how incoming msg requests map onto new or existing 
		ParSyncMsgInfos
	- Create/look for proxy element as needed.
	- Each msg request puts the proxy into the ParSyncMsgInfo.
	- Make local msg.



==============================================================================
