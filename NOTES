12 Oct 2013.
This is the async13 branch. It is based on the current (buildQ) branch and
is designed to test out asynchronous messaging in MOOSE. This will drop
all the multithreading and queueing code framework with a view to simplifying
and speeding up calculations. It is also meant to implement parallelization
from the ground up, using either MPI or Charm++ messaging.
It is meant to keep the old buildQ API as much as possible.

Compiled the reduced code base, got most of the unit tests to work. 
Checkin 4792.

Fixed a dangling 'else' statement in the main.cpp that was preventing the
unit tests from closing. Now it is OK. Checkin 4793.

Next: eliminate threading. 
	much eliminating later, it compiles but doesn't clear unit tests.

Stuck in Shell::doReinit in the Qinfo::buildOn/buildOff around line 686
Eliminated all buildOn/buildOff calls. Now it croaks much sooner in the
unit tests.

===========================================================================
13 Oct 2013.
Further cleanup. Moved the process loop into Clock. Gets some way through
unit tests but fails in testShellAddMsg(), seems that between Clock::doReinit
and doStart the message info isn't going around.
Checkin 4794.

===========================================================================
14 Oct 2013

Clever trick to break a python script at a specified place to use gdb:

import os
import signal

PID = os.getpid()

def do_nothing(*args):
    pass

def foo():
    print "Initializing..."
    a=10
    os.kill(PID, signal.SIGUSR1)
    print "Variable value is %d" % (a)
    print "All done!"

signal.signal(signal.SIGUSR1, do_nothing)

foo()

From Stack overflow by Michael Hoffman in 2011.
===========================================================================
16 Oct 2013.
Cleaning up threading references from Clock. Still doesn't get through the
unit tests, some issue with doReinit crops up in testBuiltins.
Some progress.
Current status is that the requestData()->send call is invoked, but data doesn't
get back in time for the reinit to complete. Need to investigate how the 
send call is handled.

===========================================================================
17 Oct 2013
Some more cleanup, it finally clears all unit tests. Checkin 4796.
Valgrind is totally happy too.

Next items:
Make benchmark for IntFire network.
Message rebuild/eliminate Qinfo.
Scheduling rebuild
Id and data handler rebuild
Synapse rebuild
Conv elimination
Prepacked buffer elimination
Parallel rebuild

===========================================================================
20 Oct 2013

Design of key sections. 

Data and field access:
Elements manage data as well as node info. All data are arrays. 
Msgs managed too. No FieldElements. No DataHandlers. Cinfo deals with Dinfo.
class Element {
	private:
		string name_;
		Id id_;
		const Cinfo* cinfo_;
		char* data_;
		unsigned int numData_;
		vector< indexRange > indexRange_; // Looks up indices by node.
		vector< MsgId > m_;
		vector< MsgTraverse > mt_;
};

Synapses are an array of fields on a Receptor. Not FieldElements.

Ids specify element and index of object entry.

Msgs specify Id and field index to fully specify Obj and field. Msg ops call 
back to Element, possibly Object.  For fast traversal, Msgs are digested into 
{funcPtr, Element ptr} sets with { Obj ptr, field} arrays. Each Obj index looks
up one such set of arrays, which can be shared for different funcs.

Scheduling: Tick 0 is base clock with a specified dt. All others are integral
multiples of it. Order within a given timestep is by tick#. No fieldElements,
just 10 MsgSrcs emanating from Clock, and internal array for the Multiples.

Conv: Used only to serialize off-node msgs, and to convert to and from strings,
and to provide rttiType to MOOSE.

Eref {
	Element* e_;
	DataId i_;	// An unsigned int for now
	// An unsigned int, used to pass field index but does not affect data 
	// lookup. Instead field index is extracted by the target EpFunc.
	FieldIndex f_;	
	char* data();
};

Prepacked buffer goes away

Parallel calls: At time of Msg digest, converted and condensed to single call
	per msg per node for outgoing. Likewise incoming msgs expanded out
	as if from original src Element, to all tgts on recipent node.
	Outgoing bufsend right away.
	Incoming polled at end of Tick by src element proxy.
	Transfer of objects by serialization + rebuild of all element node info
	+ rebuild of digested msgs.

I think that the Element and Msg stuff are pretty intertwined. But lets start
with the Msg and see how far we can go.

............................................................................
Alternative to this set is to have Synapses and other FieldElements
as independent Elements, using pointers cleverly to refer back to parent.
Possible disadvantage is some juggling to refer back to parent quickly.
Advantage is to eliminate FieldIndex.
Disadvantage is to proliferate Elements, many proxy Elements on all nodes.

For now trying fieldIndex for synapses.
===========================================================================
20 Oct 2013.
Begun massive refactoring of Msgs. To start with, redid the OpFuncs, 
EpFuncs and SetGet headers. Threw out UpFuncs and FieldElement stuff.
Now the headers seem consistent, but SrcFinfo::send calls need to be 
populated, which can only happen after I redo Element.

Compilation begun, far more to do. Checkin 4801.

===========================================================================
21 Oct 2013.
Marching through refactoring. Messaging there in skeleton but I'm not
satisfied. I would like a generic way to handle the following cases:
Traverse through single msgs, each with a unique Eref.
	This is what I've implemented for now.
Traverse through synapses, same Element but many indices and fieldIndices
Traverse through obj arrays: same Element, many indices, all fieldIndices = 0.
Traverse through 'all' obj: same element, auto through indices.

Won't try to do this through all field indices. Unclear where that would be
relevant.

I've eliminated the FieldElements and DataHandlers. This caused some
issues with the Cinfo element instances, which used to have FieldElements
for the SrcFinfos, destFinfos and so on.
I've hacked around it by explicitly creating Finfo elements as children of the
Cinfo elements. It compiles but looks precarious. Need to fix up Finfo
field access.
Checkin 4802.

===========================================================================
22 Oct 2013
Tediously slogging through compiling testAsync.cpp. Done, but the many
tests with IntFire and synapses are not so clean and will need some thought
on how better to set up this interface.
===========================================================================
25 Oct 2013

Back to Element lookup options. 

With just an integer data entry lookup:
- Synapses would need to be FieldElements, one per receptor
- Copies of neurons either to a new element tree, or arrayed in dataId.
- Need to be able to create child Elements on every data entry.
- Msg cleaner
- 

With data entry plus field entry lookup:
- Synapses could be to be FieldElements, one per receptor, or tables.
- Copies of neurons either to a new element tree, or arrayed in dataId.
- Do not need to be able to create child Elements on every data entry, but handy
- Msg has extra field.

===========================================================================
28 Oct

Some memory estimates.

Integer data lookup:
- Elements
	Suppose I have 1e6 neurons and 1e10 synapses.
	Assume I have 100 synapses per receptor, this is still about 1e8 synapse
	Elements. Somewhat strenuous if I attempt to put this all on each node.
- Msgs
	In sparse Msg form, it is essentially 2 longs per synapse, but only
	for synapses on local node. The higher level Msg would be negligibly 
	small.
- Synapse data
	2 doubles per synapse on the node. A modest circular buffer for the 
	queue.

Field entry lookup.
- Elements
	Here I would have 1 'synapse' per receptor on the parent neuron. A few
	hundred at best. Rest is just a matter of looking up indices.
	I could put the relevant Elements on each node easily.
- Msgs
	Same as above. 3 longs per synapse makes 
- Synapse data
	2 doubles per synapse on the node. A modest circular buffer for the 
	queue.


So it is pretty clear that the integer data lookup would require a different
way to set up messaging, one where the skeleton simulation cannot fit on one
node.

Note that I have to do Data entry mapping to nodes in either case.
Let's see what that looks like.

===========================================================================
1 Nov 2013
Data entry mapping.
Goals: 
	From any node, find quickly which node holds data.
		This doesn't have to be real-time: not used for sending msgs,
		but used for setup.
	Keep it compact. Can't have a full index of entire data list.

Design: 
	Element Ids: On all nodes
	Data Ids: Decomposed between nodes
	Field indices: Only on one node, that of the parent Data Id.

Use cases:
	- Neuronal model decomposed. Small # of neurons on each node, many 
	nodes, some degree of spatial organization helps, load balancing may 
	occur.  Little data transfer due to spiking.
	- 3D space decomposed in rdesigneur. Unpleasant large matrix, simple
	reacs but lots of diffusion. Heavy data transfer, each timestep for
	matrix solution. Very spatially organized, unlikely to do funny 
	balancing.
	- rdesigneur cell models: Each model on its own, doesn't need to talk
	to others except via spiking.
	- Spiking neuron model decomposed. Huge # of neurons and synapses on
	each node, many nodes, some spatial orgn. Load balancing may not be
	so useful. Lots of data transfer.
	- Large cluster vs small cluster/cores on a machine.

// Returns blocks
getNode( unsigned int numNodes, unsigned int numData, unsigned int dataId ) {
	return dataId * numNodes / numData;
}
// Returns nicely arrayed.
getNode( unsigned int numNodes, unsigned int numData, unsigned int dataId ) {
	return dataId % numNodes;
}
Now, what if load balancing has happened? Add check for that.
getNode( unsigned int numNodes, unsigned int numData, unsigned int dataId ) {
	if ( auto i = dataMap.find( dataId ) ) {
		return i->second;
	}
	return oldNodeMethod( numNodes, numData, dataId );
}
// May also have a situation where the data is on every node: a global.
getNode()
{
	return GlobalNode
}

So we just need a virtual class or function for this to sit on every Element.
Quite easy. Eventually will want to have a way to garbage collect if lots of
object movement has happened. Could even have a policy switch field, ugly though
it may seem.

Other thing is to put back the FieldElements, but have them point to the parent
data_.

Checkin 4818.
For Msgs, since the MsgId is no longer needed for fast msg sending, I'll just
have the MsgId be a regular ObjId pointing to the message. 

Should also add callbacks to Elements for add and delete messages.  These
are done as extra args in the Cinfo constructor, typically zero. 
The callback functions pass in Element and Msg target info.

Created FieldElements. Nothing there yet for FieldElementFinfos. Not yet
compiled through SparseMsg.cpp.

===========================================================================
2 Nov 2013
Grinding through compile. ElementValueFinfo and similar things need sorting.

===========================================================================
3 Nov 2013
Grinding through compile. Cleared Shell.cpp, but many things to do with path
are postponed.
I have completely redone the scheduling. Now the Clock does it all, and
much more simply and possibly faster. Eliminated Tick related objects.
Now compiles through the scheduling directory, but tests only as placeholders.
Now compiles through the builtins directory. 
Now a bit stuck with the Synapse and FieldElement, which I have sidestepped
so far. 

Desiderata: 
- Very fast Synapse handling. This requires that both the parent object
	and the synapse entry are rapidly accessed.
	I had thought I would do this by returning the base data entry, with
	the index from eref and lookup func for the Field from EpFunc.
	Needs an extra lookup compared to the regular Msg because we need to
		engage the parent class as well as the specific Synapse.
	addSpike( double time ) {
		ringBuffer->addEvent( time + delay_, weight_ );
	}
	This requires the Synapse to a) have a pointer to the ring buffer,
		and b) the parent class have a ring buffer in it to use.

	A more complicated one might also look at association with postsyn Vm,
	but that would be computed at time of arrival, handled by the Receptor.
	If we do this by passing pointers in from the UpFunc we could refer
	to the Receptor level:
	Receptor::addSpike( const Eref& e, double time ) {
		const Synapse& syn = synapses_[e.fieldIndex()];
		ringBuffer.addEvent( time + syn.delay(), syn.weight() );
	}
	This requires that there be a common base class for the Receptors that
	all Synapses could use.
	Or we could refer to the Synapse level:
	Synapse::addSpike( const Eref& e, double time ) {
		auto rec = reinterpret_cast< Receptor* >( e.plainData() )
		rec->ringBuffer.addEvent( time + delay_, weight_ );
	}
	This too requires a common base class for the Receptors.

- Easy access to the Field, preferably deal directly with its pointer.
	This is certainly doable with the lookup, and even more so with
	the precomputed Msg ptrs, but then how to handle parent?
	In cases where the parent isn't needed, we could design an EpFunc
	to directly call the Field.
	The first option above would do this directly.

The first option is nicest, also fits well with the idea of single precomputed
target pointers for messages. Costs an extra pointer in each Synapse. Minor.
This implements very smoothly, but there is another consideration. 
The synapse class needs to provide a callback so that when additional spike 
messages come in, the synapse buffer is expanded to match.
- Callback detects message addition and removal
- Callback would modify # of synapses.
- Callback may assign a fieldIndex to the message itself.

This is fine, but in the Synapse we have a problem because the callback
needs to talk to the parent SynHandler if it wants to resize the Synapse array.
Easily handled by looking up the parent ObjId of the Synapse.

In the process of setting up the SynHandler. Next:
+Fix up Msgs to do the precompiled fast lookup
+Fix up FieldElement stuff to access synapses from Eref
Fix up FieldElement and Element stuff to deal with callbacks
Fix up Msgs to use ObjIds to identify
+Fix up Msgs to handle the specification of fieldIndex.
Work out how to talk to nodes on parallel machines. Socket?
Work out how to do SetGet on parallel machines
Work out parallel messaging.

===========================================================================
4 Nov 2013.
Fixing up testAsync.cpp:testSetGetSynapse, now that I've restructured
the FieldElement stuff.

May need to go back and redefine ObjId so it specifies the fieldIndex too, 
just like the Eref.

Getting close to compilation completing. Had to go back to TableBase.

Stuck a bit in Table.cpp, figure out what to do with recvDataBuf.
===========================================================================
5 Nov 2013
Finally cleared first pass of compilation, link still fails with lots of 
unresolved functions. Checkin 4824.
Finally compiles. Doesn't run.

Many nasty bugs later, I have now to deal with the MsgDigest for fast 
messaging. In testAsync::testSendMsg.
===========================================================================
6 Nov 2013
The 'send' command has now taken shape with the MsgDigest. It is far from
as efficient as envisioned:

1. SrcFinfo->getBindIndex()
2. Compute offset of specific DataId * specific src func.
3. Look up MsgDigest vector from Eref.
4. Iterate through MsgDigest
5. Lookup function
6. Iterate through targets
7. Check if they are ALLDATA
	8a: Make temp eref
	8b: lookup numData
	8c: Create reference k
	8d: Iterate over ALLDATA
	8e: call function
else
	8a. Call function.

Here are some cases:
				# ops
Single object target:		8 + F
N unique object targets		5 + 3N + NF
Single Alldata target 
	with X entries		10+2X + XF
N Alldata target 
	with X entries each	5 + 5N + 2X + NXF

F is cost of function.

Immediate goal: Have the Msg do its own digest... No, need to store
this in Element because there would be a number of targets in each digest.

Now setting up Msg::sources and Msg::targets() to build digest.
===========================================================================
7 Nov.
Seems like some messages are beginning to go...
Now about 10 unit tests in. 

Stuck in SparseMsg.cpp:211. At this point there are no fields allocated,
so we're not finding any targets.
===========================================================================
8 Nov.
Steady progress. Some things to consider:
- Should we retain ext fields? Python deals with this kind of use mostly.
	If not we could simplify the function for checkSetGet.
- Need to redo the Msg as object tests in the unit tests

Now clears testAsync.
Fails in testMsg.cpp, but both those tests require some major pending updates:
to the ObjIds handling Msg, and for automatic updating of MsgDigest.

So commented them out.

Implemented the ObjId based Element tree structure. This lets individual
DataEntries have their own child Elements.
Yet to compile and the check is in testShell.cpp:121

===========================================================================
9 Nov.
A design and semantics issue with the capability to have each ObjId be parent
to a sub-tree: What happens when we replicate the entire tree?

Suppose, on a/b[100], we create a unique element child c[50] on 
	a/b[23]

Now we use createmap or equivalent to make 10 copies of a. This does:
	a[10]/b[1000]
What about c in this? My reading is that we should get

	a[1..10]/b[23,123,223,323,423,523,623,723,823,923]/c

where in each case c is a separate Element. 
The reason is that we do not permit a given Element to be parented by many
different objects. One could in principle do even this through sparse msgs,
but I think the semantics get to be too untidy.
Other aspect of the semantics is that the parent of any entry in b would be 
a[0]. So there is a special status given to those Elements parented off zero.

Yet another point: if I have a FieldElement, its parent dataIndex should be
identical to its own dataIndex. Implemented.

Now the Neutral::path is working well. Also went through and cleaned out
all uses of Id::operator()() since they were confusing.

Further progress, now clears testScheduling.

A bit stuck on destroying Elements. Currently cinfo_ is set to zero to tell
the Elements that they shoult NOT remove the msgs. cinfo_ isn't a good way
to do this because it is needed later. So we could set the id_ to zero.
But I may need to bifurcate the Element class so that the base class of 
FieldElement doesn't try to destroy the data.

Tried it. It was rather elegant: all the messaging stuff went into the
ElementBase, and the data stuff into Element - just like the FieldElement
was just doing data. But all the rest of the infrastructure refers to
Element and assumes that FieldElement is derived from it.

One way out might be to retain the split, but have the base class be Element,
and have a DataElement as counterpart to FieldElement.

Create functions would refer though to the DataElement and the FieldElement.
Lots of that.
I suspect that the DataElement will need considerable expansion with the
new multinode code. May as well anticipate.

So I redid this with the base class as Element, and DataElement that which
handles regular object arrays as data. FieldElements stay as is, now
quite symmetrical with DataElements.
Compiles and gets a little further with unit tests.

===========================================================================
10 Nov 2013.
Need to sort out semantics of copying with the provided multiplier.

consider: /a[10]/b[20]
copy /a /z
The bare copy should traverse the tree and produce an identical tree:
	/z/a[10]/b[20]

copy /a /z 5
Should I reduce a to 5, or make 5 copies of the whole thing? Or just 
forbid the multiple copy operation and reserve it for createmap type calls?

Clearly any deep copy should preserve the child indexing. So I cannot
do any reductions, will have to copy integral multiples of the whole tree.
Should I only allow unit copies? No, keep the feature, and use it for
more complicated createmap type calls.

Another case. I may need to do a specific object copy:

Copy a[3] /z
Here the rest of the tree is ignored unless the there is a tree rooted on a[3].
This precludes doing a copy of a[0] unless I implement a special parental
message that goes from the element as a whole. But even that would return an
index if queried.
Don't worry about this for now.

Did substantial cleanup on path handling.
Now completes testShell.

Trying to get clock to run a simulation in testBuiltins. The reinit::send
command fails because it doesn't find anything in the MsgDigest.

Ran into trouble with requestData calls. These were handled specially, 
and the new framework doesn't like them. There is the opportunity to do them
really simply, for example, pass the function a reference to be filled in
by the target 'get' function. Backward compatibility means that I can't now
change all the A get() functions to get( A& ), that would have been the 
simplest.
I've implemented a skeleton of a very simple GetOpFunc which just takes
a pointer as an argument, and dumps the value into the pointer. Problem
comes if I want to call such a function across nodes. Will everything
grind to a halt while I wait for the request to go and the response to come?
What if there is a deadlock: two nodes waiting on each others' returns?
Will have to set up the off-node 'get' call as a loop polling for the
answer, and updating any calls that the off-nodes make.

In the meantime, carried on with debugging, and now it clears unit tests but
with a segv at the end. Checkin 4837.

Much work with valgrind later, now it is completely clean. No segv either.
Checkin 4838.

List from 3 Nov, updated here:
*Eliminate multithreading
*Change to integral ticks for scheduling.
*Fix up Msgs to do the precompiled fast lookup
*Fix up FieldElement stuff to access synapses from Eref
Fix up FieldElement and Element stuff to deal with callbacks
*Fix up Msgs to use ObjIds to identify
*Elements should know when to call the MsgDigest.
*Fix up Msgs to handle the specification of fieldIndex.
Work out how to talk to nodes on parallel machines. Socket?
Work out how to do SetGet on parallel machines
Work out parallel messaging.


Starting on the ObjId identification of Msgs.
Did a bit in OneToOneMsg.cpp and OneToAll.h

===========================================================================
11 Nov 2013.
Callbacks
	- Message changes: add, drop, reconfigure: need message id.
	- Scheduling/clockdt changes
	- Changes in node decomposition (?)


Eliminated MsgIds, replaced with ObjIds. The change makes it much easier to
handle Msgs. Compiled but doesn't clear any tests yet. Checkin 4839.

Discrepancy in ptr looked up from mid to created ptr.

Fixed. Then much messing around trying to get the system to quit cleanly.
Eventually did by brute force, rather than the recursive removal of objects,
since the removal of the objects handling messages caused problems with
subsequent removals.
Something like an anti-bootstrapping.
Now terminates cleanly and valgrind is happy. Checkin 4840.
Some tests to do with this before going on to automatically deciding when to
do digestMessages.
Restored a few unit tests for the various msg classes. Not too bad.

For: Elements should know when to call the MsgDigest.
I'll have it so that the Msgs call each of their target Elements to set
a 'dirty' flag when the Msgs change. This flag is checked at the time of
reinit or process, but no. Should not be handled by the Object.
Options:
	- put in a conditional in every Send. In fact the act of 
	getting the MsgDigest vector could do this. e.msgDigest.

Starting with an 'isDirty' flag for messages that change. Need to fill in.
===========================================================================
12 Nov 2013.
Implemented the isDirty flag, but called it isRewired. 
Reactivated some of the unit tests on message field assignment. All cleared.

Considering the callbacks.
- Bypass altogether. Let the user track changes and ensure that the right
	number of synapses are defined. Possibly give a helper function.
	Possibly help with offsets for each incoming SparseMsg so one does
	not lose all assigned Weights/delays of one when the other changes.
- Give FieldElements an automatic scan of incoming msgs to check for field
	size. Activate this whenever the fields are accessed.
- Put in a function in SparseMsg::randomConnect and its
	ilk, to assign synapse::numField.
	Problem is that there may be other messages. Could cobble something
	together for the SparseMsg objects, but there are other possible Msg
	types which should not need to know this stuff.
- There are prototype callback functions in Synapse: 
	addMsgCallback and dropMsgCallback. But the message involved would
	have to know what to do with the info about existing synapses.

Summary: Need to provide offsets for each Msg. Rest is up to user. Need use
	cases.

General things to fix:
	- Use new 'send' command for all the Srcfinfos.

For now, bypass altogether. Move on to multinode.
- Fill in send buffers:
	- Message digest consolidation
	- Arg conversion.
- Figure out best design for internode comms. 
- simple block mapping of DataId to rawIndex.
- Global objects and synchronization
- Communication between nodes for setup operations
- Communication between nodes for Set/Get operations
- Clock ticks between nodes.
- Messaging between nodes.

Working on standalone prototype for the MPI. Stuck.
Some useful tricks for debugging:

mpicxx -g proc2.cpp
mpirun -np 2 xterm -e gdb a.out

Possibly the problem is that the local node has not filled in any entry
in recvReq. Yes. Now compiles, but the run goes into an infinite loop.
===========================================================================
13 Nov 2013.
Incremental debugging, now looks close. proc2.cpp.
Works but it isn't clear to me why there is the present limit on the number
of nodes it can handle. I would have thought N^2 = numCalls would be the limit.
But it works for powers of 2 up to 128, which is as far as my laptop will
manage. Possibly I shouldn't have so many Irecvs dangling at a time.

Next to set up an AllToAll type transfer to see if that goes faster. Certainly
looks simpler.
Working in proc3.cpp

Then put in handling for sporadic big mesages.

Then do benchmarking on ghevar and on cluster.

===========================================================================
14 Nov 2013. proc3 works. The
time tests are all over the place but proc3 does seem a little slower than
the proc2. Perhaps I could do another version with non-blocking calls.

Looking at how regular msgs put stuff in send bufs for off-node delivery.
- Element::putFuncsInOrder: Check if tgt is off-node, put in a dummy
	OpFunc that stores MsgBindIndex. Tgt will come from eref in 
	SrcFinfo::send.
- Derivative of OpFuncBase that also stores msgBindIndex. Need virtual 
	constructor
	function from OpFuncBase so that the PutFuncsInOrder can synthesize it.
	- Derivative needs manage the convs and buffer
- Converter that takes arguments and bungs into provided double buffer.
	Conv.h needs to be stripped down and redone.
	static const T& buf2val( const double* buf, unsigned int& size )
	static const unsigned int val2buf( const T& val, double* buf );
- Collapse all same-node targets for the same func, into just one entry
	for any given node.  In Element::putTargetsInDigest
- Someone to manage the buffers. Even if they are global. Could have a
	special class, postmaster?
	- Presumably same object (postmaster) to send stuff to other 
	nodes on tick.
	- Same object to parse arrived buffers and send out.
	- Quite a bit more here.
- Need to set up the node map on Elements too.
- Decide on buf structure. Say 1 double for size + 1 double for originating
	ObjId, + double for bindIndex. Assorted doubles for arguments.
	Actually make a header structure and use that, however many doubles
	it takes.

Started with Conv.h



===========================================================================
16 Nov 2013.
Filling in blanks for multinode stuff.
OpFuncBase now has something
DataElement now knows more about node handling.
Need to put buffers somewhere standard: postmaster?
postmaster needs to be elaborated.

- Global objects need to have assignments and sends go to all nodes.
	Actually just define a special DataElement subclass.

PostMaster::clearPending:
	Goes through recvBuf. 
		calls SrcFinfo::sendBuffer with buffer
			SrcFinfo converts to arguments
			Sends to all targets on specified bindIndex
			(which is a function of this SrcFinfo)
			Originating Eref is fully specified so
			there is no ambiguity about which subset of
			msgs srcs were involved.
			The MsgDigest must digest off-node stuff into
			same array.


 Here is an outline of how messages go between nodes, from PostMaster.h
			.................
This is how internode message passing works. I'll describe this at two
levels: the movement of data, and then the setup.

Level 1: Movement of data.
1. Object Sender sends out an argument A in a regular message.
2. The SrcFinfo scans through targets in the MsgDigest via func,tgt pairs
3. The off-node (Func,tgt-Eref) pair holds a HopFunc and a special
     Eref. The main part of the Eref is the originating object. The
     FieldIndex of the Eref is the target node.
4. The HopFunc fills in the Send buffer of the postmaster. It uses
     the target node info, stuffs in the ObjId of the originating object,
     and converts and stuffs each of the arguments using Conv< A >.
     Thus the Send buffer contents are a header with TgtInfo, and then the
     actual arguments.
5. This happens for all outgoing messages this cycle.
6. Postmaster sends out buffers in process. It then waits for incoming
     stuff in the recvBufs.
7. The scene now shifts to the PostMaster on the remote node. In its
     'process', the clearPending call is executed. It looks at the recvBuf
     and extracts the tgtInfo. This tells it what the originating object
     was, and what SrcFinfo to use for the outgoing message. !!!! fixme
             (Currently I use BindIndex which ought to be right but is done
             very indirectly. I need to check.)
8. we call the SrcFinfo::sendBuffer call from the originating object.
 The sendBuffer call converts the arguments
 back from the bufer to their native form and despatches using the
 regular messaging. Note that the MsgDigest will have done the right
 thing here to set up the regular messaging even for off-node 
 DataIndices on this element.
9. Messages reach their targets.

Level 2. Setup.
1. Objects and messages set up the regular way. Objects may have 
subsets of their Data arrays placed on different nodes. Messages
are globals, their contents are replicated on every node.
2. When a SrcFinfo::send call is about to execute for the first time, 
it is digested: Element::digestMessages. During this step each of the
messages is expanded by putTargetsInDigeest into target Erefs.
3. The target Erefs are inspected in filterOffNodeTargets. Off-node
targets are removed, and we record each pair of Source DataId and node.
4. We now call putOffNodeTargetsInDigest. This generates the
HopFunc by calling OpFunc::makeHopFunc with the fid of the SrcFinfo.
5. putOffNodeTargetsInDigest then examines the Source/Node pairs and 
creates HopFunc/Eref pairs which are stuffed into the msgDigest.
Note that these Erefs are the hacked version where the Eref specifies
the originating object, plus using its FieldIndex to specify the target
node.

Possible optimization here would be to have a sendToAll buffer
that was filled when the digestMessages detected that a majority of
target nodes received a given message. A setup time complication, not
a runtime problem.
			.................


I've now put together most of the code. Compilation in progress.
Checkin 4851.
===========================================================================
18 Nov 2013. Trying to compile HopFunc.cpp.
19 Nov 2013. Compiled. Trying to debug in testConvVectorOfFcetors.
20 Nov 2013. going through unit tests. Convs were messy but now OK.
	Need to handle copy of Msgs to n targets.

Other than that pending matter, the unit tests clear. Checkin 4856.

===========================================================================
22 Nov: See if the mpi_scatter will work with larger recv buffers than send
sizes. The documentation isn't hugely clear on this.
Tried it out in proc4.cpp, which is based on the scatter code in proc3.cpp. 
Seems to work at first, but then fails once up to 16 nodes and higher.
Let me check the mpirecv can handle this asymmetry. In proc5.cpp, which is
based on proc2.cpp. Yes, this works. It is documented too.

From the 12 Nov list:
* Fill in send buffers:
	- Message digest consolidation
	- Arg conversion.
+ Figure out best design for internode comms. 
* simple block mapping of DataId to rawIndex.
- Global objects and synchronization
- Communication between nodes for setup operations
- Communication between nodes for Set/Get operations
- Clock ticks between nodes.
+ Messaging between nodes.
	- MPI implementation

Next is to apply the MPI implementation, use the MPI_Irecv approach as the
MPI_Scatter won't handle variable-length messages.

Added most of code for PostMaster MPI stuff, yet to compile.


===========================================================================
22 Nov: Compiles.  Checkin 4857.
Beginning unit tests. Goes OK until it has to issue a Set call to an off-node
data entry in testShell. Will need to fix SetGet. Also looks like unit
tests are not going across nodes.

Looking at SetGet. While the use of a HopFunc takes us most of the way,
there are 5 issues remaining:
- I may set a field. The hack in the send command knows that the src eref 
	is not a field, so it uses the fieldIndex as the node identifier.
	As I can just query the eref about the target node, this is can be
	handled provided the system knows it is a SetGet operation.
* PostMasteraddToSendBuf doesn't know that this is a SetGet rather than msg.
	Subclasses of the HopFunc? Switch statements off a flag passed in by
	bindIndex?
	Created a HopIndex class that lets met track both.
+ How to set up the bindIndex to refer to the correct SetGet operation on the
	target node?
	Make a static global vector of all OpFuncs, refer to this.
	Need to be sure that the dynamically assigned OpFuncs come strictly 
	after the statically assigned ones.
+ On the target node, SrcFinfo::sendBuffer was used to convert back. Here it
	won't work as the sendBuffer refers to the regular send call.
	Need to refer to something quite different. This could be handled
	if the TAG on the mpi msg tells the target node to do something else.
	I would need to get the OpFuncBase or Hopfunc to convert buffer to 
	arguments and call the OpFunc->op().
- How to handle return values for get funcs?

Stages in off-node 'set':
1. call SetGet::set. Checks if isDataHere(). If so, easy. If not, make a 
	HopFunc with the flag to say it is a Set operation. Call hop->op.
2. hop->op does the generic addToBuf.
3. The postmaster sees the special flag to tell it that it is a Set operation
	in p->addToSendBuf.
4. PostMaster sends the data off immediately [and polls for return]. 
	Appropriate SETTAG.
5. Tgt PostMaster is polling for any SETTAG from any source. Gets it.
6. Uses TgtInfo to look up appropriate OpFunc from BindIndex, and Eref.
7. Calls OpFunc1Base<A>::op->opBuffer( Eref, buf). This does the 
	conversion from the buffer and calls the virtual OpFunc with the 
	converted argument.

This would be it for the Set unless I wanted to make it a blocking Set.

Stages in off-node 'get':
Same as 1 to 6 but the originating PostMaster has to do polling for the return.
7. Calls GetOpFunc1Base< A >::op->opBuffer. This creates a temporary A field
	for the return value and gets it.
8. Converts into a buffer explicitly.
9. Sends back using yet another tag.
10. The return poll harvests the value. The GetOpFuncBase knows what type to 
	expect back and use in the buffer.

This is going to be tough. First things first with the set. Perhaps should
separate out the messaging in the unit tests, too.

For the Shell operations, I think it should be yet another SHELLTAG or maybe
even separate tags for the various shell ops. There are only about a dozen
of them. They all seem to be forward only ops, no returns.
Could we use a variant of Set?

I also need to figure out how to do setVec and getVec.
===========================================================================

Week 1: Threading out
Week 2: Messaging refactoring
Week 3: Element and field indexing refactoring. Data Handler out.
Week 4: Scheduling refactoring, recompile and unit tests. 
	Copy refactoring.
Week 4.5: MsgIds replaced with ObjIds. Automatic message compilation.
Week 5: Standalone tests on mpi framework
Week 6: Multinode messaging design, implementation, compilation, no tests.
Week 7: Multinode Set/Get. Compile and test.

Major changes in MOOSE:
1. Refactored messaging and eliminated queueing. Messages now use the same 
high-level structure, but at runtime they do not dump 'send' calls and
arguments to a queue. Instead the messages are compiled into function-object
pairs and use these to directly call the target objects with the arguments.
This permits the use of pointers as arguments in messages.
This means that message calls are not synchronized by the queue - they occur
as called.
This means that the stack can be traversed to find what called what.
This is likely to be much faster, haven't yet benchmarked.
2 Eliminated threading.  As a necessary precondition for the above, threading
was eliminated. In addition to obviating the need for queues, this greatly 
simplified the process loop and scheculing.
3. Used integral scheduling. Earlier the clock emitted ticks which could have
arbitrary doubles for the timestep. Now there is a base clock dt, and all
tick events are integral multiples of this base. There is no ambiguity about
ordering nor any issue with rounding. Much simpler too.
4. Indexing of data is simplified. There are three integers to look up data:
	Id -> Identifies the Element which is a wrapper for data and messages.
	DataId or DataIndex -> Identifies a data entry in the Element. A simple
		one-dimensional array, can have a single entry.
	FieldIndex -> Identifies a FieldObject on a data entry. The Field Object
		must be associated with a FieldElement. Most objects do not
		have any field objects. Cases which do include synapses, which
		are subsidiary objects sitting on each receptor.
	A corollary of this is that there is no more multidimensional indexing
	of data entries. No more field masks.
5. MsgIds are no more. Instead Msgs are identified by ObjIds just like any other
	object.

6. The Set commands now directly call functions on the target object to assign
	values. But see parallel specialization.
7. The Get commands now directly call functions on the target object to obtain.
	values. But see parallel specialization.

8. Any DataEntry can now be the parent of an Element. Earlier this was a bit
	ambiguous and many parts of the code required instead that only Elements
	could be parents.
9. Synapses now use a ring buffer instead of a sorted queue to handle spike 
	arrival times.

In addition to these changes which are evident in serial code, there are many
changes to enable parallelization.
10. There are multiple Element classes, all derived from Element. Each has its
own rules for node decomposition. This is automatic: upon creation the 
appropriate data entries are put on the appropriate nodes. Note that every
node has the full Element tree, but only subsets of the DataEntries are 
put on different nodes.
11. Parallel messaging is implemented by using special flags during the 
'compilation' of messages. This intercepts off-node messages and puts the
message request and arguments into a buffer such that any given node only
gets a single request for the message, even if there are a hundred targets for
the message on the remote node. The buffer for each node is sent off at the
end of each clock tick in a non-blocking manner. Arriving buffers are digested
and the messages sent out to all targets on the local node.
12. Parallel field assignment and once-off function calls (set) is implemented
by detecting if target is off-node, and if so, putting into a different buffer.
This is dispatched in a non-blocking manner, but only after clearing all 
pending incoming field assignment calls.
13. Shell simulation control operations are implemented through 'set' calls.
14. Get calls work in a similar manner to 'set', except that data also gets
	sent back.

===========================================================================

25 Nov 2013: Starting to compile with the Set calls included. But this is a
bad way to develop. Should instead do multinode messaging separately,
and then do the MPI-based real test.
Compiled, clears single-node unit tests. Checkin 4858.
Now to separate out the MPI-based tests.


Setting up cross-node Shell::doCreate by going through the newly
implemented SetGet functions.
Need to unit check the argument passing and fitting into buffers,
by HopFunc.

Compiled, too sleepy to tackle furter.
===========================================================================
26 Nov 2013.
Finally tracked down the problem with off-node Set calls. The
order of evaluation of the Conv functions in OpFunc6Base::opBuffer()
is backwards. The last argument, arg6, gets evaluated first, and so on.
Presumably backwards in all cases. Need to redo conversion syntax.
Turns out C++ explicitly does NOT specify an evaluation order.

So did the less elegant but simple code to extract arguments. Now seems to
work for creating objects, but the unit tests don't seem to go all the way 
through.
===========================================================================
27 Nov 2013.
MPI is being difficult about reporting where it croaks. By stepping through
it looks like it is testShell.cpp:1718, testTreeTraversal()
From what I can see it looks like an innocuous call to Shell::doCreate 
of a Neutral on line 50.
Turns out i was being careless with the indexing of the MPI_Request array,
which is continguous rather than indexed by node number. Fixed. 

Next bug is with path traversal. This croaks on both nodes. I think 
what is happening is:

Node 0: The path traversal does not handle the rawIndex/DataIndex properly
Node 1: The 'get' call is sent across as the baseclass 'set' and this 
causes problems. (This possibility is somewhat of a guess).

So I need to tackle both the 'get' call and the path traversal logic.
But it would be good to have the 'set' call properly unit tested.

Worked on the path traversal. Should now be able to do it without having to
inquire across nodes.
Clears the path unit test. Now fails in testShell.cpp::testCopyFieldElement
which needs to assign the vector of sizes just to set up the system.
All very well, but at this point I don't have the capability to copy except
from Globals.

Some pending cross-node things:
Proper unit test for Set across nodes. I think it works.
Get. If I get this to work I can check the 'set' call too.
SetVec + unit test. Try the testCopyFieldElement with the objects created as
	locals.
GetVec
The whole list of Shell commands.


===========================================================================
30 Nov 2013
Subha suggests having elements which put contents on specific nodes.
Perhaps a better thing to do for automation is to have Elements which keep
the entire contents on one node. I would prefer to be able to do models without
reference to size of computer.

Now working on Get. In Postmaster.cpp.
===========================================================================
1 Dec 2013.
Some input on handling random numbers.
Hi,
  Got some answers to the second problem: (1) if the number of random
numbers to be created for each node is known beforehand "jump ahead"
will work (just mailed Upi about it)
(http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/JUMP/index.html)
  (2) And for the online case, there seem to be methods for this
(http://software.intel.com/en-us/forums/topic/283349): MT2203 gives up
to 6024 independent streams and Charles Leiserson (the one who wrote
"Introduction to Algorithms" with Cormen, and Rivest) published a
paper on another system for this:
http://supertech.csail.mit.edu/papers/dprng.pdf

I believe Pritish will have more insight into what is best suited for
our purpose.

Best,
  Subha

On Sat, Nov 30, 2013 at 4:14 PM, Upinder S. Bhalla <bhalla@ncbs.res.in> wrote:
> Hi, Pritish,
>     Good to hear from you. It is good timing and we could use your inputs.
>
> Based in good part on the discussions we had during your visit, I've redone
> the MOOSE basecode to be single-threaded and done lots of other pending
> fixes along the way. I'm desiging at present for a simple MPI interface,
> easily adapted to Charm++.
>
> Couple of key design questions have come up, again motivated by our earlier
> discussions, and I don't remember the suggestions you had on them.
>
> 1. The python/scripting interface to MOOSE. With the current design, we need
> to have node zero talk to the Python script on the one hand, and to all the
> other nodes via MPI. The other nodes do not need to know Python. As I
> recall, you had suggested an alternative architecture where there was some
> kind of socket communication to one node. In principle we could have the
> Python-aware node outside the cluster, but then I'm not sure how one would
> tunnel MPI through to the cluster nodes. Do advise.
>
> 2. Parallel random numbers. Suppose synaptic weights need to be assigned by
> a random seed across all nodes. Each node knows what to do with its own
> subset of numbers, but the problem is that we may have to grind through a
> few million numbers in the random sequence before we get there. Even worse
> is how to handle random numbers emitted at runtime for node-specific
> calculations. How does one ensure that the outcome is the same independent
> of number of nodes? I'm sure this is a classic problem in parallel
> simulations.
>
> Best,
>     Upi

Now compiles but doesn't even start to clear unit tests.
===========================================================================
2 Dec 2013
Now clears serial unit tests. Fails on target node when called from 
remoteGet in parallel unit tests. Indexing issue.
Oddly, the other node croaked in clearPendingSet.

===========================================================================
3 Dec 2013.
Understood bug, now to fix. I was trying to use 'get' for the path of
an object that was off-node. The good news is that it does indeed
correctly do most of the work of going off-node to get the object.
The bad news is that it loses the fieldIndex somewhere along the way.

Put in a bit of a hack so that the TgtInfo puts the fieldIndex into
the 'size' field for Sets and Gets, which do not use the size field.

Now carries on to Shell::testCopyFieldElement.
Lots of errors on Node01, for inability to create objects. Line 401
in Shell::testCopyFieldElement.
Fixed the cpoy at that point, but a few lines later it goes bad.

Problems start in testShell.cpp::testChildren.
Seems that I need to fix up Shell:::doDelete.
Did that. Now there is a problem in testShell.cpp:450, all I'm doing
is trying a 'get' on numField.
===========================================================================
4 Dec 2013.
Incrementally going through unit tests. 
Currently a bit stuck in testShell.cpp::copyFieldElement, not because
of real errors but because SetVec and GetVec are not yet implemented.

Next error is due to trying low-level functions to look up values, in 
testShell::testObjidToAndFromPath()
Easy to fix one part of it in line 511, but would be good to
have a general solution.
===========================================================================
5 Dec 2013. Checkin 4878.
Now it is a heisenbug. I'm trying to do a simple set/get on an offnode field,
works when I step through it using gdb, fails when run without stepping.
In testShell.cpp:517.

Possibly the set is not complete when I ask for the 'get'.

Redid SetGet a bit so that both use the same MPI tag. As hoped, this fixes
the sequencing problem of set and then get.

Next problem in testShellSetGet is another SetVec issue. Postpone.
On now to the message tests. This requires further fixing of the
Shell commands.

Man Shell commands now fixed. The program now goes on till 
testShell.cpp:912, where again it runs into the SetVec problem.
Checkin 4880.

Working on SetVec. In SetGet.h and HopFunc.h: 60 ish.
Pending:
- What to do with globals
- Propagate changes through to the dispatchBuffers call and the postmaster
- Handle stuff on remote nodes.

===========================================================================
6 Dec 2013.
Working in FieldElement.cpp:85 to put in the skeleton for getNumOnNode.
The serious version of this function sould look up remote nodes to get
the values.
===========================================================================
7 Dec 2013.

Compiling the changes in for the SetVec.

Discussions with the MOOSE team, seems like there is a feeling that the
path semantics need to change for the FieldElements. Specifically, use only 
the fieldIndex part to index the FieldElement, since they are always children
of an Element whose DataIndex applies both to the base element and to the
FieldElement value.
pare
the FieldElements as always a child of a regular Element 

Silly bug in unit tests, the messages maintain their own indexing for
lookup. and this is incremented when a new message is created. But the
unit tests on the master node make and delete several messages, leaving
their indices different from those on the slave nodes.

split the unit tests into nonmpi and mpi parts,
Provide a global index when creating messages.
Clear out the indices after each block of unit tests.

Messy thing comes up with msgs. The DataId for look up of the msg objects
can get misaligned on different nodes, already does due to unit tests on
node 0. I've implemented a hack to use the DataId for the msgs as generated
by the master node. Unfortunately all the Msg::copy functions and likely 
many others need to make msgs on the fly, they are not able to use something
generated centrally. Current messy solution is to let the copy operations use
automatic incrementing.

===========================================================================
8 Dec.
Accumulating issues
'send' from a global: the MsgDigest needs to only do stuff to local node.
'send' to all: Need to use ALLDATA for targets on local node, and then 
send single calls to remote ones.

Major cleanup done on the MsgId generation, not perfect but the key commands
issue a direct specification of the mid on all nodes and the rest of the time
it does an increment which should happen identically on all nodes.

Checkin 4890.
In the meantime, on with the unit tests to testShell.cpp:942 at which point
we need getVec.

Bypassed that using a loop of 'get' calls. Now it starts messaging, which
of course crashes.
===========================================================================
9 Dec
Spent a lot of time adding a unit test for filterOffNodeTargets. Which works,
but doesn't fix the problem with the messages not going out.

Two bugs now: 
- in testShell.cpp::testCopyFieldElement:460. Number of copies
is wrong for 4 nodes, but OK for 2.
- in testShell.cpp::testShellAddMsg:1001. Basically the messages don't go.

Seems that the Clock does not have any messages to go out.


===========================================================================
10 Dec
Sorted outgoing messages of clock. The ALLDATA flag in the targets had 
confused it regarding which node to send to.

Now it seems that although there is a message from a1 to a2, the msgDigest
doesn't show any outgoing node.

Nasty. Tracked down to error in how SingleMsg was handling sources() and
targets().

More subtleties needed fixing in how the off-node targets were identified.

Now it is down to getting the postmaster to schedule message sending.
First problem is that the reinit call isn't propagating to the off-node
clocks.

Much struggling later, turned out to be just that I needed to make clean
and recompile. But now it is hanging in a confusing way, in a section 
before the 'start'. Also it thinks reinit has been called twice on the
worker node.
===========================================================================
11 Dec.
Fixed an issue with the clearPending() command so that it can safely be
called recursively. This helps.

Now it is trying to send a message on node 1 but the list of targets includes
off-node entries.

Much painful debugging of the filterOffNodeTargets later, now it clears
the message sending. Hooray! But it promptly fails in the next unit test,
where I copy messages. Checkin 4900.

Next place of failure seems to be when the system tries to send messages
to nonexistent objects, deleted when the testShell::testShellAddMsg
function ended, but accessed by the next unit test testCopyMsgOps
in Shell::doReinit. Somewhere in clearPendingSend.
===========================================================================
12 Dec.
Turns out we're accessing objects a1 and b1: these were the msg sources of
messages on the respective nodes. So it seems likely that the postmaster is
reprising the old message calls. Similar to what happened with SetGet.

Step 1: Put Postmaster on Clock 9, guaranteed to be the last called. Doesn't
fix it, but this was a necessary cleanup anyway.
Question is, do the deleted objects somehow send out stuff on the originating 
node, or is it something left over in the recvBuf on the target node?

Since the thing croaks on doReinit, which does NOT trigger a send call on the
Arith object, it is most likely something in the recvBuf. But possibly
the send buffer isn't being cleared either.

Fixed it up. The send buffer was not being cleared and also there was no
check that all sends had completed, before going on to the next cycle.
With this we advance further till the next instance of getVec.
Actually the problem is with the copy command. 
Turns out I was not correctly offsetting the original data when doing the
copy. Now clears that test. Checkin 4907.
Next, in testSyncSynapseSize: Fixed a low-level lookup that would only work
on a single-node calculation. Then yet another getVec is commented out.
With this it clears all unit tests on 2 nodes. Major milestone.
Checkin 4908.
On 4 nodes it croaks in testShellAddMsg:968, which is our old friend where
it starts off the little test of all the different message types.
Fails on 2 of the 4 nodes: one with a bad msg index in Arith::send, and 
another trying to finish up a SetVec. Perhaps we're not synced properly?

Working on GetVec. Mostly there with the code framework. Still need to
tie the Field< A >::getVec function to HopFunc.h::GetHopFunc::opVec.
===========================================================================
13 Dec
Trying to compile with the getVec. Issue with how the class is derived from
OpFunc1Base< A* >, where the pointer causes friction in the definition of 
opVec.

Trying to compile.

===========================================================================
14 Dec.
Trying to clean up the mess with GetVec. Compiled but buggy.
Turns out this is a bit of a dead-end. I had earlier subclassed 
GetOpFuncBase< A > off OpFunc1Base< A* > which was very clean and elegant for
returning values. In trying to do GetVec I've done it off opFuncBase1< A >,
but that has messed up the earlier return structure and seems to be unnecessary
anyway. I may need to revert back to 4908
===========================================================================
15 Dec.

GetVec now works on 2 nodes, and the whole unit test clear. 
But GetVec still does not work for 4-node tests.

Some more fixing, now getVec does seem to work but again there are problems in
testShellAddmsg. Chekin 4919.

I think there may be a problem with setVec on multiple nodes.

===========================================================================
18 Dec.
SetVec it is. Still to track down. I'm concerned it isn't even
getting caught at HopFunc.h:103. Is the control flow bypassing this?

===========================================================================
19 Dec
Found two problems on the 'send' side when trying to send a 5-entry
array to 4 nodes in the subdivision 2, 2, 1, 0.
1. The e.getNode() reports node 2 as the target for e={elm, 5, 0}
	Should either be node 3, or no node.
2. The setSendSize is 4, but reports a dataSize of 2. Actually
	both should be zero as nothing is to be transmitted. In fact
	the system should flag this and not even attempt to send anything.

Huge mess in HopFunc.h:OpVec and co. There is a confusion between 
serial index of FieldElements, between DataId and FieldIndex.

Sort of fixed. Now it fails in some of the messaging calls on 4 nodes,
typically on the last entry, but in other cases too.
===========================================================================
21 Dec
The ones that fail are: bdefg
b: OneToAll. Fails on last entry
d: Diagonal. Fails on last entry.
e: Sparse. Fails on first and last entry.
f: OneToAll, where all entries go to all others. Fails on all.
g: Sparse, where all entries go to all others, except self. Fails on all.

All these could be explained if the last entry, which is on node 2, has
problems both sending and receiving.
Look at what happens on its postMaster.

From Process on PostMaster on Node 2:
(gdb) p *(TgtInfo*)(&sendBuf_[0][0])
$47 = {static headerSize = 3, id_ = {id = {id_ = 264}, dataId = 4, 
    fieldIndex = 0}, bindIndex_ = 1, dataSize_ = 1}
(gdb) p *(TgtInfo*)(&sendBuf_[0][4])
$48 = {static headerSize = 3, id_ = {id = {id_ = 266}, dataId = 4, 
    fieldIndex = 0}, bindIndex_ = 1, dataSize_ = 1}
(gdb) p *(TgtInfo*)(&sendBuf_[0][8])
$49 = {static headerSize = 3, id_ = {id = {id_ = 268}, dataId = 4, 
    fieldIndex = 0}, bindIndex_ = 1, dataSize_ = 1}
(gdb) p *(TgtInfo*)(&sendBuf_[1][0])
$50 = {static headerSize = 3, id_ = {id = {id_ = 266}, dataId = 4, 
    fieldIndex = 1}, bindIndex_ = 1, dataSize_ = 1}
(gdb) p *(TgtInfo*)(&sendBuf_[1][4])
$51 = {static headerSize = 3, id_ = {id = {id_ = 268}, dataId = 4, 
    fieldIndex = 1}, bindIndex_ = 1, dataSize_ = 1}
(gdb) p *(TgtInfo*)(&sendBuf_[3][0])
$52 = {static headerSize = 3, id_ = {id = {id_ = 266}, dataId = 4, 
    fieldIndex = 3}, bindIndex_ = 1, dataSize_ = 1}

Looks very reasonable. The sendSize in all cases is also reasonable
Stepped through, looks like node zero didn't get the data that was sent
by node 2. I was monitoring in PostMaster::clearPendingRecv.

Turned out an error in how I understood the MPI_TestSome arguments. The 
status index I was using was wrong.

With this fixed, I now clear the testShell.cpp:testShellAddMsg function
which is the crucial test of message passing. But now stuck in 
testCopyMsgOps.

This bug is because I don't let DataElement start with zero entries. I 
force it to be 1. Let's examine this logic. No, seems bad. I should
permit zero entries on the DataElement.

With that it clears unit tests on 4 nodes. Hooray! But the quit call does
not succeed in getting it to quit.

Also fails on 5 nodes now. in testShell.cpp:925
This is due to my trying to do a direct access to a data entry on the
master 0 node. The data is not on this node.

Fixed this little bug. 
Now it fails again in testShellAddMsg. That is turning out to be
the most definitive unit test.

Just for completeness, ran on 3 nodes. Clears unit tests there too.

Checkin 4924. Now to tackle why it fails on 5 nodes.
Failure is due to erroneous message outcome as detected on master node.

Here the message outcome is like this:
(0, 0) (1, 2) (2, 2) (3, 3) (4, 4) 
(5, 5) (4, 4) (3, 3) (2, 4) (1, 1) 
(15, 15) (15, 11) (15, 16) (15, 13) (15, 15) 
(14, 14) (13, 9) (12, 13) (11, 9) (10, 10) 

This is massages d,e,f and g.
d: Diagonal. Fails on second entry.
e: Sparse. Fails on second-last entry
f: OneToAll, where all entries go to all others. Fails on all but first and last
g: Sparse, where all entries go to all others, except self. Fails as above.

Important check: is it consistent? No.
d (0, 0) (1, 1) (2, 4) (3, 3) (4, 0) 
f(15, 15) (15, 15) (15, 13) (15, 15) (15, 13) 
g(14, 14) (13, 13) (12, 10) (11, 11) (10, 8)

Ran it a number of times without gdb. Fails in most cases for 3 and above.
Again, not consistent in how it fails.

I would guess that this happens because the processes launch off without
a barrier at the start. Let's try.
No, that doesn't fix it.

===========================================================================
22 Dec 2013
Another try: Put a barrier on PostMaster::reinit.
That doesn't work either. From the variety of error messages, it is clear that 
at least part of the problem is synchronization. In some cases it claims that
I'm trying to set off a new simulation while an old one is still running.
In many cases the message sending in the testShellAddMsg comes out wrong.

I may need to make 'set' a blocking call. However, all the 'get' calls have
worked.

Put in a barrier in PostMaster::reinit and process, after all the loops
and operations. With this it clears unit tests for 1 through to 16 nodes.
Also quits cleanly. Checkin 4925.

So this is how long it took:
Week 1: Oct 12-19: Threading out
Week 2: Oct 19-26 Messaging refactoring
Week 3: Oct 26-Nov02: Element and field indexing refactoring. Data Handler out.
Week 4: Nov 02-12. Scheduling refactoring, recompile and unit tests. 
	Copy refactoring. MsgIds replaced with ObjIds. Automatic message compilation.
Week 4.5-5: Nov12-15 Standalone tests on mpi framework
Week 6: Nov 16-23 Multinode messaging design, implementation, compilation, 
	no tests.
Week 7: Nov 23-30 Multinode Set. Compile and test.
Week 8: Nov 30-Dec 7. Multinode Get. Most of the Shell commands work. SetVec.
Week 9: Dec 7-14. Multinode messaging. Much cleanup. Clears all unit tests 
	for 2 nodes. Began GetVec.
Week 10: Dec 14-21: GetVec works. Much bug stomping through scaling up to 
	any number of nodes. 
Dec 22: Clears all unit tests on any number of nodes from 1 to 16.

Did a valgrind on the single node. Looks quite filthy. All seems to be in MPI.
Uninitialized variables, leaks, the works.

Now to look ahead. One minor item is to have Id::path return a string without
trailing braces.  Done. Checkin 4926.

Next steps:
	- Get it working with python
	- Benchmarks

===========================================================================
25 Dec 2013.
Tried to get a script-based test model to go. Very awkward, no luck.
Resurrected a benchmark/unit test for and int fire network. This is now in 
testBiophysics.cpp::testIntFireNetwork. Doesn't match values accurately for
the basic run test. Also, fails even with 2 nodes.
===========================================================================
26 Dec 2013.

Fixed issue with SparseMsg::randomConnect: It now more properly figures out
which matrix entries need to be filled on local node.
Next issue is that of overflow of the send/recv buffers when setting a very
large matrix for the synaptic weights.

Fixed that simply by making it bigger. But now I need to do something about the
get command and it isn't so simple because we need one buffer per node.

===========================================================================
27 Dec 2013
in testIntFireNetwork (runsteps=5)
    at testBiophysics.cpp:125

Getting the weight vec fails for 4 nodes, values are wrong.

===========================================================================
28 Dec
Traced the problem with getvec to the setvec most likely. But this reveals
a more fundamental issue: What range should the set/getVec calls span for
FieldElements? Currently I'm trying to scan everything. 
Given that the Python interface treats FieldElements as equivalent to the
regular Elements (with the proviso that the DataId is pre-specified), we
should use the set/getVec calls only spanning the FieldIndex for FieldElements.
Not as handy as being able to access all the entries in a single call. But
perhaps that kind of access should be a separate function that uses a wildcard.

Put in a little fix so that objects cannot be renamed if there is an existing
object with the same name. Unfortunately one of the unit tests creates
a parentless object... testAsync.cpp:192
===========================================================================
29 Dec.
Easily fixed, arranged adoption of the parentless object. Clears unit tests.
Valgrind is not so happy, but I think most of this is MPI being awful.
For the SetVec, I'll change call semantics so it only goes to one array.
If it is a FieldElement, then it is indexed by the FieldIndex. If it is
a DataElement, it is indexed by the DataIndex.
As a more general Set call I should look into having a specification of an
ObjId list with provision for a specific or an 'all' entry.

Some more cleanup on the name change and also on the Shell::doCreate, so that
the tests are done at the start of the function rather than after it has
been dispatched to the nodes. Now correctly returns Id() on failure.
Added unit tests for this.
Checkin 4937. After a minor fix for the back-ported SpikeGen, 
Valgrind seems clean.

Things to do to clean up the SetVec etc.

- Iterator for Erefs/elements.
- Add the node-specific Element class
- Redo HopFunc::opVec.
- Redo SetGet::setVec to take ObjId argument.

I've examined the iterator options. Currently not worth it.

Reimplemented the SetGet::vec functions for the updated semantics, where
the vector returned is the last array. For DataElements it is the
array of values from all DataEntries, and for FieldElements it is the
array of values from all FieldEntries on a particular DataIndex. Many
unit tests need fixing now.

Checkin 4939.
Cleared unit tests on single node. Fails on 2, doing testShell.cpp::testCopy. 
I think the copy itself fails, not the setget. But possibly the values oon
the remote node were not set in the first place.

===========================================================================
30 Dec
In Python, 'element' refers to a single object. A bit like GENESIS.
There is 'ematrix' for the vector of data or the vector of fields: in each
case a one-dimensional vector of objects. Suggest to rename the Python 
'ematrix' to evec elevec elmarray earray erray elmtab etab elmvec wrapper
eptr elmptr emulti elmulti multielm etuple elmtuple elmvec eset elmset 
elmatrix arrayelm multielm elmarray elmindex elmdex elemdex elemulti elookup


Traced the problem for the testCopy. As far as I can see the copy itself is
OK, but the original assignment of values to the Global has failed on the
remote node.

Now clears unit tests with up to 8 nodes. But the IntFire test isn't
complete yet. Looking at the output values. They do not match. One systematic
thing: the values for Vm get steadily smaller with larger numbers of nodes.
1. Check if off-node IntFires are clocked.		Yes
2. Check if off-node IntFires get off-node msgs.	No.
===========================================================================
31 Dec.
Now this is strange. The smaller scale test for multinode message worked,
see testShellAddMsg.

Made fixes to path and doFind functions to convert to and from string paths.
This is to fix things with the pymoose code and conventions.

Examining the message passing. Turns out no data was passing from node 0 to
node 1, though the postmaster functions were indeed called.

OK, I think I have it. When the SparseMsg sets up using randomConnect, it
only fills up local node messages. So there is no info of off-node msgs for
Element::disgestMsgs to work on.

What I need to do is recode this a bit so that the msgs themselves generate
a list of nodes to which they project, and have this used by digestMsgs.

As a quick test, I reinstated the entire SparseMatrix so all the messages
are represented. Outcome is a bit puzzling. The node 0 response is identical
to the single-node version, but the last node value isn't and it varies with
number of nodes. I wonder if this is related to the problem with the match
to the previous version.
Scaled the above run up to 12 nodes, at which the Vm100 moves over to other
than node 0. Now the response changes.

Tried increasing the synaptic delay so that it is at least one timestep long.
Otherwise we might get within-timestep delivery of spikes to node0, but not
to any other node. Doesn't work.

So the clue here is that the node0 value Vm100 is independent of # nodes, 
whereas the last node value Vm900 changes every time. Are the local
messages going out correctly on non-node-zero?

Tried a number of values from node0. Turns out that at least one of them
(Vm101) goes back and forth between a couple of values depending on numNodes. 
None of the other node0 reported values changes.
All of the node1 reported values change. Is there node misdirection?

Tried on 3 nodes. Each node is receivng the same number of bytes as was
sent, respectively to the other nodes. So it doesn't look like we're missing
on one message or another.

Something wrong about what is sent, it seems.

===========================================================================
1 Jan 2014.
Things to try:
- Send the DataId and node of the sender instead of the timestamp. Work 
backwards to figure out what is missing in different node configurations.

Here it is with only node 0:
        11      14      22      23      28      34      36      38      45     48       50      51      56      60      66      67      71      73      77     84       86      96      99      107     109     112     117     124     126    127      135     145     149     150     156     157     160     172     175    178      183     185     192     198     205     213     216     237     244    245      246     248     264     269     270     273     274     281     291    297      300     304     316     318     324     325     331     341     348    349      353     357     358     365     369     375     383     390     397    400      402     412     414     421     427     430     435     448     450    454      456     473     474     475     488     490     491     492     495    497      498     501     506     514     517     522     526     530     533    539      546     547     556     566     570     582     586     587     594    596      604     607     613     614     622     627     634     636     639    650      654     655     656     668     671     676     682     686     690    692      700     701     704     710     713     714     719     722     724    725      726     730     734     735     737     738     739     740     741    745      748     750     754     756     762     769     783     788     792    793      798     801     811     815     818     824     833     837     841    843      845     854     856     857     865     866     876     880     888    892      899     902     903     906     908     911     912     915     917    929      932     965     968     980     982     983     987     992     994    998      1000    1001    1006    1009    1011    1012    1021

Then: Confirmed we get same set when split into 2 nodes.
Then: Check what arrives. 
	a. The value of what arrives (dataId) matches the
	dataId of the sender. As it should.
	b. The arrivals seem to match all the ones that were sent.
Then:	Check that the arrivals go to the right targets OK.
	Check that the locals go to the right targets OK.
	Check that the weights and delays get set correctly.

Here are some debug prints to check these. Each entry is the dataId,FieldIndex:
The call args are the func#, srcDataId.
(gdb) call e.e_->printMsgDigest( 1, 0 )
0:      0:              9,0     10,0    14,0    20,0    53,0    54,0    55,0   66,0     67,0    72,0    74,0    80,0    81,0    89,0    105,0   124,0   132,0  134,0    135,0   138,0   139,0   140,0   162,0   189,0   197,0   208,0   222,0  233,0    243,0   245,0   248,0   261,0   286,0   324,0   337,0   355,0   369,0  375,0    378,0   381,0   392,0   394,0   415,0   421,0   426,0   442,0   451,0  458,0    473,0   476,0   483,0   501,0   503,0   507,0   530,0   540,0   543,0  546,0    590,0   593,0   618,0   632,0   636,0   638,0   655,0   671,0   688,0  690,0    696,0   703,0   711,0   717,0   732,0   751,0   755,0   764,0   770,0  778,0    781,0   786,0   807,0   826,0   868,0   896,0   907,0   921,0   924,0  925,0    927,0   934,0   941,0   950,0   959,0   963,0   970,0   976,0
(gdb) call e.e_->printMsgDigest( 1, 1 )
1:      0:              3,0     26,0    30,0    38,0    58,0    60,0    106,0  108,0    109,0   110,0   129,0   142,0   143,0   147,0   157,0   166,0   172,0  176,0    177,0   198,0   206,0   215,0   217,0   225,0   235,0   265,0   267,0  269,0    278,0   289,0   291,0   303,0   311,0   327,0   328,0   353,0   357,0  391,0    399,0   406,0   408,0   412,0   425,0   468,0   476,1   478,0   487,0  489,0    490,0   501,1   502,0   507,1   509,0   536,0   540,1   568,0   576,0  578,0    579,0   591,0   597,0   603,0   624,0   625,0   655,1   658,0   664,0  698,0    713,0   729,0   747,0   749,0   753,0   765,0   771,0   802,0   803,0  806,0    820,0   829,0   841,0   847,0   857,0   860,0   874,0   881,0   885,0  893,0    894,0   900,0   912,0   915,0   919,0   922,0   923,0   924,1   929,0  944,0    947,0   951,0   959,1   960,0   965,0   966,0   975,0   979,0   987,0  1002,0   1017,0  1018,0
(gdb) call e.e_->printMsgDigest( 1, 2 )
2:      0:              2,0     7,0     8,0     19,0    35,0    36,0    43,0   47,0     58,1    60,1    61,0    70,0    79,0    88,0    90,0    108,1   115,0  120,0    124,1   125,0   130,0   149,0   156,0   157,1   158,0   162,1   181,0  190,0    192,0   196,0   199,0   202,0   203,0   221,0   223,0   264,0   288,0  300,0    319,0   320,0   325,0   353,1   359,0   371,0   382,0   383,0   393,0  399,1    421,1   423,0   425,1   427,0   442,1   446,0   450,0   454,0   455,0  466,0    476,2   487,1   497,0   507,2   514,0   516,0   518,0   526,0   533,0  534,0    548,0   560,0   568,1   570,0   572,0   578,1   581,0   606,0   631,0  638,1    651,0   671,1   673,0   674,0   678,0   680,0   697,0   704,0   706,0  727,0    738,0   745,0   752,0   779,0   785,0   789,0   796,0   800,0   850,0  858,0    866,0   868,1   872,0   885,1   899,0   905,0   913,0   923,1   955,0  958,0    974,0   976,1   997,0   1004,0  1007,0
(gdb) call e.e_->printMsgDigest( 1, 900 )
900:    0:              18,91   26,95   29,91   33,82   43,108  45,87   49,94  78,97    81,98   89,96   91,84   110,88  133,88  140,103 143,92  166,81  176,86 180,84   190,83  192,94  197,84  230,101 237,95  243,94  249,74  256,88  259,99 269,93   277,86  303,83  311,104 312,83  314,94  322,78  332,86  347,89  393,101402,87   417,90  458,99  474,95  483,98  485,90  500,87  523,76  525,100 536,86 539,96   544,91  546,99  583,96  584,102 610,74  620,91  623,89  638,80  644,82 645,97   648,85  662,118 666,82  689,85  691,101 692,96  697,79  716,101 719,94 730,83   742,65  751,82  756,92  757,81  760,86  768,97  769,102 796,83  797,65 800,87   811,97  813,92  824,81  825,87  844,94  858,86  870,95  874,107 875,88 895,100  911,88  917,88  927,88  928,77  933,77  940,97  967,74  988,82  995,98
(gdb) call e.e_->printMsgDigest( 1, 901 )
901:    0:              22,77   24,97   46,102  67,81   81,99   86,102  91,85  99,80    112,95  136,90  155,82  171,90  181,84  234,93  238,98  250,88  269,94 278,94   287,95  311,105 313,87  323,84  348,98  349,87  357,89  361,76  377,84 378,91   399,87  406,80  422,92  438,117 446,90  453,80  459,81  466,83  477,70 486,103  493,106 497,85  500,88  505,82  513,100 518,95  529,100 531,103 535,93 537,97   542,82  544,92  556,83  559,87  563,96  565,91  567,87  583,97  588,78 596,83   600,65  612,87  635,99  641,91  715,88  743,83  748,84  766,76  791,83 792,72   794,92  799,82  801,100 804,101 841,88  846,86  865,96  868,101 874,108877,100  903,89  933,78  938,89  952,82  983,92  985,89  1002,95 1015,107       1019,85  1021,81
(gdb) call e.e_->printMsgDigest( 1, 902 )
902:    0:              0,81    10,91   16,104  39,89   50,106  57,86   61,76  69,89    75,91   82,88   83,70   91,86   96,90   101,91  102,89  103,92  106,92 108,85   110,89  118,87  132,87  141,89  153,106 158,106 160,98  166,82  169,83 185,105  228,81  231,99  249,75  267,86  280,78  287,96  289,82  321,96  322,79 325,84   328,120 342,103 380,95  387,81  388,104 401,82  405,81  420,83  424,80 434,99   471,93  476,82  482,90  486,104 488,92  504,76  517,83  522,89  525,101526,78   539,97  542,83  548,102 564,94  566,101 570,79  577,82  584,103 592,87 593,90   595,119 605,88  612,88  622,95  623,90  644,83  647,94  654,91  670,92 694,100  703,74  705,86  711,104 722,83  728,92  734,96  750,107 757,82  762,83 769,103  786,103 790,87  808,72  819,97  828,109 832,93  833,82  846,87  847,97 852,96   871,88  888,101 893,78  901,98  904,84  914,101 934,94  940,98  980,72 991,94   995,99  1005,86 1020,85

Now to see what happens on the nodes in a 2-node simulation.
Node 0:
(gdb) call e.e_->printMsgDigest(1,0)
0:      0:              9,0     10,0    14,0    20,0    53,0    54,0    55,0   66,0     67,0    72,0    74,0    80,0    81,0    89,0    105,0   124,0   132,0  134,0    135,0   138,0   139,0   140,0   162,0   189,0   197,0   208,0   222,0  233,0    243,0   245,0   248,0   261,0   286,0   324,0   337,0   355,0   369,0  375,0    378,0   381,0   392,0   394,0   415,0   421,0   426,0   442,0   451,0  458,0    473,0   476,0   483,0   501,0   503,0   507,01:         0,1
(gdb) call e.e_->printMsgDigest(1,1)
1:      0:              3,0     26,0    30,0    38,0    58,0    60,0    106,0  108,0    109,0   110,0   129,0   142,0   143,0   147,0   157,0   166,0   172,0  176,0    177,0   198,0   206,0   215,0   217,0   225,0   235,0   265,0   267,0  269,0    278,0   289,0   291,0   303,0   311,0   327,0   328,0   353,0   357,0  391,0    399,0   406,0   408,0   412,0   425,0   468,0   476,1   478,0   487,0  489,0    490,0   501,1   502,0   507,1   509,01:         1,1
(gdb) call e.e_->printMsgDigest(1,2)
2:      0:              2,0     7,0     8,0     19,0    35,0    36,0    43,0   47,0     58,1    60,1    61,0    70,0    79,0    88,0    90,0    108,1   115,0  120,0    124,1   125,0   130,0   149,0   156,0   157,1   158,0   162,1   181,0  190,0    192,0   196,0   199,0   202,0   203,0   221,0   223,0   264,0   288,0  300,0    319,0   320,0   325,0   353,1   359,0   371,0   382,0   383,0   393,0  399,1    421,1   423,0   425,1   427,0   442,1   446,0   450,0   454,0   455,0  466,0    476,2   487,1   497,0   507,21:         2,1
(gdb) call e.e_->printMsgDigest(1,900)
900:    0:              18,91   26,95   29,91   33,82   43,108  45,87   49,94  78,97    81,98   89,96   91,84   110,88  133,88  140,103 143,92  166,81  176,86 180,84   190,83  192,94  197,84  230,101 237,95  243,94  249,74  256,88  259,99 269,93   277,86  303,83  311,104 312,83  314,94  322,78  332,86  347,89  393,101402,87   417,90  458,99  474,95  483,98  485,90  500,87
(gdb) call e.e_->printMsgDigest(1,901)
901:    0:              22,77   24,97   46,102  67,81   81,99   86,102  91,85  99,80    112,95  136,90  155,82  171,90  181,84  234,93  238,98  250,88  269,94 278,94   287,95  311,105 313,87  323,84  348,98  349,87  357,89  361,76  377,84 378,91   399,87  406,80  422,92  438,117 446,90  453,80  459,81  466,83  477,70 486,103  493,106 497,85  500,88  505,82
(gdb) call e.e_->printMsgDigest(1,902)
902:    0:              0,81    10,91   16,104  39,89   50,106  57,86   61,76  69,89    75,91   82,88   83,70   91,86   96,90   101,91  102,89  103,92  106,92 108,85   110,89  118,87  132,87  141,89  153,106 158,106 160,98  166,82  169,83 185,105  228,81  231,99  249,75  267,86  280,78  287,96  289,82  321,96  322,79 325,84   328,120 342,103 380,95  387,81  388,104 401,82  405,81  420,83  424,80 434,99   471,93  476,82  482,90  486,104 488,92  504,76

Node 1:
(gdb) call e.e_->printMsgDigest(1,0)
0:      0:              530,0   540,0   543,0   546,0   590,0   593,0   618,0  632,0    636,0   638,0   655,0   671,0   688,0   690,0   696,0   703,0   711,0  717,0    732,0   751,0   755,0   764,0   770,0   778,0   781,0   786,0   807,0  826,0    868,0   896,0   907,0   921,0   924,0   925,0   927,0   934,0   941,0  950,0    959,0   963,0   970,0   976,0
(gdb) call e.e_->printMsgDigest(1,1)
1:      0:              536,0   540,1   568,0   576,0   578,0   579,0   591,0  597,0    603,0   624,0   625,0   655,1   658,0   664,0   698,0   713,0   729,0  747,0    749,0   753,0   765,0   771,0   802,0   803,0   806,0   820,0   829,0  841,0    847,0   857,0   860,0   874,0   881,0   885,0   893,0   894,0   900,0  912,0    915,0   919,0   922,0   923,0   924,1   929,0   944,0   947,0   951,0  959,1    960,0   965,0   966,0   975,0   979,0   987,0   1002,0  1017,0  1018,0
(gdb) call e.e_->printMsgDigest(1,2)
2:      0:              514,0   516,0   518,0   526,0   533,0   534,0   548,0  560,0    568,1   570,0   572,0   578,1   581,0   606,0   631,0   638,1   651,0  671,1    673,0   674,0   678,0   680,0   697,0   704,0   706,0   727,0   738,0  745,0    752,0   779,0   785,0   789,0   796,0   800,0   850,0   858,0   866,0  868,1    872,0   885,1   899,0   905,0   913,0   923,1   955,0   958,0   974,0  976,1    997,0   1004,0  1007,0
(gdb) call e.e_->printMsgDigest(1,900)
900:    0:              523,76  525,100 536,86  539,96  544,91  546,99  583,96 584,102  610,74  620,91  623,89  638,80  644,82  645,97  648,85  662,118 666,82 689,85   691,101 692,96  697,79  716,101 719,94  730,83  742,65  751,82  756,92 757,81   760,86  768,97  769,102 796,83  797,65  800,87  811,97  813,92  824,81 825,87   844,94  858,86  870,95  874,107 875,88  895,100 911,88  917,88  927,88 928,77   933,77  940,97  967,74  988,82  995,981:                900,0
(gdb) call e.e_->printMsgDigest(1,901)
901:    0:              513,100 518,95  529,100 531,103 535,93  537,97  542,82 544,92   556,83  559,87  563,96  565,91  567,87  583,97  588,78  596,83  600,65 612,87   635,99  641,91  715,88  743,83  748,84  766,76  791,83  792,72  794,92 799,82   801,100 804,101 841,88  846,86  865,96  868,101 874,108 877,100 903,89 933,78   938,89  952,82  983,92  985,89  1002,95 1015,107        1019,85 1021,811:               901,0
(gdb) call e.e_->printMsgDigest(1,902)
902:    0:              517,83  522,89  525,101 526,78  539,97  542,83  548,102564,94   566,101 570,79  577,82  584,103 592,87  593,90  595,119 605,88  612,88 622,95   623,90  644,83  647,94  654,91  670,92  694,100 703,74  705,86  711,104722,83   728,92  734,96  750,107 757,82  762,83  769,103 786,103 790,87  808,72 819,97   828,109 832,93  833,82  846,87  847,97  852,96  871,88  888,101 893,78 901,98   904,84  914,101 934,94  940,98  980,72  991,94  995,99  1005,86 1020,851:               902,0


Well, even this looks OK. That leaves assignment of weights and delays. The
weights at least are checked already as part of the unit tests.
Another thing I can think of is there being an error in how the 
SpikeRingBuffer handles incoming at different times. Nothing apparent, the code
is really simple.

Next: monitor what the spikeRingBuffers contain on each cycle.
Here is the buffer on node 0 for IntFire0 at time = 0.4:
(gdb) p *(&buf_.weightSum_[0])@20
$6 = {0, 0, 0, 0, 0.0011231906618922949, 0.028195377006195489, 0, 0, 0, 
  0.011927361548878253, 0.012636251500807702, 0.011373002212494612, 
  0.02413556597195566, 0.030104355947114525, 0.014308227128349244, 
  0.0051488068467006086, 0, 0, 0.0080248786695301527, 0}

Here it is for IntFire900:
(gdb) p *(&buf_.weightSum_[0])@20
$9 = {0.0039365481445565818, 0, 0.019248148943297565, 0.0095276742568239578, 
  0.012616655579768122, 0.015804447340779008, 0.031027889060787857, 
  0.020450418312102557, 0, 0.018955300822854043, 0.029514752686955036, 
  0.025616842862218617, 0.010785101428627968, 0, 0.023539618398062886, 0, 
  0.013914890796877443, 0.029127159682102499, 0.0079548832122236495, 
  0.022189490515738725}

Here we are on 2 nodes:
IntFire0: (on node 0)
(gdb)  p *(&buf_.weightSum_[0])@20
$3 = {0, 0, 0, 0, 0.0011231906618922949, 0.028195377006195489, 0, 0, 0, 
  0.011927361548878253, 0.012636251500807702, 0.011373002212494612, 
  0.02413556597195566, 0.030104355947114525, 0.014308227128349244, 
  0.0051488068467006086, 0, 0, 0.0080248786695301527, 0}

IntFire900: (on node 1)
 p *(&buf_.weightSum_[0])@20
$6 = {0.0039365481445565818, 0, 0.019248148943297565, 0.008282735007815064, 
  0.001244939249008894, 0.012616655579768122, 0.013555899155326187, 
  0.020450418312102557, 0, 0.018955300822854043, 0, 0.046050151288509367, 
  0.019866545689292251, 0, 0.023539618398062886, 0, 0, 0.015623985570855439, 
  0.035372948120348156, 0.022189490515738725}

OK, finally we see some differences. Interestingly, it is all-or-none. Many
values are identical. Suggests that individual messages are not getting 
through. Now the problem is to trace these back to where they came from.
See the one which has zero in one and nonzero in another, should be able
to find the offending weight.
Given that many of the entries are correct, it seems that the weights are
probably OK. It is just messages not arriving.

Added yet another printf debug the Synapse::addSpike call, to see what
arguments come through. Specifically, some of these seem to be missing.

============================================================================
2 Jan 2014
On one node, reporting src DataId, dest FieldIndex.
monitoring synapses on IntFire 100:
        28,3    34,4    51,6    216,18  248,21  297,28  383,36  454,43  495,45 498,46   586,51  734,71  750,73  783,75

monitoring 900:
        157,14  172,15  178,16  300,27  375,33  435,39  450,41  475,45  498,46 582,51   607,53  650,58  682,65  722,69  724,70  737,73  748,74  762,76  793,78 801,79   837,83  845,86  857,87  865,88  908,91  911,92  929,95  1011,103

On 2 nodes,  monitoring IntFire 100: (output only on node 0):
        28,3    34,4    51,6    216,18  248,21  297,28  383,36  454,43  495,45 498,46   586,51  734,71  750,73  783,75

On 2 nodes, monitoring IntFire 900: (Output only on node 1):
582,51  607,53  650,58  722,69  724,70  737,73  762,76  793,78  801,79 837,83   845,86  857,87  865,88  908,91  911,92  929,95  1011,103
157,14 172,15   178,16  300,27  375,33  435,39  450,41  475,45  498,46

There are two incoming events missing on node1, 900. Importantly, they are
missing from what should be the local set of activities on node 1. 
The events coming from the sub-512 set are all there.
The missing events are: 682,65 and 748,74

Better directly check that all Vms and other values are set correctly.
Should print out the time-series for 682 and 748.

One node:
Here it is for 100:
0.2,0.00710756  0.4,0.00568605  0.6,0.00454884  0.8,0.0138681   1,0.0110945
For 682:
0.2,0.783242    0.4,-1e-07      0.6,0   0.8,0.0247299   1,0.0417334
For 748:
 0.2,0.778782    0.4,-1e-07      0.6,0   0.8,0   1,0
For 900:
0.2,0.276274    0.4,0.221019    0.6,0.176816    0.8,0.156851    1,0.133103

Two Nodes:
For 100: 
0.2,0.00710756  0.4,0.00568605  0.6,0.00454884  0.8,0.0138681   1,0.0110945
For 682:
0.2,0.783242    0.4,0.626594    0.6,0.515236    0.8,0.420256    1,0.365541
For 748:
0.2,0.778782    0.4,0.623026    0.6,0.51831     0.8,0.422253    1,0.337803
For 900:
0.2,0.276274    0.4,0.221019    0.6,0.176816    0.8,0.156851    1,0.132107

Differences: 682 and 748 seem to have spiked and rebounded on 1 nodes and not
on 2. There is a subtle difference for the 900 final value as well.

Let's look at other values on the IntFire:
On 2 nodes:
For 100:
Vm_ = 0.0071075635496526957, thresh_ = 0.80000000000000004, tau_ = 1, 
  refractoryPeriod_ = 0.40000000000000002, lastSpike_ = -0.40000000000000002, 
  bufferTime_ = 4
(gdb) p synapses_[0]
$3 = (Synapse &) @0x34e54d0: {weight_ = 0.0083155348012223847, 
  delay_ = 1.4613027032464743, buffer_ = 0x34af128}

For 682:
Vm_ = 0.78324243589304388, thresh_ = 0.80000000000000004, tau_ = 1, 
  refractoryPeriod_ = 0.40000000000000002, lastSpike_ = -0.40000000000000002, 
  bufferTime_ = 4
(gdb) p synapses_[0]
$4 = (Synapse &) @0x2c8ee60: {weight_ = 0.011778635610826314, 
  delay_ = 0.78097819723188877, buffer_ = 0x2c5a868}

On 1 node:
For 100:
Vm_ = 0.0071075635496526957, thresh_ = 0.80000000000000004, tau_ = 1, 
  refractoryPeriod_ = 0.40000000000000002, lastSpike_ = -0.40000000000000002, 
  bufferTime_ = 4}
(gdb) p synapses_[0]
$2 = (Synapse &) @0x2cc1ff0: {weight_ = 0.0083155348012223847, 
  delay_ = 1.4613027032464743, buffer_ = 0x2c6c0d8}

For 682
Vm_ = 0.78324243589304388, thresh_ = 0.80000000000000004, tau_ = 1, 
  refractoryPeriod_ = 0.40000000000000002, lastSpike_ = -0.40000000000000002, 
  bufferTime_ = 4}
(gdb) p synapses_[0]
$5 = (Synapse &) @0x2dafe30: {weight_ = 0.011778635610826314, 
  delay_ = 0.78097819723188877, buffer_ = 0x2c7d1a8}

I don't see any difference. Worrying. Why are they behaving so differently?
Is there something different in the buffers that causes it to fire on one
node but not on two?

On single node, looking at 682:
(gdb) p synapses_[65]
$2 = (Synapse &) @0x2db0448: {weight_ = 0.011979404771700501, 
  delay_ = 2.7584519190713763, buffer_ = 0x2c7d1a8}
(gdb) p *(&synapses_[65].buffer_->weightSum_[0])@20
$6 = {0.017451648577116431, 0, 0.020829295343719426, 0.027436947519890967, 
  0.011310790828429163, 0.0075863821897655731, 0.0028163404529914261, 0, 
  0.0060605108505114918, 0.0035257226461544635, 0, 0.0042454327363520864, 
  0.017304209023714066, 0, 0, 0.030237772795371709, 0, 0, 0, 
  0.018134191795252264}
Current bin is zero. 

On 2 nodes, looking at 682:
$2 = (Synapse &) @0x2c8f478: {weight_ = 0.011979404771700501, 
  delay_ = 2.7584519190713763, buffer_ = 0x2c5a868}
(gdb)  p *(&synapses_[65].buffer_->weightSum_[0])@20
$3 = {0, 0, 0, 0.015841135480441155, 0, 0, 0, 0, 0.0060605108505114918, 
  0 <repeats 11 times>}
Current bin is zero. 
 
So, at start, there are a whole lot of values in the spike queue in the 
one-node case that are absent in the two-node case. Key point is that some
are present in zero delay bins, where they should not be allowed as the
delay value is 2.something. Presumably some of these were going to be 
delivered later. I wonder if this is aliasing, going all around the delay
buffer for some long delays. As a quick check, I've doubled the buffer time.
One node:
 Vm100 = 0.00232901, 0.241473, 0.182856, 0.0102305, 0.0857292
Vm900 = 0.105924, 0.259907, 0.0230763, 0.107449
Two nodes:
Vm100 = 0.00232901, 0.241473, 0.182856, 0.0102305, 0.0857292
Vm900 = 0.0958305, 0.260413, 0.0324635, 0.107449

Still not fixed. Actually even the old buffer time was longer than the 
simulation run itself, so there should never have been any entries on the
first few bins.

Just realized that there was a hack where the 'time' field was used to 
send out the DataIndex of the calling object. Fixed.
1 node case:
 Vm100 = 0.00734036, 0.243398, 0.196683, 0, 0.0857292
Vm900 = 0.115276, 0.290259, 0.00774015, 0.107449
Two nodes:
 Vm100 = 0.00734036, 0.243398, 0.196683, 0, 0.0857292
Vm900 = 0.116182, 0.294987, 0.00774015, 0.107449

Nope, note the first and second entry in Vm900 don't quite match. So
on with the debugging.
Here is the buffer on Vm900 for 1 node:
(gdb) p *(&weightSum_[0])@40
$7 = {0, 0, 0.020633713547140359, 0.020450418312102557, 0, 
  0.0090267524495720869, 0, 0.016535398601554334, 0.027418064908124505, 
  0.0091998224612325433, 0.01710238739848137, 0.017760914247483016, 0, 0, 
  0.008282735007815064, 0.022528946460224686, 0.01175383843947202, 
  0.015918520665727555, 0, 0, 0.028642979008145629, 0 <repeats 19 times>}

Here it is for 2 nodes:
$4 = {0, 0, 0.013555899155326187, 0.020450418312102557, 0, 
  0.0090267524495720869, 0, 0.016535398601554334, 0, 0.0079548832122236495, 
  0.01710238739848137, 0, 0, 0, 0.008282735007815064, 0.022528946460224686, 0, 
  0, 0, 0, 0.016026323428377509, 0 <repeats 19 times>}

Actually this isn't a good test, because the time of the sample is after
all messaging for lower entry values in the first case, but before the
node0 entries reach the IntFire in the second.

Nevertheless, when I trace through (on 2 nodes) till the point where the 
first nonzero entry is actually used, it is still 0.013555899155326187.
So some spikes are just not reaching.

Perhaps I should change the nature of the problem and set it up so that
the IntFire calculations are independent of update order. At this point
this is supposedly done by the delay time but that is fragile. If the delay
is less than the timestep then it will fail. The ring buffer is also fragile,
not to mention wasteful and clumsy in how it is implemented.

For the IntFire, the obvious solution is to split the Vm computations off
from the message sending. It would be
	Init: Harvest the ring buffer for incoming spikes.
	Process: send if needed.
	Clock 9: exchange with nodes.

============================================================================
3 Jan 2014
I think I may have identified a (the?) problem. The spike gets delivered to
a different bin depending on whether it arrives before or after the 'pop'
of the ring buffer. Let's fix by passing in the current time to the ring
buffer upon pop, so that all incoming spikes have an absolute time reference.
This will also enable automatic buffer resizing, highly desirable.

This seems to work. Don't have to do the separation into init and process.
Also the ring buffer is much more robust now. The only thing we need to
guarantee is that the delay exceeds the step-size for the internode sync.

Checkin 4950.
If we revisit the time this took, it looks like this:
Week 1: Oct 12-19: Threading out
Week 2: Oct 19-26 Messaging refactoring
Week 3: Oct 26-Nov02: Element and field indexing refactoring. Data Handler out.
Week 4: Nov 02-12. Scheduling refactoring, recompile and unit tests. 
	Copy refactoring. MsgIds replaced with ObjIds. Automatic message compilation.
Week 4.5-5: Nov12-15 Standalone tests on mpi framework
Week 6: Nov 16-23 Multinode messaging design, implementation, compilation, 
	no tests.
Week 7: Nov 23-30 Multinode Set. Compile and test.
Week 8: Nov 30-Dec 7. Multinode Get. Most of the Shell commands work. SetVec.
Week 9: Dec 7-14. Multinode messaging. Much cleanup. Clears all unit tests 
	for 2 nodes. Began GetVec.
Week 10: Dec 14-21: GetVec works. Much bug stomping through scaling up to 
	any number of nodes. 
Dec 22: Clears all unit tests on any number of nodes from 1 to 16.
Week 11-12: Key semantic change that affected paths as well as SetVec and 
	GetVec: All element 'arrays' are treated as indexing of the last vector
	only. Thus FieldElements use fieldIndex and regular Elements use
	dataIndex. Many bugfixes. Set up unit test with IntFire array, 
	horrendous debugging, but it finally works on multiple nodes. The
	Python interface also updated to reflect all of these.

Next steps.
- Work with subha to get the Python interface up to speed
	+ Need to add a field to use vector of pairs of xy values, for filling 
		in the Sparse matrix.
	+ Write python version of IntFire test.
+ Fix Obj creation so one can't make up an invalid ObjId that way.
	Checkin 4958.
- Naming conventions. Need to dig up and redo.

============================================================================
4 Jan.
	Here are the naming conventions as noted by Niraj:
 - CamelCase, as before.
 - Append src fields with "Out".
 - Don't do anything special for dest fields and shared fields.
 * For set/get fields, start with 'set'/'get', followed by the field name exactly the way it is. For example, 'getn' for 'n', and 'getVm' for 'Vm'.

I don't remember the thought behind the last one: is the idea that I should
remove the automatic insertion of an underscore by the ValueFinfo code?

Confirmed this. Also put in code to automatically change the first char of the
field name to upper case, i.e., staty with camel case. Many unit test
fixes after this. Checkin as 4959.

============================================================================
5 Jan
Implemented change in name for src fields. I'm leaving 'output' as is though.
Went through cleaning out legacy naming and patching up the unit tests to
match. Now clears unit tests on 1-5 nodes.
Two other legacy SrcFinfo names are retained: 'process' and 'reinit'.
Checkin 4960.

Now porting over the ghastly kinetics directory. In the process redid the
Zombie function, this will need significant fix-ups in the code.

On the semantics of the mesh handling: 
- I want to be able to access volume reasonably fast, but not necessarily at
	 msg rates.
- I may need to have the mesh update all contained pools and reactions,
	including changing parameters and telling them to expand their arrays.

Msg-based handling:
	explicit
	immediate completion of operations as soon as mesh changed
	need to always set up messages when pools created, won't work without.
	Could end up with odd tree arrangement
	Coding needed now.

tree-based handling
	Implicit
	Would need to implement an internal traversal if mesh changes.
	A bit slower to get vols from pool.
	Dont' need the pool messages, could handle without.
	Could get funny scaling if pool moved to another compartment
	Coding deferred somewhat.
	Will need some nasty node-specific ops, perhaps new implementation?

I should really avoid having two ways to do things. If we wish to have a
	guarantee that all pools are within their parent compartment then
	the tree handling should apply.

============================================================================
6 Jan 2014
Discussed preferences on compartment policy with Harsha. She prefers the
tree handling. So let's go with that. It will break a lot of scripts.

Implemented this, relatively easy since all volume and compartment queries
were going through the lookupVolumeFromMesh set of commands.

Managed to compile all of the kinetics directory. Unfortunately there are 
rather a lot of dependencies on things like mesh and SimManager. Would
prefer to avoid. In doing so I fixed up the zombie handling code. The
policy is very simple now: the zombieSwap function allocates the data for
the new class, destroys the old data, and replaces the Cinfo on the Element,
all without touching any of the messaging. Perhaps should put in a test to
make sure that the swapped classes are message-compatible.

Checkin 4961.

Went through and cleaned up SrcFinfo names. Checkin 4962.
Went through and implemented the zombify functions. Checkin 4963.

Ported back the Mesh classes. Trying to compile.

============================================================================
7 Jan 2014
Fixed but in MsgElement.h: had not implemented zombieSwap. Checkin 4965.

Discussion with Subha. Things to fix.
- Implement bool ObjId::bad() const.
This handles cases of returning a bad ObjId (rather than root) and also cases
where a user makes an ObjId using direct indexing, and needs to check if it 
is valid.
- Fix up ShowMsg functions in Msg handler classes
- Clearly specify message semantics, particularly as they relate to 
	FieldElements. Single and Sparse are OK, but others need work.
		OneToAll: should traverse both the dataIndex and fieldIndex.
		OneToOne: Should go to only the last index: data or field as
			case may be.
		Diagonal: Also to last index.

OK, implemented bool ObjId::bad. 
Also replaced ObjId::dataId with ObjId::dataIndex for consistency.
============================================================================
8 Jan 2014.

Fixed up OneToOne to deal with fieldElement targets. This may already be useful
in mapping mol pools to MeshEntries. yet to compile 
============================================================================
9 Jan 2014. 
Added OneToOneDataIndexMsg to handle cases where we only want to map dataIndex
to and from, even if target is a FieldElement. Compiled. Checkin 4972.

Put in code to report all targets of a given SrcFinfo: both the object targets
and the functions they call. Works but not in parallel.
Also added Neutral fields to report a list of Value, Src and Dest Finfos.
Checkin 4973.

Now looking at Hines numbering and setting up to do the efficient linear
diffusion in dendrites.

============================================================================
10 Jan 2014
Checked in some updates to help the LookupField function. Also added
the snippet for a test IntFire network.

subha@chamcham:~/src/moose_async13/tests/python$ mpiexec -host localhost -np 2 /usr/bin/python test_pymoose.py : -np 1 ../../moose

Tried lots of variations on this, none were able to launch it properly.
============================================================================
11 Jan 2014.

Estimating balancing
Say 1 micron compt, so total of 5000 compts.
Say 10 molecules diffuse in all. So 50K updates per dt.
Say 1/dt = D/dx^2, so 1/dt = 10e-12/1e-12. So dt = 0.1. 
Say 100 compts have reac systems with 250 mols in each. So 25K updates per dt.
Comparable. But reaction dt may be longer.

Designing MeshEntries.
	Current design uses DataIndex for entry arrays.
	Could better use FieldIndex. But this would force the model
	to be on a single node. Unless we separate the meshing info from the
	chemical solver.

============================================================================
12 Jan 2014.

Multi-solver design.
1. elec dt <= diffusion dt < chem dt.
2. I'm coupling elec and chem solvers through adaptors plus the regular
	object structure. Should try to do same for chem and diffusion.
3. The object decomposition should favour the more intensive solver. In
	this case diffusion. 
4. How does one define an object that is hosted by more than one solver?
	a) Have the solvers talk to each other directly, and only one solver
	talks to the object. There should be a unique representation of object.
	b)  Have multiple representations of the object coupled by adaptors
		or even regular messages.
	c) Have object proxies: equivalent to b) with implicit adaptors.
	d) Have zombies that talk to two solvers at once.

Specific use case
- Put diffusion solver so that each node does subset of molecules.
- Put chem solver so that each node does subset of spatial compts
- Put zombie molecules decomposed as per diffusion.
	- Each zombie has pointer to solver.
		Should I derive all chem solvers from PoolSolverBase which is 
		required to know how to talk to pools?
		Should I derive the solver from the PoolBase?
	- Use a special Element class that puts the contents on a specific
		node, and has just a single pointer for the data. This
		pointer is a subclass of PoolBase and it wraps the mesh, 
		with the additional index to identify which species it is.
		So all access functions are directed eventually to the mesh.
		Nodes lacking data don't need any pointer at all, as the
		access functions should never be called for them. But
		perhaps the mesh base should specify this.
	- How do I provide the extra fields in the Zombie, for xyz etc? The
		subclass of PoolBase could do this.
	- I should perhaps separate the mesh definition from the diffusion 
		solver?
		Or are the two so closely dependent that it doesn't make sense?
		- I would want to have a different numerical method for
		voxelized stochastic reac-diff.
		- Could derive a compartment mesh class from a base that 
		provides all the mols and vols. This is all that the zombie 
		needs to know.
	- Note that this arrangement will mean very straightforward volume
		conversions for pools. Reacs will remain messy.
- The chem solver matrix set up needs only elements and msgs.
	- The chem solver rate parameters need only the reac rates which are
- The core chem solver just solves chemical kinetics for a single compt.
	- It needs to take updates for changes due to diffusion.
	- It needs to solve stuff for cross-compt reactions. 
- The chem multicompt solver just runs the core solver through many compartments
- MeshEntries as FieldElements vs DataElements
	- They depend on what the Compartment is doing
	- They only have volume and xyz, and diffusion related parameters,
		possibly other things that can be
		computed rather than stored. But most of the diffusion terms
		are now used internally when we do derived classes for diffn.
	- The parent Compartment may scale them up and down as per need.
	- They don't send or receive any messages any more.

- Updates for changes due to diffusion:
	- Gather: kinetic solver on each node needs to pick up concs for all
	species, for the compts that the solver handles. Best done as a 
	'send' call from the molecules. The messaging system knows how to
	partition.
	- Compute: Kinetic solver could take latest concs or average with 
	previous concs, compute value at next step.
	- Scatter: Kinetic solver now sends values back.
	This design assumes that the kinetic solver has its own entire
	grue of zombies to act as a front for all these messages.
		
What it comes down to is that either the solvers have specialized messages,
	or they each manage the entire slew of zombies.
	- For specialized messages, it would have to be something like:
		- Vector of slice of indices/molecules for that node
		- Ability to send to specific node, preferably with node# as
			argument to the 'send' command.
			

Summary:
- Have only the compartment manage the zombies.
- Later derive diffusion engines from the compartment class.
- Build special Element class to put contents on specific nodes, and use
	single pointer for the data. 
- Build Pool and other zombies
- Port over chem solver
- Add in math expression evaluator
- Implement node-specific messages.
- Add in messages between compartment and kinetic solvers that scale to
	multinode diffusion.
- Not entirely clear what the MeshEntries do anymore. Just readonly volume 
	terms, possibly some interface to the display etc. But that should
	really be done by the geometry.


.....................................................................
Minor progress, began compilation of mesh directory. Most files are still
not compiled. But I can start with kinetic setup now.

============================================================================
14 Jan 2014
Trying to compile in features needed to test multinode. Stuck.

============================================================================
19 Jan 2014
Progress on getting old kinetics stuff to work. Various models load and
work but not the Kholodenko oscillatory model.

Compared oscillatory trafficking model (which does work) with the same model
in GENESIS. Output is essentially identical, and speed of MOOSE is within
about 10% of GENESIS. Tried to run with profiling but the python interface 
confuses Gprof and it doesn't emit the profiling file gmon.out. My feeling is 
that 10% should be reachable with some optimization. This would make MOOSE 
messaging as fast as the super-optmized messaging in the kinetics code in 
GENESIS - a very good sign.

Did some optimizations of MOOSE, got it to the desired 10% faster. Then reran 
with the GENESIS graphics turned off. Alas, now the GENESIS version runs 4x
faster: 5.6 seconds vs. 24.5 seconds.
Ran in various configs, still get same ratio: GENESIS kkit is about
4.5 times faster. Confirmed plot output is essentially identical.
If I look at the gprof logs in bench2.txt, it seems like the Pool::vProcess
from MOOSE alone takes almost half the time of the entire GENESIS run.
The other thing of note is that a whole lot of small functions follow
in MOOSE that are each called between 2 and 100 million times, each
contributing just a little to a total time of about 15 sec.
The main Process steps together take just 5.05 sec. Add the clock loop
(surprisingly large at 1.78 sec) and we're at 6.83sec. 
The key difference then is that in the optimized GENESIS code, the pointer
lookups to other objects are an essentially zero time operation. Despite all
the effort I've put in to the MOOSE messaging, there is a whole lot of lookup
and function shuffing, which adds up. I can only hope it scales well on many
nodes.
Compiling the buildQ branch with optimization to compare.
This takes about 23.5 sec to do just 1000 sec of runtime. So the
new version is about 9x faster. Of this almost all is messaging.

Tried to do a similar thing for the recurrent IntFire network.
Unfortunately, the system throws an error when I increase the runtime.
Checkin 4995.

Currently two errors to tackle:
* Kholodenko model doesn't work: wrong output.
* recurrentIntFire.py crashes when set to run for 10 seconds. Suspect the 
	synapse event handling again.
Implementation things to handle:
- Wildcard handling of full paths with indices.
- LookupGet across nodes.
- The Msg information functions for src msgs: msgSrcs and MsgSrcFunctions.
- Diffusion handler.
- Element that puts everything on one, specified node
- GlobalFields: things that need to be updated on all nodes even if
	the Element is local.
- Getting specific values in a vector across all nodes. For example,
	the # of elements.
- Moving objects between nodes.
Porting:
- Ksolver
- Biophysics directory
- Hsolver.

============================================================================
20 Jan 2014
Subha has gotten Pymoose to start up on multiple nodes. But the
test recurrentIntFireNetwork.py fails due to an Id on node 1 not
having its Element yet defined.

============================================================================
21 Jan 2014
Tracked down the problem: The Function Ids were scrambled because the static
initialization sequence differed on different nodes. This is likely to be
a recurring problem. I tried to forcibly set up a specified static init
sequence but this got stuck. The alternative is to do a clean sorting of
Function Ids. Trying that now. Working on compile.

============================================================================
22 Jan 2014
A couple more fixes later and we now have it run with the python interface.
But the outcome is not correct.
From 1 node:
[ 0.116055    0.14895359  0.05631837] [ 0.24245462  0.01996774  0.13110393]

From 2 nodes:
[ 0.09717813  0.12748211  0.03540703] [ 0.22541585  0.          0.13110393]

for 3 nodes:
[ 0.09717813  0.12748211  0.03540703] [ 0.21894769  0.          0.13110393]

============================================================================
23 Jan 2014.
Put in some printf debugging to check that weights and delays are assigned
the same, and also numSynapses, on different numbers of nodes. Seems OK.
Possibilities:
	- printing stuff out before the run has completed on all nodes
	Unlikely. I have a barrier to synchronize all nodes within the 
	PostMaster::process.
	- Messages not set up properly
	Here to check the numSynVec. Seems OK.
	- Messages not sent properly.
	- Messages being processed in a funny order.
	Unlikely, given that the same code works from C++

Subha has the good idea to try the C++ unit test run with the same combo of
pymoose wrapper and bare moose code.
This needs me to do 
setenv DOUNITTESTS 1
Ran with
 mpirun -np 1 python testu.py : -np 1 ../../../moose
and it rewarded me with an error in testShell.cpp:1016 which is in
testShellAddMsg, indicating a problem with the messaging.
Is the postmaster on the remote node not getting clocked?

Tracked it down. The postmaster was not getting scheduled at all. If I sched
the postmaster in the script it works. Let's try to move the scheduling call
into the 'init' function of main.cpp, since this is also used by the
python initializer.
This doesn't work, it clashes with one of the pre-existing unit tests.
Subha will do something in the moosemodule initializer instead.
Done, this works.
Some cleanup on testing and reporting of unit tests.

Tackled the bug in recurrentIntFire: was a problem in spikeRingBuffer.
With this ran some benchmarks with the recurrentIntFire.py model. Very
gratifying: it gives the same answer from 1 to 5 nodes, and the
runtime goes down from about 8.2 sec to 5 sec with 2 nodes on my 2-core
laptop.
Had to update the model so that it exhibits sustained activity.

Just for fun, redid with optimization. Now 3.26:1.95sec,
and for a 500 sec run it is 16.8:9.97 sec. Roughly the same factor speedup.
Did some fixes in moosemodule to be able to compile.
It would be interesting to compare this with the buildQ branch.
Checkin 5006.

============================================================================
24 Jan
>>> moose.showfield( '/model/kinetics/Reac1' )

[ /model[0]/kinetics[0]/Reac1[0] ]
kb                = 0.1
name              = Reac1
Kb                = 0.1
numSubstrates     = 2
numProducts       = 1
numField          = 1
numData           = 1
className         = Reac
Kf                = 0.199264663575
path              = /model[0]/kinetics[0]/Reac1[0]
kf                = 0.199264663575

The kf and Kf  (num and conc) are equal even though the vol should lead to a 
scaling. 

This turns out to be a messy one. The automatic camel-case ification is
converting 'get_kf' to 'getKf' which clashes with the 'get_Kf' for the 
conc-dependent version.

Should I call it numKf? That is probably cleanest since it clearly separates
it from the default Kf.

============================================================================
25 Jan
Did this. Fixes the problem and now the test on the Genesis script 
	kkit_objects_example.g
works except for the sumtotal.
Also the Kholodenko model now works too.  Checkin 5019. 
Fixed the sumtotal too. Checkin 5020.

Here is a matrix for diffusion in a branching neuron.

1   3
 2 4
  7   5
   8 6
    9
    10
    11

	1 2 3 4 5 6 7 8 9 10 11
1	x x
2       x x         x
3           x x
4           x x     x
5               x x 
6               x x     x
7         x   x     x x
8                   x x x
9                 x   x x x
10                      x x  x
11                        x  x


Now working on setting up a test case.
============================================================================
26 Jan
Fixed minor bug with Shell::doFind, wasn't handling the root element.

Back to the matrix elimination. Some fixes and debug printfs.
Now to put in the computations on the output vector.
Done that.  Checked in as 5025.
Now to put in comparison with GSL solution.
Put it in. My code is wrong somewhere. Doing simple test case.
Fixed a bug. Now the answer for the bigger matrix is close but
not a perfect match to the GSL version. I guess the best test is to do a
back-multiplication.
This revealed a bug, tracked down eventually and fixed. Now values match
for the bigger 11x11 test case above. To put the calculations in perspective,
there are only 10 forward-elimination steps and 11 backward substitution steps
in the solution for the 11x11 case.
Checkin 5026

Next: 
- Package into a standalone class that can plug into CylMesh and NeuroMesh.
- Implement with NeuroMesh 
	- Simplify NeuroMesh, eliminate the separate spine and PSD meshes.
		The Mesh is basically a diffusive zone.
	- Handle numbering
	- Bring in enough biophysics that the model morphology can be loaded.
	- Handle multiple instances for range of DiffConsts and molecules.
	- include active transport
	- Separate between nodes
From 19 Jan:
* Kholodenko model doesn't work: wrong output.
* recurrentIntFire.py crashes when set to run for 10 seconds. Suspect the 
	synapse event handling again.
Implementation things to handle:
+ Wildcard handling of full paths with indices.
- LookupGet across nodes.
- The Msg information functions for src msgs: msgSrcs and MsgSrcFunctions.
- Diffusion handler.
- Element that puts everything on one, specified node
- GlobalFields: things that need to be updated on all nodes even if
	the Element is local.
- Getting specific values in a vector across all nodes. For example,
	the # of elements.
- Moving objects between nodes.
Porting:
- Ksolver
- Biophysics directory
	HHGate is not working as an element field of HHChannel - somehow it gets a wrong Id. Are we supposed to replicate the FieldElementFinfoBase::postCreate function in createGate function because deferCreate is true?
- Hsolver.

============================================================================
27 Jan 2014
Planning design for diffusion solver and parallelization
- Implement separate diffusion solver, talks to mesh but independent
- Array of entries on diffusion solver put different molecules on different
	nodes. Each entry does entire diffusion matrix for that molecule.
- Molecules will need to be placed on Elements that can be assigned to specific
	nodes, to match those used by the diffusion solver. Molecules will be
	'solved' by the diff solver.
- Kinetic solver ksolve will also form an array, in this case one per diffusion
	compartment. Each ksolve will handle all the molecules in the reaction
	system in that compartment. The ksolve entries will be distributed
	between a bunch of nodes
- Ksolves will communicate with their molecules via messages. It will be 
	'push' messages on either side, the molecules sending 'nout' as 
	computed by the diffusion solver, and the ksolve sending back an update
	value of 'n' to go to the respective solver. This will be on the usual
	update clock of the system, ie, 1-10 ms.
- Need a 'Transpose' msg class to handle this: transposes the compartment-vs-
	molecule slices of the diffusion and ksolvers on each node. This 
	consolidates all data going to each node into a vector to avoid there
	being a huge mass of single messages.


GSOC project outline.

The MOOSE team (Upinder S. Bhalla, Harsha Rani, Subhasis Ray, Aditya Gilra, 
Aviral Goel, Niraj Dudani)
at NCBS Bangalore offer a project on 
"Optimization of parallel computation in MOOSE"

Project overview:
The Multiscale Object-Oriented Simulation Environment, 
(MOOSE, http://moose.ncbs.res.in) has recently implemented a new 
MPI-based parallelization scheme that supports automatic setup-time 
load balancing and transparent object access across nodes. There are 
several optimizations that could be developed from the current 
design. These include improved scaling with large numbers of nodes, 
GPU implementations of numerical engines, and run-time load balancing.
Due to the modular nature of the MOOSE code, such optimizations can be
carried out on tightly focussed portions of the code yet be tested with and
be useful for complex simulations. Thus, with a relatively quick learning
curve, this project should have substantial value for the MOOSE project,
and give the participant an insight into the design of a large neuronal
and systems biology simulator.

Request to fix wildcards with defined indices.
============================================================================
28 Jan 2014
Fixing wildcards turned out to be a huge operation. Basically had to
clean up lots of wildcard code to use ObjIds instead of Ids. This had 
ramifications all over the place including the Python API. Still issues
remain. For example:

a = moose.Neutral( '/a', 22 )
b = moose.Neutral( '/a[2]/b', 5 )
c = moose.Neutral( '/a[2]/b[2]/c', 10 )
x = moose.wildcardFind( '/a/##' )
>>> x
(<moose.Neutral: id=188, dataIndex=0, path=/a[2]/b[0]>, <moose.Neutral: id=189, dataIndex=0, path=/a[2]/b[2]/c[0]>)

This should instead give a whole lot of ObjIds.

Much pain later, fixed. Added unit tests for it.
Subha has uncovered another bug. Fixed. Added unit test. Checkin 5033.

The way to handle the NeuroMesh is to use getStencil to build up the
structure before reordering.
============================================================================
29 Jan.
- Find twigs
- Find first-order junctions joining 2 or more twigs
- Find 2nd order junctions
etc.
- Go through entire sparse matrix, count numCols on each row.
- Those with just 2 entries are twigs. Note them.
- Start from a twig. Iterate down to connected rows till you hit a row with
	> 3 entries . Stop. Note the branch entries, two or more of them.
- Start from another twig. Likewise.
- Count each 'hit' on each noted branch entry.
- Now go through branch entries. On each noted branch entry, check if it 
	has been hit by as many twigs as the branch entry has columns. If so,
	start off a sequence with this in the same manner.
- 
- Now go through branch entries 
4 Feb.
Trying to compile. Something wrong with 
g++  -lgsl -lgslcblas fastElim.cpp

============================================================================
6 Feb.
Turned out to be easy: the compile line was the wrong way around.
g++ fastElim.cpp -lgsl -lgslcblas 

Now looking at algorithm for sorting tree. The use of the matrix for doing this
is horribly inefficient. A tree structure would be the right way to do it.
Working with a parents vector.

Implemented, compiles, crashes when run. Progress!
============================================================================
7 Feb,
Parents vector test works. Now to try with a much more mangled tree.

I suspect now the solver itself is wrong. The mangled test had more cross-
terms.
============================================================================
9 Feb
Finally defeated it. The solver was wrong, enormously so. Logical errors both 
in the forward substitution and backward elimination stages. The amazing
thing is that it actually gave the correct solution for the earlier examples.

For a 11x11 matrix, (11 compartment) model, it does 24 (a -= b*c) operations
and 11 multiplications. So pretty efficient. For a linear cable it is
2(N-2) of the (a -= b*c) operations, and N multiplications.
I think the full figure for # of operations is 
2( B + (N-2) ) where B is # of branch points, N is number of compartments.

Checkin 5068.

Minor diversion: fixed a bug in wildcarding that Subha and Harsha had pointed
out a while ago. Checkin 5069.

Next: 
* Check if NeuroMesh still works, see if it properly generates 
	the 'parents' vector and sparse matrix for data.
- Build production diffusion solver for 1 node, M molecules
- Rebuild production ksolve for 1 node
- Rebuild production ksolve coupling to diffusion solver, 1 node
- Build production diffusion solver for N nodes, M molecules.
- Set up production ksolve for N nodes with diffusion
- Build production diffusion solver for N nodes, M molecules.
- Rebuild production GSSA solver for 1 node

============================================================================
11 Feb.
Minor fixes to synapses so that they are less likely to point to an empty 
buffer. Checkin 5081

To get started with NeuroMesh, implemented a small snippet to load and
run a model defined in a dotp file. This is neuronFromDotp.py.

Compiled NeuroMesh, extracted its array of parents, seems reasonable.
============================================================================
13 Feb
Designing interface between mesh, diffusion solver, and ksolver. The volume has
8 voxels and 6 molecular species, of which 4 diffuse.

				Node#
dsolve:	A [0,1,2,3,4,5,6,7]	0
dsolve:	B [0,1,2,3,4,5,6,7]	1
dsolve:	C [0,1,2,3,4,5,6,7]	2
dsolve:	D [0,1,2,3,4,5,6,7]	3
Mesh:	E [0,1,2,3,4,5,6,7]	Any, doesn't matter as there is no compute load
Mesh:	F [0,1,2,3,4,5,6,7]	Any, doesn't matter as there is no compute load

Ksolves:
           0 1 2 3 4 5 6 7 
           A
           B
           C
           D
           E
           F
Node #     0 0 1 1 2 2 3 3


So I can't just rely on the Dsolve providing an interface to the ksolve. The
bare mesh will have to do so too. 
Alternative is for the ksolve to talk via the MOOSE object API. Lots of 
messages, but the outcome would not depend at all on how the object was
zombified. The ksolve would have to get values, and correspondingly update,
every pool on every timestep. Saving grace is the low frequency of timesteps.
This would be trivial to do over messaging so the parallel decomposition would
be easy enough. Question is the comm cost.

Ops would be: Diffusion calculations: 0.1 ms
reac calculations: 1-10 ms: Takes the latest diffn values, puts them back.
Harvest needed from many nodes.
If no diffusion, then just needs to insert mesh values unless there is an 
update. 

Approach:
If we are going to do any diffusion at all, then dsolve is used for every
molecule even if it does not diffuse. We simply don't do any calculations 
for the non-diffusion cases. Mesh does not do any zombification.
If we don't have a ksolver then the kinetics won't work on the dsolve.

The dsolver zombie therefore can be set up to assume the existence of a 
ksolve affiliated with each compt.

ksolver cannot simply work by adding another pointer to the dsolver zombie,
because its job is both to get values and to set them. The pointer wont
help to get the values.

So the ksolver has to use messages. Basic set and get would seem to be
sufficient. Best case is to have an array of ksolves and do a one-to-one
message to the zombie.

Issue 1:
When a value is assigned to the pool or reac: Need to inform solver if it is
'n' or 'conc' on a pool. Or solver samples all values at each dt, which is
what it does for the diffusion case where it knows that all values will change.
	Could implement a msg triggered by setN and setConc, in poolBase.
	Poor design as it puts in a base class something needed only 
	in special cases. 

Issue 2:
What happens to clock of zombies of ksolve? Will need to unsched.

Option 2:
Pool maintains 2 pointers, one for ksolve and one for dsolve. Updates both
when a field changes. 
Ksolve has its ptrs to the pools, and requests for values as needed by its
(ksolve's) clock.

Either option:
	- Need to implement a 2-tier ksolve class: Single parent with stoich
	matrix and messaging to offspring, and array of kids each with 
	its voxel values.
	- Need to revisit cross-compt reactions and do in a much simpler,
	cleaner manner. Possibly exp-Euler reacn or RK45. Separate set from
	main body reactions, so that we don't have multiple stoichs for the
	different voxels.


============================================================================
20 Feb.
Working on the zombie handler structures for pools. Dsolve, DiffPoolVec,
and soon the Ksolve, Gsolve, and VoxSolve

============================================================================
21 Feb. Try to put skeleton code together for ksolve and diffusion directories.

Trying now to compile.
============================================================================
23 Feb.
Compiles. With this I can now begin to put in the ksolver and dsolver.

Immediate interface to the zombies is the VoxelPools class, which is
subclassed off ZombiePoolInterface.

There is an array of VoxelPools on the Ksolve, one for each voxel it handles.
Note that this could be done as a separate object, which would then take care
of node assignments. But the VoxelPools doesn't really have anything to 
interface with the rest of MOOSE, so we do it within the Ksolve.

Ksolve handles everything.
	Ksolve contains a Stoich. Used to be StoichCore.
		Stoich defines the reaction system.
		Stoich has the Stoichiometry matrix
		Stoich contains the gsl_odeiv2_system which is shared by all 
			the voxels.
		Stoich does NOT deal with pools.
		Stoich can also be used as a standalone class
			For getting steady states
			For doing GSSA calculations
	Ksolve contains a vector of VoxelPools.
		VoxelPools contain all the stuff to do reactions for one voxel:
		- S_, Sinit_, vectors for each molecule
			(I don't think y is needed)
		- The gsl_odeiv2_driver which is used ineternally and has to be 
			separate, as far as I can see, for each voxel.
		- a process function to advance the simulation.
	Ksolve contains info about mapping of local VoxelPools to the global
		indices.
	Ksolve contains info about how to get and set updates for all 
		voxelPools, including those whose diffusion calculations are
		performed off-node. It assembles node-specific stuff into
		vectors for sending and receiving.

For starters, we need to build the Ksolve, Stoich and OdeSystem.

Next: build the ksolve and Stoich to control the VoxelPools. 

============================================================================
24 Feb
Issue with zombification of pools. ZombiePools have two solvers, one for
diff and one for kinetics. The PoolBase zombification function takes a single
Id. 

Options:
	PoolBase::zombify: Take 2 ids.
	PoolBase::zombify: Take 2 ZombiePoolInterface ptrs.
	PoolBase::zombify: Take only the ksolver Id
	PoolBase::zombify: Take only the dsolver Id. If needed, put in a
		dummy. Do kinetics later, by explicitly setting 
		ZombiePool::setKineticSolver
	PoolBase::zombify: Don't take any Ids at all. Always have a dummy
		Dsolver available for the ZombiePool. Later replace with the
		real one in an explicit assignment. Likewise do kinetics later,
		by explicitly setting ZombiePool::setKineticSolver
	Don't even bother to zombify the pool.
	
How would we use it?
	Case 1: I have only a single-compt reac system, no diffusion.
		Here I don't have a dsolve. The zombification can use the
		Id of the ksolve to derive the one and only VoxelPool, and
		assign it for the zombie.
	Case 2: I have a reac-diff system. We first configure the diffusion,
		since it is simpler. By now the things are already zombies...
		We then set up the ksolve. Here we need to make suitable 
		numbers of the VoxelPools for each node, and assign them
		to the appropriate zombies as their parent.

OK, so the approach is:
	1. If Dsolve is present, zombify the pools already. Tell the ksolve
		how many voxels to use.
	2. Ksolve passes itself (or its Id) to the Stoich for assigning to the
		zombie. Only one pool entry exists, so assignment is easy.
		Here we can do the zombification call anyway, which ignores
		it if the pool is already a zombie.
	3. I had put the VoxelPool as the ZombiePoolInterface, but this
		won't work for diffusion arrays where only a single entry 
		for the ZombiePool needs to look up each pool on each voxel.
		So the ksolve has to be the ZombiePoolInterface.
	4. Don't worry about ordering, just put the ZombiePoolInterface
		pointer into the first available slot.
	5. Set up an unschedule function from Shell, also fix scheduling
		so useclock only ever sets up one msg.


============================================================================
25 Feb
Compiling stuff. Need to decide if all my vectors (other than those just from
the wildcard) should remain as vector< Id > or become vector< ObjId >
The use case: Should always be as Ids.
The wildcards: generate vector< ObjId >

============================================================================
26 Feb
Finally compiled it, most things are currently just placeholders.
No tests applied yet. Checkin 5114.

Next:
- Implement other zombies
- Implement test model, probably one already exists in unit tests.

============================================================================
27 Feb
Implemented ZombieBufPool. This was trivial.
Implemented ZombieReac. This entailed substantial fixups of Stoich and Ksolve.
Implemented ZombieEnz and ZombieMMenz, with more fixups of Stoich.
Checkin 5120.
Implemented the ZombieFuncPool, which is itself a placeholder.

Next step: Make a test. In demos there is a reasonable test of different
reacs and enzymes and sumtotals and tables. Should implement as C++ code
so it can be a self-contained unit test.

First pass: model created, not yet tested that it is built properly or runs.
============================================================================
3 March: 
Basic test model in testKsolve working, compare with the Genesis model 
kkit_objects_example.g
Currently tested only using Exp Euler, but now ready to deploy using
the ksolver. Checkin 5128.

Working through Stoich setup test. Currently stuck in ZombiePool::setSolver
which is being passed a Stoich though it expects a ksolve.

Given that i want to use the Stoich in several non-ksolve contexts, I need
to find a way around this.

Added a zombie field.
Now looks like it sets up the system, but there is a problem when 
deleting because the stoich tries to unzombify things that may
already be gone.
Fixed. Now clears unit tests. Checkin 5136.
============================================================================
4 March:
Implemented the framework for the numerical solutions. Nothing works yet
but it clears unit tests without croaking.
Next: 
- Put in vector 
- Check entries from unit test
- Run and test
============================================================================
6 March.
Solver taking shape. The Stoich matrix is well-formed, reinit happens, but
there is a crash when we run things. The Stoich::updateRates function 
never gets called.

Now that is better, but there is an error with the reinit: the values are
not all set.
============================================================================
8 March

We have a problem with interdependent ordering in initiatlizing the
reac system.
We need pools defined before stoich in order for the zombification to be 
able to assign concInits.

We need the stoich assigned before pools in order for the ode system to be
reasonable.

OK found a bug. I need to set the # of pool species. Working on this,
there is a problem because I also need to set the initConcs of each.

Sort through the dependencies:
ZombiePool depends on ptrs to dsolve or ksolve: ZombiePoolInterface*,
	to access its n and nInit.
	It needs these right away, as soon as zombification happens,
	in order to do the field conversion.
Stoich depends on Id to the poolInterface (ie, the ksolve) to assign numPools,
	and to do zombification calls for pools. Note the numPools assignment
	is redundant.
	Can it be deferred? No.
Ksolve stores Id as well as ptr to Stoich, but uses the stoichPtr_ only.
	Uses it for setting the ode.gslSys terms:
		getNumVarPools, params, and to set
	the stoich term for the VoxelPools in setNumAllVoxels.
	Also needed at runtime to do all the n/nInit field accesses voxelPools.
VoxelPools does a redundant resize of S_ and Sinit at the time of setStoich, 
	separate from the call from Stoich via ksolve. 

Ideally we would like to do these assignments at the last minute or in a 
single call.
There is also the general problem of handling concInit when voxelPools
are resized.

Suggested sequence is:
	Ksolve should know how many voxels it has. This does not need
	to depend on anything yet. Can start with a default of 1.
	Ksolve should be able to assign the base storage for all the 
	voxel bpools. This just needs to know how many pools (species) there 
	are. However, the only thing it needs from the stoich at this time
	is the number of var pools, it could set up the ODE stuff already
	Stoich needs to have the VoxelPools ready before it does the 
	zombification. So it needs to tell Ksolve the number of pools at the 
	time of resizeArrays, which happens during allocateModel.
	Then it does the zombification.
		This is when the nInit should be set.

	If we ever resize the number of voxels, we should do so at a level
		which can extract the concInit from the already established
		zombiePools.

0: no dependence.
0. Ksolve gets numVoxels (Optional if default of 1 is OK) 
0. Stoich is assigned Ksolve Id.
0. Ksolve is assigned Stoich Id.
1. Stoich::path is set. The model building starts.
2. Stoich tells Ksolve the value of numPools. Ksolve builds the VoxelPools.
	Can also assigne ode system at this time.
3. Stoich completes the zombification. The concInits are converted.

(4): only if needed: Can resize numVoxels, but have to do this with reference
	to the pool Ids so that we can extract concInit for scaling.

One issue found: I was using a temporary to store gslSys.
Next: the y vector used in the VoxelPools::gslFunc differs from &S_[0].
This means that I probably should not touch the pool entries for funcs.

Even if I comment this out, I get negative values in the gsl function after
one timestep. They are 'small', that is < 1e-5 of the other values,
but I don't know if this is something that will clean up with the 
integrator.

Progress: I eliminated the assertion for negative values, and it actually
runs. Gets stuck on the integration for a long time, I suspect that the
absolute error should be eliminated. Tried this, didn't help. I think the issue
may be again the problem with assigning S vs y. Have to figure out how
to assign y.

 Answers are wrong but not off-scale.
I think the assignment of the external table needs to go both to S and to y.
The sumtot is of course not working.

Some more relatively minor fixes. Now all ops are go. The main compromise is 
making the y vector the same size as numAllPools, with a dydt of zero. Still 
runs a trifle slow, will analyze.

Next steps:
- Do in a diffusion array, without diffusion, but different init concs.
- Get diffusion engine going.
- Resurrect SteadyStateSolver.

============================================================================
10 March 2014.
Checkin 5155
============================================================================
11 March 2014.
SteadyState compilation starting to take shape.

============================================================================
12 March 2014
Compiled SteadyState. Yet to test. Checkin 5158.
============================================================================
13 March 2014. SteadyState tests in progress. Runs etc but does not converge
to the correct values, though they do fit the conservation laws. Repeat runs 
from a given starting point give the same result. Nearby starts give nearby
answers, so it isn't totally random.
============================================================================
14 March 2014.
Got steady state solver to work. Some cleanup needed but basics in place.
Test script running in testSteadyState.py

Fixed crash on exit for Stoich.
============================================================================
16 Mar 2014.
A bit of a diversion: The unit tests were failing in testCompartment.cpp
on multiple nodes. Nans. I suspected a problem with internode messaging. 
Turned out to be quite subtle. The order of arguments was swapped in the Raxial
message. This was due to the known problem of order of evaluation of 
arguments in a multi-argument function call. I had fixed this in some
cases, but not in the handling of internode message buffers. 
With this corrected it clears unit tests from 1 to 8 nodes.

 
Steadily incorporating the components for the GssaSolver. Using a common
base class VoxelPoolBase.

Compiling the main Gsolve class. 

============================================================================
17 Mar 2014.
Gsolve compiles. Now to test.

After some fixing, now it seems good for most of the testRunGsolve test,
but doesn't yet handle sumtotals.

Implemented a snippet to show its use: scriptGssaSolver.py.
Almost identical to scriptKineticSolver.py, just replace Ksolve with Gsolve
and tweak volumes.

At this point there is no propagation of volume rescaling into pools and
reacs.

============================================================================

19 Mar. I think I've fixed sumtotals, but there is something wrong with the
enzymes in the unit tests for stochastic stuff.

Fixed it. The problem wasn't the solver, it was the volume scaling again.

Next: Fix volume scaling. The parent ChemCompt should traverse all children
and redo volumes, rates and so on.

============================================================================
20 Mar.
From Jan, here are the steps
Week	Date		Steps
1	Jan1-5		Naming policy stuff
2	Jan 5-11	Kinetics brought in. Volument handling revisited.
			Msg fixes.
3-4	Jan 12-25	Kinetics works, mostly. Fixes to Python/MPI stuff.
5-6	Jan 26-Feb7	Standalone fast matrix elimination. Fixing wildcards.
7-8	Feb8-21		Ksolve skeleton in place.		
9-10	Feb22-Mar8	Ksolve works
11	Mar9-14		Steady State solver ported and works.
12	Mar 15-20	GSSA ported and works.


Starting with cleaning out Dsolve and co. Probably don't even need to have
DiffPoolVecs exposed as FieldElements.
Done.
============================================================================
21 Mar.
Next steps:
- Assign either NeuroMesh or CylMesh to solver, using a SparseMatrix interface.
- Parse Sparse Matrix.
	- Handle regular diffusion with per-molecule diffusion consts
	- Handle motor transport overlay.
- Build test cases
	- Simple diffusion
	- Reac-diff with propagating waves
- Build inter-compartment reactions
- Set up adaptors and other multiscale effects
- Set up RDesigneur
- Set up interface to Mumbl

============================================================================
23 Mar.
Set up code for Dsolve to take NeuroMesh or CylMesh. Long way from testing,
but it compiles. Also the FastMatrixElim tests are now being performed
in the unit tests. Checkin 5190.

Starting on code in testDiffusion: testCellDiffn
============================================================================
25 Mar. Implemented volume rescaling in ChemCompt: propagates down to child
pools, reacs and enzs.

Tested in snippet, oddly it doesn't work there.

============================================================================
28 Mar. This turned out to be really silly. I had not scanned through base
classes for Pools, Reacs and Enz. So the objects were invisible when zombified.

Now on to diffusion.

Working through setup. There is a problem in how the mesh sets up the
stencil sparse matrix for the diffusion connectivity: It has an empty diagonal.

nC(t) = nC(t+1) - dt*[(D - M)*(n+1)C(t+1)*adx(+) + (D + M)*(n-1)C(t+1)*adx(-) -
 nC(t+1)*D*(adx(+)+adx(-)]

So the terms are:
n-1:    dt*(D+M)*adx(-)
n:   1 -dt*[ D*adx(-) +D*adx(+) + M*adx(+) ]
n+1:    dt*D*adx(+)

Implemented this, now the matrices are made but yet to test.
Checkin 5205.

The matrices cruise happily through the solution, and it isn't exploding,
but very little diffusion seems to be taking place.

============================================================================
29 Mar.
Re-derived the diffusion and motor calculations.

Taking the compartments in order as -, 0, + from proximal to distal, we have

dn0.dt|t+1 =
	D*n-/(h-*a-)*(a0+a-)/(h0+h-) + M+*2*n-/(h0+h-)		// n- terms
	-D*n0/(h0a0)*[(a0+a-)/(h0+h-) + (a0+a+)/(h0+h+)]	// n0 diff terms
	-M+*n0*2*[1/(h0+h+) + 1/(h0+h-)]			// n0 motor
	+D*n+/(h+*a+)*(a0+a+)/(h0+h+) + M-*2*n+/(h0+h+)		// n+ terms

And n0(t) = n0(t+1) - delta_t*dn0/dt|t+1
In this calculation, M+ is motor speed in the + direction, away from soma, and
M- is motor speed toward soma.
a is measured at the middle of each compartment.
h is the length of each compartment.
n-, n0, n+ are the numbers of molecules in the respective compartments.

So the stencil is not enough. Will need a and h terms for all compartments. 
Simplest is to generate vectors of them and discard the whole stencil
business. Will have to rethink how we handle cube mesh, but this will be
OK for NeuroMesh and for CylMesh.

There is a slightly more accurate version for tapering cylinders, which uses
the internal calculation for voxel volume as a section of a cone.

dn0.dt|t+1 =
	D*n-/(V-)*(a0+a-)/(h0+h-) + M+*2*n-/(h0+h-)		// n- terms
	-D*n0/(V0)*[(a0+a-)/(h0+h-) + (a0+a+)/(h0+h+)]	// n0 diff terms
	-M+*n0*2*[1/(h0+h+) + 1/(h0+h-)]			// n0 motor
	+D*n+/(V+)*(a0+a+)/(h0+h+) + M-*2*n+/(h0+h+)		// n+ terms

Since we already have these volumes pre-calculated, we can use them too.


Implemented most of this. Now going through the compilation.
============================================================================
30 Mar.
Testing proceeds, not yet working. Incorporated testCylMesh to try to 
track down the problems in a simpler system.
============================================================================
31 March.

Got diffusion calculations to work for CylMesh, confirmed against 
analytical solution. Only issue now is how accurate to make it.
This has taken a while, not least because my earlier math was off.
Next steps:
* Assign either NeuroMesh or CylMesh to solver, using a SparseMatrix interface.
x Parse Sparse Matrix.
	- Handle regular diffusion with per-molecule diffusion consts
	- Handle motor transport overlay.
* Redid with correct mathematical expressions.
- Fix ordering after Hines reordering. Take cylMesh vectors and shuffle, figure
	out how to reorder back to original voxel indices.
+ Build test cases
	* Simple diffusion
	- Reac-diff with propagating waves
	- Motor transport alone, in either direction.
- Connect to Stoich. Generate Pool arrays.
- Build inter-compartment reactions
- Set up adaptors and other multiscale effects
- Set up RDesigneur
- Set up interface to Mumbl

============================================================================
1 April. 
Thinking about reordering issue. We start with

M.[A] = [a]
  [B]   [b]
  [C]   [c]
  [D]   [d]
  [E]   [e]

Following the reordering with lookupOldRowFromNew = [3,2,1,0,4]
we would have

M'.[D] = [d]
   [C]   [c]
   [B]   [b]
   [A]   [a]
   [E]   [e]

Rather than redo the vectors, I can munge the indices of all the entries in the
opvec. The matrix remains as M' but due to the redoing of the indices in the
opvec, every referece to the vectors now is back to the original ordering.

So what I need to do to the opvec is to apply lookupOldRowFromNew to its 
indices. I'll also need to redo the DiagVal.

============================================================================
2 April.
Working on the reordering. Implemented something, seems reasonable.
A bit more testing shows a problem with the branching neuron diffusion.
Did a simple test on mass conservation, it fails. It gets worse at longer
times, presumably molecules are reaching a junction where there is an error.
The reordering changes this error, but it is wrong either way. I won't
revisit the reordering yet because I don't know where the error is from.

Implementing test for symmetry because I suspect this may be an issue.
Implemented. This symmetry test catches a discrepancy.
Next: Narrow down with a small test cell.

============================================================================
5 April.
Looked at code. On reflection it doesn't seem like we need the matrix to
be symmetric, since we can have diffusion between compartments with different
volumes. But we do need the terms on either side of the compartments to match
to ensure mass conservation.

I need to expand out the test small cell by hand to figure out where the
calc is going wrong.

============================================================================
13 April.
Fixed some indexing issues, to my surprise this fixes the error in mass 
conservation that was causing current unit tests to fail. Still would like to 
have an analytic expression to test for a branching cell. Possibly a 
symmetrically branching cell would behave like an equivalent cylinder.

============================================================================
14 April.
To coordinate Dsolve and Ksolve. There needs to be a 'transfer' function
that if needed uses direct MPI calls to do a bidirectional transfer of
values between them, organized by node and the data resident on each. I think
there is a high-level MPI operation to do this. 
If possible map through the ZombiePoolInterface.
In each case, we construct a vector destined for a specified target node.
Aim to consolidate all entries due to go to the specified node.

During operation we call the transfer with a special clock tick. For example,
we might have
Hsolve process: Tick 0, dt 50 us
Dsolve process: tick 2, dt 10 ms
Dsolve transfer to Ksolve: tick 3, dt 100 ms
Ksolve process: tick 4, dt 100 ms
Ksolve transfer to Dsolve: tick 5, dt 100 ms
Hsolve adaptor to pools, e.g. Ca: Tick 6, dt 100 ms
Spikegen adaptor to pools, e.g. Glu for mGluRs: Tick 6, dt 100 ms
Ksolve adaptor to Hsolve: Tick 7, dt 100 ms

For now, just deal with Ksolve to and from Dsolve.

Set up a couple of functions in ZombiePoolInterface, getBlock and setBlock.
These do just that - get and set blocks of pool #s. Meant to be used by
a handler that knows which pools and voxels on each node, and that deals
with internode transfer as per need.

Checkin 5247.

Next: 
Figure out how Ksolve knows # of voxels. It has a numAllVoxels field.
We already know how Dsolve knows # of pools: gets it from the Stoich.
Set up a toy reac-diff model using stoich to set up reac system.
Figure out how to call the transfer functions. Possibly a ZombiePool Proces?

============================================================================
20 April
Worked on script for diffusion in cylindrical compartment with 25 voxels.
Does everything but doesn't diffuse.
Perhaps the Dsolve isn't properly assigned to the ZombiePool.
Yes, that is the problem. 

============================================================================
23 April. Trying to compile.
============================================================================
24 April. Compiled, just needed a cleanout.

Lots of stepping through the script and gdb later, turns out that I can
get the solver to set up the diffusion calculations but:
* The Zombie solver used by the pools should be set up automatically.
* The assignment Dsolve::setInit for assigning concInit from the
	script fails for poolNum > 0 because convertIdToPoolIndex fails.
	This fails because Dsolve::poolMap_ isn't set anywhere.
* The diffusion constant needs to be transferred into the 
	DiffPoolVec array on Dsolve, but before the pool is zombified.
* The non-diffusing pools need to be identified and their diffusion ops 
	ignored.
* The two solvers need to talk.
============================================================================
26 April.
Try this:
ZombiePool just has a single ZPI.
Ksolve maintains ptr to a ZPI. If set, then all field value ops are forwarded.
	Else Ksolve does its own diffusion. If ZPI is subsequently set then
	the diffusion term gets handed over.
Stoich doesn't know about Dsolve, just talks to the Ksolve.

============================================================================
27 April.
Or:
* Dsolve::setStoich extracts poolMap. Must happen _after_ Stoich::path is set.
* Zombie pools maintain their own diffConst, to be used to set up the Dsolve.
* Dsolve::build should be moved over to setStoich. 
	* Need to get dt somehow.
	* Should avoid losing concInit values.
* Dsolve::reinint needs to reinit values.
Also: Implemented a Dsolve::path field which gives an alternative way to
	set multiple pools to diffuse.

At this point the unit tests clear. I haven't tested the use of the stoich
in setting the pools to diffuse. Nor have I put the new interface into the
snippet.
Also if there is a pool with diffConst = 0, it will fail. 
- Need to put in dummy calculations for diffConst=0, buffering, and sumtots.

Checkin 5283.

Implemented dummy calculations for diffConst=0. I'm not dealing with 
buffering here, in principle we could have a buffered diffusing molecule so
I will let the usual diffConst deal with the diffusibility of buffers.

Tried out the script version. Works with direct path assignment (which
I have done in the unit tests). Doesn't crash but doesn't work with
the stoich assignment. Perhaps need another unit test here.

Some more fixing later. Stoich assignment seems to work and give a
reasonable output, but since it isn't talking to the other solver it
isn't visible from the pool either.

Next step: Dsolve extracts id of poolInterface from Stoich when the
Stoich is being assigned, and the two ZombiePoolInterfaces set up
pointers to each other to transfer blocks of data at the timestep of
the slower solver. In practice, the Ksolver would request the updated
concs, do an interpolation (trapezoidal) to get its start,
do an integration timestep. Then get the delta from its start, add it to
the latest concs, and send those back to the Dsolver.

For now trying the simple addition to the latest diffusion values.
Fails in Dsolve::getBlock around 514.
============================================================================
28 April.
Fixed it, trivial matter of passign the wrong range.
After a bit of schedule juggling it works.
In principle we're now OK to do reac-diff. 
I would prefer to have the pools reporting values from the dsolve by default
as it is usually going to have the faster rates.

For now, let's give it a nice reac-diff problem to chew on, preferably in
the script.

Done, works insofar as mass conservation goes. Added script to snippets,
cylinderDiffusion.py.

Also implemented a 1-D Turing pattern model. Relatively easy to set up.
Not terribly fast, has to run for 200 sec to build up.

From Jan, here are the steps
Week	Date		Steps
1	Jan1-5		Naming policy stuff
2	Jan 5-11	Kinetics brought in. Volume handling revisited.
			Msg fixes.
3-4	Jan 12-25	Kinetics works, mostly. Fixes to Python/MPI stuff.
5-6	Jan 26-Feb7	Standalone fast matrix elimination. Fixing wildcards.
7-8	Feb8-21		Ksolve skeleton in place.		
9-10	Feb22-Mar8	Ksolve works
11	Mar9-14		Steady State solver ported and works.
12	Mar 15-20	GSSA ported and works.
13-17	Mar 21-Apr28	Dsolver implemented, set up ReacDiff with Ksolve.
			Not to mention almost 3 weeks of travel too.

Next:
- Build inter-compartment reactions
- Set up adaptors and other multiscale effects
- Set up RDesigneur
- Set up interface to Mumbl

============================================================================
30 April
Working on motors. After a sign fix things are better, but the molecules
march off the end of the compartments. Should pile up instead.

Still confused about motors.
============================================================================
1 May
Revisited math for setting up motors. Did so. Now it works. Added snippet
cylinderMotor.py.

Checkin 5337.

Some fixes in the problems with SpikeRingBuffer. Latest issue was that
it wasn't scheduled, so it didn't know CurrTime. When fixed I get
spiking but need to see how it matches.

============================================================================
2 May
Working on adaptors. Now compiles and clears unit tests but need to 
incorporate it and figure out what to do with requestField msg - do I
really want this?

============================================================================
3 May.
Working through loadMulti snippet. Ran into silly wildcard bug. Fixed.
Also a silly ZombieFuncPool bug. fixed.
Stuck now with Stoich calculations.
Valgrind may help.

Something seriously wrong with the ksolver. I have implemented stuff
to deal with setting up a reasonable initial dt, but this isn't 
fixing it. It was working earlier. Symptoms are that there is a mismatch
between internal vectors of mols and the visible ones.

Will have to fall back to ee for the demo of multiscale models in 
NeuroHDF

============================================================================
5 May
Working on script in snippets/MULTI/TuringInNeuron.py.
============================================================================
6 May
Even though the numpy plots don't work, I can explicitly scan through the data
points and save the ones I want. Ran a sim embedded in the regular neuronal
geometry, examined in xplot. The Turing
pattern in the first few compts is fine, but then the concs go all over the
place, apparently in some coordination with the segments of the neuron.
============================================================================
7 May
Confirmed inverse relationship between volume and the concs in the output.
Concs are not so bad, they can easily be redone and stored in Sinit.
My intuition is that the rates will also be different in each voxel,
because they are expressed in # units.

Working on Ksolve.cpp, goal is to extract concInit from each pool and use
it to fill in the  concInit in all the array entries
============================================================================
8 May
Some fixes to the documentation.
Further fixes on Ksolve.

Working on building a Turing pattern in neuronal geometry. I thought the
problem was initial conditions. Turns out that the concs were set
explicitly, after the reaction system was distributed. The problem is that
the rates have not been scaled suitably across the voxels.

Options:
- Put in a volscale vector in the VoxelPools, to multiply each rate
	This needs changes to the Stoich::updateRates function.
- Do inline volume scaling to multiply each rate. Only volume is stored in voxel
	This needs changes to the Stoich::updateRates function.
	It will involve numerous repeated powers and multiplications.
	Minimal memory costs.
- Put in independent Stoich ptrs for each volume in model. 
	Call Stoich::updateRatesAfterRemesh for each.
	This needs a Stoich on each Voxel Pool.
		To make it efficient, could share Stoich Ptrs among all with
		same volume. 
	Still very costly.
- Put in independent RateTerms for each volume in model.
	Call Stoich::updateRatesAfterRemesh for each.
	Need a way for Stoich to use specified RateTerms set in updateFuncs.
	Possibly again share ptrs, here the RateTerm vector, among all with
		the same volume.

OK let's try the last. It is not horribly memory-expensive, nor is it going 
	to cost a lot of multiplications each cycle.

Stoich::generateRescaledRates
Stoich::updateRates using rates as an argument.
Stoich::updateReacVeolocities. Seems like it should move into the 
	VoxelPoolsBase.

VoxelPoolsBase::myVol or some other way to map to rates_ vector.
VoxelPoolsBase::rates_ ptr to vector. Would like to have a reference counter
	so I avoid having to worry about management. Either that or the
	master Stoich keeps the original of all of them.
Something to figure out matching volumes. Probably in Ksolve. Possibly
on Stoich as it is asked for a ptr for a new volume.

Will need the equivalent changes in GssaVoxelPools.
Ksolve needs to have the voxel vols assigned to it, to pass to the VoxelPools,
	so that they can ask the Stoich which volIndex they should use.

Massive code changes to do the above. Some things work but
the TuringInNeuron test still doesn't give a nice flat line for
mol concs.
============================================================================
9 May
Sequence of actions on setup of reac-diff solver system.

0. Compartment is set up to a cell (neuroMesh) or volume (other meshes)
1. Dsolve and Ksolve both assign compartment. Needed to figure out vols and
	numVoxels. This doesn't need anything else assigned.
2. stoich.addPoolInterface fills in the pool interface vector. Just 
	assigning the Ids at this point, though it might be nice to
	compute the ptrs since this lets us avoid exposing functions to
	the MOOSE interface.
3. Call Stoich::setPath. This builds the whole system.
	3.1 At an early stage in setPath..resizeArrays it sets numPools on
	each of the PoolInterfaces. Note that they already know the
	# of voxels and vols from 1).
	3.2 at setPath..Stoich::zombify gets numVoxels from the PoolInterface.
	3.3 After Zombification the Stoich builds the list of unique volumes,
		and with it the vector< vector< RateTerm* > >.
	3.4 After this the Stoich calls each of the PoolInterfaces and assigns
		Stoichs unto them. 
		- Ksolve uses this to go through each VoxelPool and set up 
		the volIndex to use for its reac calculations.
		- Dsolve uses this to get the vector of all pools from the
		Stoich, and extract DiffConst and MotorConst.
4. Assign the Dsolve to the Ksolve::dsolve field. This is used during
	the process function to pass data back and forth between the solvers.


To simplify further, 
- I could assign the compartment to the Stoich.
	It would use this to get the volumes right up front. I could then
	use this to fill in the compartment info to the Ksolve and Dsolve.
- I could also use the ZombiePoolInterface ptrs to do step 4, that is,
	assign Dsolve to the Ksolve and vice-versa (as a dummy).

Yet to consider:
	- Dsolves that span multiple Ksolve reac systems
		e.g., Ca diffusion that goes into spines, where spines have
		different reacs.
	- Ksolves that work with different Dsolves
		e.g., the spine Ksolve that needs to talk to the spine
		Dsolve as well as the Ca Dsolve.
	- Ksolve that maps to only a subset of voxels on a Dsolve.
		Same as above.
	- Reactions that go between different Ksolves.
		e.g., Spine to PSD and back again.

Alternatively, and much simpler
	- Intercompt reacs
	- intercompt diffusion processes, implemented same as reacs but 
		set up by examining molecule names between compts.

Decision.
0. Compartment is set up to a cell (neuroMesh) or volume (other meshes)
1. Compartment assigned to Stoich. Here it can assign unique vols already.
2. Dsolve and Ksolve assigned to Stoich using setKsolve and setDsolve.
	2.1 At this point the Stoich::useOneWay_ flag is set if it is a Gsolve.
3. Call Stoich::setPath. All the rest happens internally, done by Stoich:
	3.1 assign compartment to Dsolve and Ksolve.
	3.2 assign numPools and compts to Dsolve and Ksolve.
	3.3 During Zombification, zeroth vector< RateTerm* > is built.
	3.4 afterZombification, stoich builds rateTerm vector for all vols.
	3.5 Stoich assigns itself to Dsolve and Ksolve. 
		- Ksolve sets up volIndex on each VoxelPool
		- Dsolve gets vector of pools, extracts DiffConst and MotorConst
	3.6 Dsolve assigned to Ksolve::dsolve_ field by the Stoich.
4. Reinit, 
	4.1 Dsolve builds matrix, provided dt has changed. It needs dt.
	4.2 Ksolve builds solvers if not done already, assigning initDt

As seen by the user, this reduces to just 4 stages, all centered on the Stoich.

Need some care with defaults to handle the old-fashioned multi-compartment 
but non-diffusive reactions.
Also this approach is terrible for building a reaction system and running it
on the fly.

I want to reverse the order of the uniqueVols.
============================================================================
10 May
Compiling. The Dsolve::setStoich function needs to be sorted in the new format.
If the Dsolve can work without a Stoich it needs a suitable fallback too.

============================================================================
11 May
Finally got the compilation done. Into testKsolve. Problem with CubeMesh
not providing the voxel information needed by the Stoich.

Works for most of the tests, still doesn't work for the Turing pattern
test in the neuronal morphology.

Checkin 5398.

Trying to find why the system hangs. Using zug.py with Kholodenko.g
and a runtime of 5000.
Valgrind says that there is an invalid read of size 8 in the
RateTerm::FirstOrder. Haven't been able to track this. The entire python
side of things seems infested with memory issues.

How to put breakpoints in a python script:

import signal
import os
PID = os.getpid()

def doNothing( *args ):
	pass
signal.signal( signal.SIGUSR1, doNothing )

and for the breakpoint:
os.kill( PID, signal.SIGUSR1 )

Tracked down the numerical integration problem. It turns out that I had
set the absolute tolerance of integration to 1e6. Set it to 1e-4 and all
is well. Also implemented stuff so that the Ksolve can define the
numerical method and the tolerances.

Tracked down the problem with TuringInNeuron calculations. Turned out to
be a whole lot of little problems to do with setting up distinct 
rateTerm vectors for each compartment volume. One of them was to do with the
difficulty of doing a doubleEq comparison for tiny values like the voxel
volumes (in the range 1e-17).
Now it is working, but the algorithm for finding the volIndex isn't going
so well. It is currently a brute force search, but there turn out to be
176 distinct volumes in this model, so an N^2 approach isn't advisable.
Checkin 5400.

Also, even though the answers are coming out well, the python script is
failing at end. Fails even to be able to plot the data using matplotlib.


Trying to run both the elec and reac-diff models together. Stuck for a
long time on plots, turned out that there was actually a scheduler bug
that had laid in wait for all this time. With that fixed I'm getting
reasonable plots. Next step is to connect the models with an adaptor.
============================================================================
12 May. Thinking about how to set off the adaptor to request data from
a number of targets. I have a variant of send, sendVec, but it isn't 
mpi-compatible.
============================================================================
13 May
Updated adaptor to use sendVec. Will deal with MPI-compatibility in due 
course. Checkin 5405.

A little stuck with this. Seems that the vector of voxelVolumes isn't 
being filled by the PSD. Need to check what is up with Spine as well.

============================================================================
Subha wants the vector of ObjIds to help with the value harvesting in 
sendVec.

Trying to load chem model standalone to figure out why the loadMulti sim
isn't working.
Trying with OSC_diff_vols.g as a good test.

The thing has loaded with all parameters set correctly. There is the tiny
correction for the NA value in MOOSE. Let's see... no, that doesn't
fix it. Tried lots of variations on it and it doesn't oscillate.
OK tracked it. The moose.loadModel only loads contents of /kinetics.

Fixed. I added an option to prefix the method with 'old_', in which case
it turns off the flag to move objects onto their volume-defining compartments.
With this fix the model runs.  Checkin 5413.

Converted Element::getNumMsgTargets to getMsgTargets. Put in unit test.
Checkin 5414.

To handle the x-compt reacs, should just put in an exp-Euler calculation
at the interface between solvers. Likewise for diffusion.

Need now to move on the demos.
============================================================================
16 May 2014.
Made tutorials directory, especially for course use.
First demo: Cleaned up and animated the TuringOneDim.py demo. Moved to 
tutorials.
Trying to get oscillatory models in.  To do:
	- The diagram for the cspace relaxation oscillator
	- Get the novak-tyson cell cycle oscillator from kkit to work.
		Loads the model and ee runs ok. So it is something with
		zombie.
	- The repressillator
	- A standard feedback-delay oscillator.: Kholdenko.g


============================================================================
17 May 2014
Many fixes to Hsolver. Seems to be doing something reasonable, see 
testHsolve.py. Checkin 5424.

Now back to looking at the novak_tyson model. This is now loading and
running but the output is incorrect. Checked that the FuncPools that are
set in place to follow a pool conc, are working.

I've set up all the other oscillators:
repressillator, slowFB (KHoldenko), relaxationOsc, Turing

============================================================================
18 May 2014
The chem oscillator tutorials are now in a directory of their own, each with a 
small png to explain what they are about. Would like to bring in the 
monstrous Novak-Tyson example but it isn't running.

============================================================================

Need to figure out what is happening in the stoich.path assignment
leading to only 2 entries.
Turned out I was doing a setCoords on the cylinder, but using CubeMesh
arguments.

Yet again: Look at volume handling.

- Need to have the volume_ field return the total volume of the compartment.
	Allow a setVolume too, but only if the compartment has a single voxel.
	This function rescales all rates and n's.
- Provide a utility function setVolumeNotRates which changes the volume
	field if a single voxel, but does not rescale rates and n's.

When loading models from SBML and kkit, use the setVolumeNotRates as needed.

============================================================================
20 May 2014
Cleanup for volume handling in meshes. Now it is possible to load a model
right onto the target mesh.
With this I have implemented a tutorial for propagating bistables.
Added a whole series of tutorials for such bistables. Still want to put in 
a dose-response curve.

There is an issue with setting rates in reaction-diffusion systems. It only
sets the rate for the largest vol compartment. I know why but it should set
for all.

Attempted to update but there were severe problems with the repository
version. Downgraded back to my version.

Fixed a bug where concInit was not being propagated to all pools when
an array was created.
Working through snippets/MULTI/loadmulti

============================================================================
22 May. 
Numerous fixes in ReadKkit to be able to load models onto NeuroMesh,
SpineMesh and PsdMesh directly, and get volumes right. With this done the
entire multiscale model loads but doesn't run.

Put in a check in Shell::doCreate to block creation of FieldElements
or abstract classes.

This is all based on 5439. There was a huge mess with upgrading so I'm taking
a backup before trying again.

Checkin 5458.
============================================================================
23 May 
Able to upload the new revision, things compile now. Apparently the hsolve
is now working well and clearing the Rallpacks.

Sahil has found a problem with the cspace reader. Trying out in 
ChemicalBistables/foo.py
This turned out just to be a numerical instability, lowering dt fixed it.
============================================================================
24 May.
Trying to get loadMulti to work. The setDiffLength in the NeuroMesh has
to be called before the cell path is set, because only the cell Path
operation sends out the spine and PSD information. Should fix.

There is a nasty crash when the calculations start. Hard to track. Valgrind
doesn't handle it either, it thinks that my code is writing past the end
of the heap block.

Two obvious places to look: The adaptor and the dangling reactions at
the interface between compartments.

Removed all references to adaptor. Still fails. 

Check if file runs as an individual model under a single solver.
Then check if it fails in a similar way when loaded in the default way under
multiple solvers.

============================================================================
30 May
Loaded in psd_merged31d.g both with separate compartments, and with the 
solver handling all compts. In both cases the model loads and runs fine.
Next to assign the solver to a different compt and see what happens.
Doing.
============================================================================
31 May
Well, it handles the spine portion fine as well.
And the psd portion too. So something goes wrong with the compartmentalization.

Went back to LoadMulti. Loaded in a version of the cell model that did
not have any active conductances. Still crashes.
============================================================================
1 June. Need to try from scratch. Make a really simple cell with a 
single soma, spine and PSD. Use a minimal chem model with just Ca binding
to a single target. Couple Ca directly to the Vm.

minimal.py now works for the following:
	- Passive charging of spine and soma
	- Simple reaction in each compt.
	- Each compt has a single voxel, 
	- No cross-compt reactions
	- No active channels.

Next:
	Active channels: minchan.py.
	Multiple voxels:

Doing active channels.
Turns out that the moment I instatiate a vgated channel the system crashes.
This is even when the vgated channel in question is not loaded.
Oddly, if I run it with valgrind it is fine.
Also oddly, I can do an assignment to alpha field of the channel with an
array, and the system does not crash.
Looking at HHGate::setupTables as called by HHGate::setupAlpha

============================================================================
2 June
Checkin 5483.
Now converted the setupAlpha DestFinfo to an ElementValueFinfo. This
fixes the problems with loading in channels in minchan.py, which tests
out a minimal model with ion channels.

The main loadMulti file still fails in the gsl calculations.
Next step is to introduce multiple voxels into the minchan model.
Did this. Just worked.

Next step is to get in some Ca channels
Did this. Just worked.

Then get adaptors doing something.
Seem to be doing something, but the Ca trace is not coming over.
	This was a nasty scheduling issue, where things in a wildcard ##
	seem to be scheduled for each individual entry, but the message from
	the Clock is OneToAll so it gets multiplied.

Checkin 5485.

Next step is to deal with cross-compartment diffusion. For now this only
crops up in NeuroMesh/SpineMesh/PsdMesh, but would like to generalize.

Setting this up in Dsolve.cpp.
See around line 200

Then cross-compartment reactions.

============================================================================
3 June.
Building up the cross-compartment diffusion setup and calculations, yet to 
compile.

Compilation now coming together. I've set up a separate MOOSE function,
buildNeuroMeshJunctions, to do the junction setup.
============================================================================
4 June
Compiled, isn't doing anything yet.
Also realized that I may as well do an implicit solution for the 2-compartment
diffusion problem at the junction. Let A be conc in one compartment, and
B be conc in the other. Then:

A(t+1) - A'(t+1) dt = A(t)
where A'(t+1) = D.( B - A ).xa/L

Later. Checkin 5487.

Implemented a unit test for testing inter-compartment diffusion. Seems
promising. Lots of fixes ensued. The minchan.py model looks better too.

The loadMulti still fails. I'll do a midchan.py derived from 
minchan.py to see where things fall apart.

midchan loads in entire cell model quite happily. I've set up the 
adaptors for Ca on the spines. Try to figure out how to do this for dends.

Then: Put in a more complicated chem model. Different ones in spine and PSD.

============================================================================
30 June. Checked in with the midchan.py.
Trying to put in adaptors on the dend. There is a skeleton code around line 188.
Unable to get the psde_rged
============================================================================
6 July.
Some cleanups to NeuroMesh so that look up of electrical compartments and
corresponding voxels is now possible.

Updates to midchan.py to utilize these lookups, now the dendritic compartments
are also utilizing adaptors for taking elec Ca values and mapping to chem.
Checkin 5619.

Let's visualize Ca in the entire model as a line plot.
Done. Things actually seem reasonable.

Next step is to get the solvers going. For the chem stuff we're already doing
solvers.
Tried Backward Euler implicit solution for elec model. Doesn't look right.
Then put in more complex chemistry and see if it still holds together.


============================================================================
7 July.
Compared with and without solver. Basic HH stuff is working. When I take
the deterministic dt down to 0.1 usec it starts to look like the output of
the solver. But the Ca output isn't coming through, even though the 
Adaptors seem to be doing all right.
============================================================================
8 July.
Cleaned up CaConc to have a base class saved with the Zombie.
Seems to work OK.
Compared multi1.py with multi1_ee.py. dt = (10e-6, 0.2e-6) respectively.
The hsolve version is more accurate as judged by convergence. The spike
timings of the ee version are off. But the shape of all graphs is good.
Checkin 5629.

Next: Load in a more complex chem model. We could try the one from the 
2011 multiscale paper. Or the variant already here in the MULTI directory.

Then I really need to get the cross-compartment reactions working again.
Would be nice to implement Crank-Nicolson in HSolve. Also the core
	matrix calculations are much more complicated than my version for
	reac-diff.


============================================================================
10 July.
The psd_merged31d.g fails with a segv. Try separate_compts.g which has
similar complexity but the cross-compartment reactions are removed.

This works, and each compt is doing something interesting and different.
Only Ca in the dends so the run doesn't take all that long.
============================================================================
11 July.
Key step now is to get inter-compartmental reactions going.
Stoich.cpp does this function 'locateOffSolverReacs'
which builds up a list of offSolverReacs_
and another of offSolverReacCompts_.
The latter is a one-to-one matching vector of pair< Id, Id >, each entry is
either a compartment for that reac, or Id().

We need to traverse this and locate 2- and 3-way junctions, set up a map
with the junctions. When building the model tree we need to find every interface
between compartments, look up the map, and set up the reaction.

That is easy. offSolverReacMap_. Now problem is how to build from this
and then how to solve it.
One option is to make zombies that are a variant of the regular reacs,
but have ptrs to the appropriate location in the solver. Problem is that 
we have singleton Reacs but multiple junctions where they would be used. So
we need to go back to using data structures within the solver.

Options for solving:
- Use proxy molecules. On each timestep, get the latest value of the proxies,
	do some internal calculation, then send back proxy values.
	So the comms with the remote solver would be a vector of doubles of
	the proxy values.
	- Messaging scheduling has to be like this:
		tick 0 ownerSolver and sideSolver sends latest to each other.
		tick 1 ownerSolver: 
			Does process.
			Uses reac and sideSolver concs to compute new conc
			Updates its own mol concs as modified by reac.
		tick 1 sideSolver
			pastes new conc into its concs.
			Does process

	
In the Ksolve::reinit, we need to tell each of the voxelPools which 
	compartments/voxels it touches. This lets the voxelPool figure out 
	which of the crossReacs to deploy.
In an earlier incarnation, we had distinct GSL computations for each combination
of cross reacs. These filled out the proxy molecules. In principle I could
make duplicates of the Stoich that did this. May be marginally more accurate
and faster.
I need to ask the Mesh which compartments/voxels each voxel touches.
NeuroMesh keeps a vector of parent voxels of each spine.

============================================================================
13 July.
Some ops to implement in the solver and elsewhere:
	- Scheduling: Have a default clock for each class. 
		*Have 20 ticks.
		*Calling remains via message.
		- Upon creation each object is scheduled.
		- Add a 'clock' field to Element as a short. -1 means not sched.
		- Remove the Shell::doUseClock stuff, instead use this field.
		- Scheduling is immediate upon setting this field.
		- Never permit multiple scheduling.
		- When zombified, object adopts default clock of zombie class.
	- Ksolver: 
		- Stoich::addElist and dropElist functions for adding and 
			removing objects from ksolve management.
		- Func to generate Stoich with specified set of compts
		- Never incorporate dangling reacs or pools into numerics.

	- Option 1 for distinct cross-compartment reacs on different voxels.
		- Stoich should have an internal data structure which fully
			specifies calculations per voxel:
			N_; rates_; funcs_; offSolverPoolIndex_[compt]; 
		- Stoich currently maintains rates_[volIndex][rateIndex].
			Should be changed to a 
	- Option 2 for distinct cross-compartment reacs on different voxels.
		- Stoich sets up exactly 2 internal data structures
			- N_ for just the bare voxels
			- N2_ for the voxels that connect to other reac systems.
			- Rates have to be set for each volume as currently.
				I'd need to do some further duplication.
			- Voxel-specific stuff is handled in the VoxelPools.
	- Option 3 for distinct cross-compartment reacs on different voxels.
		- Stoich maintains the minimal set of data structures required
			- N_ for different x-compt reaction cases
			- rates_ for different volumes and x-compt cases.
			- Info about mapping of different pool Ids to index.
			These are the authoritative data structures and the
			memory management is the job of the Stoich.
		- VoxelPools maintains per-voxel info
			- for chem calculations: ptrs to N_ and rates_
			- For data transfer: Arrays for which pools to manage.
			- Accumulates outgoing data into vector for the Ksolve
		- Ksolve interfaces with the rest of the world.
			* farms out the calculations to the pools_ vector of
				VoxelPools.
			* Accumulates data transfer info, transfers it.
			* Disperses incoming data transfer info to pools_.
		
	Clearly, Option2 isn't going to help much. Option 3 is perhaps a little
		cleaner in that it moves some of the per-voxel management to
		the voxels where it belongs.

Need a way to propagate changes in concInit of proxy pools to all affected
Stoichs.

Implemented skeleton code for Ksolve interfacing to each other.
Intermediate checkin with the updated Clock.cpp and the skeleton code for
Ksolves to send data for cross-compartment reactions.
Checkin 5644.
Next: fill up the xfer_ vectors in each VoxelPool that determines which pools
	transfer pool 'n' to each other. To do this I need the list of 
	compartment juxtapositions. There is a ChemCompt::matchMeshEntries
	function but it is patchily implemented.

Another problem: Suppose we have a cross-compartment reaction in which
multiple voxels are involved. I really need to send over the dx of each 
voxel, rather than the 

I should be able to set up the ordering without further information because
each of the ksolves has full information about the molecules and the
voxelization. The order of molecules should also be unambiguous based on
their Ids.

Jn# is given by the sequence obtained from matchMeshEntries.
# of mols is always fixed because the same mols will be present in all voxels of
	any compartment.
So order in transfer sequence is to be the array of [jn#][mol#]

On a given stoich we only know local molecules. Need to pick up the 
other stoich. For now use external function do to this.

Use cases:
Suppose I have a single extracellular compt that connects to several spines.
I have transporters on each spine. Just to make it nasty, have them as MM.
Transmitter_ext --- transporter_spineMemb ---> transmitter_spine

ext will send out just one value, but to several spineMemb voxels.
ext will not have any proxies.
ext will receive many values indicating decrement, one from each spineMemb.
spineMemb will send out many values to the single transmitter_ext.
spineMemb will send out many values, one-to-one for the transmitter_spine
spineMemb: each spine will will have a proxy for transmitter_ext and another
	for transmitter_spine
spineMemb will receive a single value from ext, which will be used by each voxel
spineMemb will receive many values from spine, which are mapped one-to-one to
	each voxel.

Now that I have an implementation, two problems crop up. 
1. We need to replicate this entire structure for each paired compartment. 
2. We need to send each message to one specific ksolve only.

For now:
*Pull it all together - compile
	Successfully compiled, still need to provide an interface to the 
	setup function, and ideally have a way to call it automatically.
	Also the setup function needs a way to set up the xfer messages too.

Set up the clock/scheduling stuff.

=============================================================================
Jul 15 2014.
Implemented DestFinfo Stoich::buildXreacs. Yet to test what it does.
Crosses function but fails when trying to run. Unclear whether it is OK
after it crosses function.
=============================================================================
Jul 16 2014.
Fixing up the calculations to deal with junction and non-junction voxels.
1* The N_ matrix is always the same, and includes on and off compt reactions.
2. The RateTerms vector differs, both for different volumes, and also for
	cases where there are additional cross-compartment reactions. However
	it is always the same length with the same entries in each location so
	that the Stoich::convertIdToReacIndex works uniformly. Where there
	are off-compartment reactions not being used, they are replaced with
	blanks.
	In Stoich::buildAllRateTermVectors, we need to also provide a lookup
	of which rate set is to be included.
	So we need a rate set for each possible combination of voxel junctions.
2a. Several functions on the lines of Stoich::setReacKf need to be updated,
	as there are multiple rates vectors, typically one per voxel.
3. the 'v' vector of reaction velocities is always sized to match the N_ matrix.
	That is, it includes on and off compt reactions. Off are always zero
	if they are not activated.
4. Each VoxelPool maintains both local and proxy pools. Proxy pools are 
	invisible to the user, any access functions must refer to the parent.
	But somehow the Sinit must propagate.
	Again, we keep things uniform for simple access.
5. We might be able to optimize calculations a little bit if each voxel pool
	knows if it is bare or decorated (with cross-compartment reactions).
	If bare then the updateRates need only iterate for the local RateTerms
	and the subsequent loop for updating yprime need only go over the 
	local numVarPools. If it is decorated then it needs to do the entire
	set regardless of which subsets of x-reacs it deals with.
6* The Dsolve does NOT handle proxy pools. It does diffusion for the core
	set of pools and it is up to the x-reac updates to propagate the 
	diffused values to other ksolves.
7. The OdeSystem should either use on+off pools, or just the on pools.
	See (5). This is currently happening in the ksolve.

=============================================================================

17 July 

Moose key things to do before release and paper:
+Ksolve cross compartment
Synapses
KKit GUI
Clocks
*Identifying message source.
Documentation
	Tutorials
		- How to load and run a neuronal model (using NEUROML)
		- How to load and run a chemical kinetic model (using
			kkit or SBML formats)
		- How to load and run a reaction-diffusion model
			kkit + dsolve
		- How to load and run a multiscale model.
	GUI documentation internal.

Packaging:
	.deb
	Fedora rpm
	mac?
	Virtual box, small as possible.

=============================================================================
18 July.
in Ksolve::setupCrossSolverReacs, we need to address points 2,3,5,7.

Fixing up some incompleteness in earlier implementation. I have two options:
1. Build a transfer vector directly from the VoxelJunction vector built by 
	ChemCompt::matchMeshEntries. This is unambiguous and has the same
	sequence and size in both compartments, but it will result in multiple
	transfers of the same data if any voxel in 'self' compt touches 
	multiple voxels in 'other' compt.
2. Populate each VoxelPool with a list of voxels to which it must send data.
	Here we must also give each VoxelPool a list of transfer indices
	from which it should receive data, in the transfer vector.
	Suppose the VoxelJunction vector was
		A	B
		4	1
		4	2
		4	3
		10	10
		11	11
		12	12
		13	12
		14	12
		15	11
	for a really odd connection.
	Then the compts represented in the transfer vectors look like this:
		A->B	4, 10, 11, 12, 13, 14, 15
		tr. idx	0, 1,  2,  3,  4,  5,  6
		transfer index on B (voxel, transfer indices)
		(1, [0]),(2, [0]), (3, [0]), (10, [1]), (11,[2,6]), (12,[3,4,5])

		B->A	1, 2, 3, 10, 11, 12
		tr. idx	0, 1, 2, 3,  4,  5
		transfer index on A (voxel, transfer indices)
		(4, [0,1,2]), (10, [3]), (11,[4]), (12,[5]), 
			(13,[5]),(14,[5]),(15,[4])
	This is also unambiguous but requires an extra level of housekeeping
	to build each direction of indices. This option does not have multiple 
	transfers. But it may involve going through lots of VoxelPools to find
	the few which do transfers - again easily bypassed with a further
	table of connected voxels.
- Independent of the above decision, the implementation also needs to
	be extended to handle multiple adjoining compartments.
OK, will do option 2 because I feel it is cleaner for a voxel to know what
it is doing, rather than have it done to it.

Implementation with this in progress. Need to put the functions in the headers
then compile. Should test as standalone. Need to put in the multiple compts.

=============================================================================
19 July
Found ugly bug: the Stoich::getPoolIdMap is a terrible function, 
see error in Dsolve::setStoich.

Lots more ugly errors, most to do with sizing the calculations to include
the proxy pools. Finally it clears multi3.py. Also stops segv on exit.

Now finally to go on to checking the cross-compt reacs.

Lots of cleanup now needed. For starters, setting up to use the 
indices to look up the xStoichs in the VoxelPools, rather than the Ids.
Idea is that the sending Ksolve will go through all connected Stoichs in
the xStoich array, and build up all the values to go out. The VoxelPools
will put together the entries in the values array.

=============================================================================
20 July
Created a small class to handle the info need for each pairwise transfer
between compartments.

Now it compiles and runs peacefully, but doesn't exercise the new code.
Somewhere I had a cross-compartment reaction sytem.
OSC_diff_vols.g will do it.
=============================================================================
21 July
Looks like it is coming together, it runs without crashing but there is
a numerical issue with the values rising steeply.

I need to revisit what is sent, what is in lastValues, and so on.

Suppose we have A0, A1, A2 as the respective proxy pools, and the
correct summated conc is A.

A[t+1] ~= A[t] + A'[t+1]	// Ideally would like to evaluate A' at t+1.


A'[t+1] ~= (A0[t+1] + A1[t+1] + A2[t+1] ) - 3A[t]

Or
A[t+1] ~= (A0[t+1] + A1[t+1] + A2[t+1] ) - 2A[t]

On compt 0, we have,
S[t+1] += A1[t+1] + A2[t+1] - 2A[t]
A[t+1] = S[t+1]

Each compt independently evalutes A[t+1] in this manner.

Staging:
initProc
	Extract values from S.
	Transfer values. 
	Handle and buffer incoming. Can't update S directly because it would
		affect the values to be extracted from objects processed later.
process
	Update S vectors using buffered values.
	Update lastValues
	Transfer data in from dsolve.
	Compute A0[t+1]
	Transfer data out to dsolve.

Need some further cleanup to ensure that the order of voxels matches in the
two transfer vectors.

It isn't handling things. 
+ Check that the xreac is being built
	* Seems so from the code
	* Tested from examination of what reacs have been converted to zombies
		Look for endo and exo. Yes, converted.
* Check that the reinit values are set up properly
+ Check that pool quantitites are being transferred back and forth
	* Assign just before a single step, see where it goes.
- Check that the proxy pool values do get updated at both ends.
	No, it doesn't. Oddly, only one of the proxies is not being updated.
		* Check that the respective Stoichs have correct # of reacs.
			They do.

Works! The one proxy was a clue. Turns out that the proxies are being 
assigned the
last pool entry, possibly colliding with the buffered or sumtotal pools.
Only one of the compartments had a sumtotal, it was the only asymmetry in the
model. Note also that in both cases the proxies have been situated at the
last entries in the pool vector. When I deleted the sumtotal in the script 
the oscillations worked.

Next: fix the indexing of the proxy vs other pool entries.
Checkin 5671. 

Then: Design a more challenging multicompartment/multivoxel system.
As a step to this, run multi3.py. This turned up numerous bugs, cleaned up.
Not clear yet if the multicompartment calculations are actually taking place,
but it loads and runs.  Checkin 5672. 

=============================================================================
24 July 2014
Multicompartment calculations: For starters just do the simplest possible:

a <===> b <===> c

This has easy analytic time-course solutions for non-spatial compartments, and 
easy analytic steady-state solutions for spatial compts.

Implemented it, unfortunately numerics are off. Now to track down.
Aha, asymmetry in the outcome depending on whether I do
stoich0.buildXreacs( stoich1 )
	or
stoich1.buildXreacs( stoich0 )
That suggests a hole in the implementation, which was to have been totally
symmetric.

Now it looks symmetric. But the thing settles to a smaller value than it
should. To 110/111 to be precise.


=============================================================================
27 July.
One of the proxy pools is initialized to zero. That would deplete the
concs. I need to have a way to set nInit even for proxies.

Compt	aS(t0)	aSinit	bS(t0)	bSinit	cS(t0)	cSinit
0	6e6	6e8	
1	0	0	6e7	6e7	6e6	0
2					0	6e6

Some changes:
1. On reinit the local Sinit should always override the incoming values for
	the starting computations.
2. The local Sinit should perhaps propagate to the Sinit for the other compts,
	use this value instead of relying on other stuff sent around.

Tracked down one major problem: I don't identify the source compartment, so I'm
filling in the wrong values.
This is something listed in the 17 July wishlist.

Implemented. Several more fixes to indexing of data being transferred for
cross-voxel reactions. Will need to generate a much nastier stress case.
For now it works at least to this point. The crossComptOscillator also
works.

Final conc = 2.0
Final vol = 111e-17
Final amount = 222e-17
Initial amount from a and c = 101e-17
So new amount from b should be 121e-17, which comes to a conc of 12.1
This works. Updated crossComptSimpleReac.py to do it.

Next to devise a crossComptCplxReac.py which has many voxels that 
communicate. Preferably something analytical.
A model with dend, spines and psd seems the best bet. I'll set diffusion
const to sero, so each has its own domain. Then different init conditions
to check that the voxels are communicating in the correct order.
=============================================================================
28 July.
Subha has clarified the use case for the item on 17 July
Consider an object that sends a 'get' request to multiple targets. It needs
two things
	- To have each return value come into a different location
	- To have an unambiguous map between the values and the sources.
Note that the SendTo function also would benefit from a message variant that
gave an unambiguous mapping and ordering.

=============================================================================
29 July
Options for the 'get' request
1. Identical to current API, except that the GetOpFunc::Op() does
	*ret++ = returnOp( e ) 
rather than 
	*ret = returnOp( e )
This requires that the calling function know exactly how many values it is
to pick up, that it allocate the buffer for them, and that it know exactly what
order the return values will take. It is fast and fragile, needs no API change.

2. Replace current API with 
	ret.push_back( returnOp( e ) )
where ret is now a vector< A >&. 
This requires that the calling function knows the order of the return values.
It is safe and reasonably fast but needs an API change. Also some care with the
ordering, specially across nodes.

3. Replace current API with
	ret.push_back( pair< A, ObjId >( returnOp( e ), e.objId() ) )
where ret is a vector< pair< A, ObjId >&
This does not requre any info on the calling func, nor any care with ordering.
API changes. May need lookup/sorting of the returned values to map to ObjIds.
Somewhat extra overhead.

Trying to implement. Compiler err
=============================================================================
30 July
Implemented Number 2. Works. Did unit test.
Viktor has done some benchmarks and it looks like the diffusion solver is
more than twice as fast as the Hsolve passive code. Just for fun I should do
a gprof to check. Doesn't generate the gmon.out. Later

=============================================================================
31 July
Built crossComptNeuroMesh.py to test the simple reaction in a number of
dendritic spines. Somewhat unplanned, the soma was bigger than the
dendrite compartments. This unearthed a hugely difficult bug.
Tracked it down to the following: cross-compartment reactions are special
in that their scaling depends on both (or all three) compartments. I need
to make and rescale rates for all combinations of vols on all three. What
is worse, first order reaction rates (which are independent of volume 
for intra-compartment reacs) do depend on relative volumes. 

- Expand uniqueVols to include vols of other compts as a pair.
- expand copyWithVolScaling to include this othervol term.

-Perhaps I should have a areVolsUniform() function in ChemCompt. Even with this
I need to check if there are any x-reacs to compts for which this is false.

This strengthens the case for just having a complete new rates vector_ for
every voxel in a NeuroMesh or a tapering CylMesh.
In a CubeMesh this comes up when we have to deal with cross-compartment
reactions, unless we require that all relevant reacs are located on the 
NeuroMesh.

Confirmed by getting all vols to be uniform, upon which the model worked
including with different concInits.
This is in crossComptNeuroMesh.py.

Next step is to fix up the rates vector so it does the right thing when
the volumes do differ.

Fixed bug with Cinfo::srcFinfoName and Cinfo::destFinfoName, to traverse
base classes if needed.
=============================================================================
01 Aug 2014
At 5729. I need to brace myself to take on the messy rewrite for the rateTerm
vector, which seems to have to be one per voxel.

Multiple considerations if I'm going to change things deeply;
- Regular rate assignments need to traverse all voxels
	Easy fix.
- To what volume is the rates_[0] assigned? The volume of the 'reference'
	voxel may itself have changed.
	If I use a preconverted rates_ vector as reference for all rates, 
	then it should be a separate rates array referred to unit volume.
- Need to be able to dynamically add and remove reacs and pools.
	Doable. Need a rebuild function is all, one which can digest a mix of
	preexisting zombies and fresh objects. 
- Need to be able to resize entire neuron or subparts of it.
	If I use the reference rates_ array, then it is just going over each
	voxel scaling the local rates to the reference ones.
- Need a flag on ChemCompt to specify if it has variable vols or a single one.
	Easlily done.


Option: use a special rates_ array
	- Very easy to go through duplicating and scaling.
	- Nasty to have two references for rates
	- 
Other option: build new rates from the original reac and enz objects.
	- Single point of reference.
	- Slower
	- May be very nasty if objects are on different nodes. Better to
	have local info.

=============================================================================
02 Aug.
- Put a master rates_ vector on Stoich. Unit SI units for volume
- Put the subsidiary rates_vectors in their individual VoxelPools.
- Use a separate sub-class of VoxelPools for handling shared rates_ vectors,
	vs the ones handling individual ones.
- Put in a message from compartment to stoich for updates. Possibly even
	recompartmentalizationisticisms.
- Stoich deals with changes to rates, and propagates.
- Need to attach info to scaled rates about vol scaling for all the x-compts. 
	Perhaps a wrapper around the rate terms? But that would slow 
	calculations.
	- Stoich knows which compts belong to which rateTerms. I can just
	refer to this.

=============================================================================
03 Aug
Steps in changeover for RateTerm vectors.
*1. Change data structs and get to compile
*2. Get it to work for one voxel.
	*2a. Get it to work for one voxel deterministic
	*2b. Get it to work for one voxel stochastic
	*2c. Get to clear unit tests.
	*2d. Get it to work for one voxel steady state finder.
*3. Get it to work for spatial
4. Get it to work for x-compt
5. Get volume changes in mesh to propagate into rates and diffusion.

=============================================================================
04 Aug
Stuff fails in steadyState finder
- The VoxelPools struct in the finder doesn't have its rates_ initialized.
- The S_ vector is dubious, with a size of only 1.

=============================================================================
05 Aug. 
Steady-state finder seems to be OK now, but one of the demos from Sahil 
isn't working.  Checkin 5735.

Some progress. But I need to do somewhat more intelligent scaling, for the
off-compt substrates and products. Will need to separate out.
For substrates, no scaling. For products, only
the product terms scale.

=============================================================================
06 Aug.

numkf = Kf * Va.NA/Multiply(Vsub.NA)
numkb = Kb * Va.NA/Multiply(Vprd.NA)
numKm = Km / Multiply(Vsub.NA) // The Va.NA cancels out in num and denom

In the code I need to separate the Multiply for substrates and products.
I also need to track which of the rate parameters are for Kf and which for Kb.

Which terms does the advance() routine call? Perhaps if there are no
connected things the local rates_ index should be truncated., or at least
	dummies put in to return zero.
Also the VoxelPools::updateRates should limit the number of matrix rows it
	computes, according to whether it handles proxies.

To fill this, just go through all reacs and find the volumes of the specified
pool.

What happens when there is a volume change?
In current form we need to rebuild the entire volscaling system. 
If so, I can use ratios in the xReacScaleSubstrates_ etc vectors, 
folding in the local voxel volme

If we know the power that each ReacTerm will take (which we do), then we
only need the ratio of Vreac to Va. This lets us replace the sub and prd
arguments with one, for the cases where there is no cross reaction.
So we store volXcompt/volLocal

numkf = Kf * Va.NA/Multiply(Vsub.NA)
numkf = Kf * Va*NA/(Vsub1*Vsub2 * NA*NA)
	= Kf * Va/Vsub1 * Va/Vsub2 * 1/(NA * Va)
	= Kf / ( ratio1 * ratio2 * NA * VA )
(Vsub1/Va * Vsub2/Va)


=============================================================================
07 Aug.
Nearly there, try to compile
=============================================================================
08 Aug
Compiles but fails with the crossComptNeuroMesh.py example. This
brings up a possible conceptual problem.


A1	B1
A2	B1
A3	B1
A4	B2
A5	B2
A6	B2


A <====> B
Suppose we have the above geometry. Reaction-wise we need to do


A1 <====> B1
A2 <====> B1
A3 <====> B1

This is possible to manage, but only if the reaction is situated in the compt
with smaller voxels, ie, in A. 

In the general case we could have 
A1	B1
A2	B1
A2	B2
A3	B2
A3	B3

and so on. This would not work with the smaller voxel approach.

For now we stick with the small-voxel rule.

- Next problem is that the current code has assumed a one-to-one map
of the junction voxels on either side, this is of course incorrect.
We need to query the VoxelJunction list for which voxels to use for
the volume.
-Check if the other code has disabled the extra reac calculations in the voxels
where there are no junctions.
Apparently not. I'm not hanlding this very well. Should just have vol of
parent voxel in there.

Tried this, didn't work.

=============================================================================
11 Aug 2014. 
Tracked part of the problem to zero sub rate.

Tracked further to Ksolve.cpp:998, where matchMeshEntries returns a nonzero
size yet clearly an empty junction as vols are both zero.

Seems like the CubeMesh simply does not put any of the relevant volumes into
the VoxelJunction when it builds it. In fact none of the mesh classes seem to.
Probably should replace Ksolve::1008.

Also remove the volume fields if they are not being used.
Turns out they are used in Dsolve. In that case I will assign the volumes
in the function that builds the VoxelJunctions.

With that done, the system runs crossComptSimpleReac, but incorrectly. The
level in compt B remains ten times that in compt A, even though compts
B and C equilibrate.

One further test: If I put reac0 on compartment 0, then the reac system
settles correctly to 2.0.

If I put reac0 on compartment1 and reac1 on compartment2 (under the principle
that the reacs should be on the smaller compartment of a junction) then
the reac system fails totally. There are reactions but each of the final
concs is 10x the previous one, in inverse proportion to their volumes.
To try: Check actual assigned numerical rates. ZombieReac::vGetNumKf and Kb
cheats, it does the calculations locally without asking the solver.

=============================================================================
12 Aug 2014.
Tracked it down. It was because the flipRet function was flipping voxel numbers
but not the voxel volumes. 
Tested with crossComptSimpleReac.py.  
	Confirmed that I get the same output regardless of which compartment
	I choose for building of the xreacs.
	Turns out I get different rates if I put the reacs in different
	compartments. The steady state is the same though. There is a
	numerical blowup if the reacs are on the biggest and smallest compt,
	and nothing at all if they are on compts which do not participate in
	the reactions.

I ran this for the other crossCompt cases: crossComptOscillator.py was fine
and crossComptNeuroMesh was nearly so, except that the dangling soma compt
looked like it was in exponential decline. This would be consistent with
there being an unreciprocated proxy on that voxel.
	
I was going to check this in but the old MAPK model doesn't seem to run
properly. May be a general problem with legacy x-compt reac systems.

I need to fix the following:
* Several of the GSSA bistable snippets don't flip state the way they used to
* Cspace models are not loaded properly
* Check that the multivol gsl models do the right thing.
- Fix the dangling proxy problem.
* Re-examine vol change calculations.
	Both ChemCompt and Stoich have independent functions doing the same
	thing: scanning through all reacs and enz and assigning rates again.
	This will not do well because the volume terms in VoxelPools don't 
	change.
	* Update the volume terms in each voxelPool. 
	* Update the vol terms for xReacScaleSubstrates and products.
	* Also update nInit in each voxel.
	* Eliminate the rescan and just use the reference rates_ vector
	* Connect up the chemCompt to the Stoich so that the request can
		propagate down. Do this at setup time.
		* Ensure that the affected off-compt reacs also get updated.

I've done the connecting up stuff for notification of volume changes. 
	Unfortunately the Gssa vols stuff has stopped working.
	I think I've fixed it. Unfortunately now the hard-coded unit tests
	fail because I had to comment out lines in ChemMesh.cpp which
	do the non-solver volume scaling.

=============================================================================
13 Aug.
Did a simple test for presence of voxelVol messages to decide whether to
do scan through all child objects, or to scale by using the messaging. This
works. Clears unit tests and most of the snippets I've tested. The problem
with the unconnected voxel in crossComptNeuroMesh.cpp remains and I'll look
at it next. 
Checkin 5740.

From 17 July 

Moose key things to do before release and paper:
*Ksolve cross compartment
+Synapses
KKit GUI
*Clocks
*Identifying message source.
+Documentation
	Tutorials
		- How to load and run a neuronal model (using NEUROML)
		- How to load and run a chemical kinetic model (using
			kkit or SBML formats)
		- How to load and run a reaction-diffusion model
			kkit + dsolve
		- How to load and run a multiscale model.
	GUI documentation internal.

Packaging:
	.deb
	Fedora rpm
	mac?
	Virtual box, small as possible.


Preparing to change synapse. As preparation, did some benchmarks on an 
intFire network. Turns out that the benchmark options in MOOSE were broken.
So were the unit test options - the thing was running unit tests every time.
Fixed. Ran the intFire with the ring buffer synapse, takes 24.5 seconds
to do 200K steps.


Designing synapse handling. Many problems with previous.
- Synapse must be a virtual base class, since we have to deal with a range of
	synapses with different learning rules.
- SynHandler<T> manages a vector< T > of some class derived from Synapse. It
	also has a priority_queue< of pair< synIndex, arrivalTime > >
	- It deals with addSpike( ptr, time ) which places the 
		index of the syn with new spike, and the time of
		generation + delay, into the priority queue.
	- It has a pure virtual function Synapse* popBuffe

- I need to be able to independently specify the synapse type and the
	parent compartment. Something like the SynChans do.
 	This will let me put arbitrary synapses on an intFire.

- I need to specify the interface for checking/updating the Synapse.
	- Pass in presyn Vm
	- pass in time since last AP??
	- Pop it if ready, go back for more.

=============================================================================
14 Aug
Orthogonal classes:
	1. Synapses: individual weight, delay, learning variables.
	2. SynHandlers: Manage synapses, manage queueing of input events,
		collective syn params for learning,
	3. Electrophys: Channels, IntFire neurons

Points of possible orthogonality:
	- Synapses vs SynHandlers: different kinds of queuing for the same 
		synapses. This is probably a minor and specialized case.
	- SynHandlers vs Electrophys: map many kinds of synaptic properties, 
		including different learning rules, to different SynChans with
		one or two exponentials, or intFires, or izzy neurons.
OK, so we will only have 2 orthogonal classes: the SynHandlers and the
electrophys classes.
API for the classes: electrophys classes need to know an Activation term
as a sporadic event, from the SynHandler. This is typically an analog impulse 
function for a single synaptic event, but could be a continuous variable if
the synapse experiences continuous input, like a dendro-dendritic synapse.
SynHandlers may optionally need to know any number of terms from the 
electrophys side, ranging from events for action potential timings to 
continuous Ca or Vm.
This unfortunately requires that the SynHandler receive a process message.

Implemented it. There are some modest differences with absolute values from
the IntFire unit test. Other than those, the code seems to run fine but
about 50% slower according to the benchmarks. This is not surprising as there
is another whole set of objects process'ed and another layer of messages.
Not to mention the queueing synapses. Need to work out how to profile it.

Also fixed a bug reported by Dilawar about a function called by the benchmarks
that is hidden by the debug flag.

Checkin 5743.

Fixed a bug in Gssa calculations, due to pickReac very rarely going out of
range of existing reactions.

Fixed dangling files which were problems for other people to compile.

Working on IntFire snippet. Somewhat stuck. Trying to get it to keep
firing by having the inputLayer of spikeGens keep ticking away.
=============================================================================
15 Aug
Fixed another compile oversight.
Still stuck with IntFire snippet, but I've exhumed an old unit test for
SynChans and that has cleaned up a bit of code.

Spent a long time working on the snippet trying to get it to behave like the
MOOSE unit test. Finally realized that I was running the snippet five times 
longer than the unit test and the activity had died down.

I can't expect identical activity because MOOSE uses a different random
number than Numpy, and I'm using Numpy to set the weights etc.

It would be nice for this and other situations to have an object to do 
simple spiking stats. Counting/averaging/ISI/rate/fano factor and so on.
Checkin 5751

I should put this in, and plot it, and then fiddle with parameters here and
in the unit tests so as to get sustained reasonable rate firing in the model.

Trying to compile the new Stats.cpp.

Done, clears unit tests, checked  in.

Now working on SpikeStats, I have a snippet trying to test it out
in recurrentIntFire.py.
=============================================================================
17 Aug
Implemented SpikeStats and RandSpike and a snippet, RandSpikeStats.py
to test them all. Quite satisfactory.
Also updated recurrentIntFire.py to achieve sustained activity and plot it.

Considering rescheduling, that is, setting up clock ticks automatically
at creation. Not an urgent matter.

Considering an optimization for messaging: have the opfuncs operate directly
on pointers to the data rather than on Erefs, which have to look the
data ptr up each time. Unfortunately many functions do rely on Erefs for
message sending, for zombie lookups and for accessing Element fields. 
What I could do instead is to expand the MsgDigest::targets_ vector to
include ptr to object. 90% of time-critical calls should be possible to 
do without further lookups.


Check if Clock sets its internal dt to the smallest dt that is actually used,
or to any small dt.


Cleaning up Demos directory
	- Need a README in each subdirectory
	- Need to clean up traub2005 and Izhikevich
	- Decide if symcomp belongs here at all
	- Does hopfield do anything?
	- Check that the files in the neuroml directory are OK.
	- Eliminate util.

Cleaning up Snippets directory
	- Go through updating and checking each one
	- Find a way to run all as a regression test, without display.
	- Have an example for every class and every function in MOOSE.
        - Put in an index so that one can find which snippet illustrates 
		which class. Or provide a listing of all class names linked
		to the appropriate file. 

Misc
	Have a sensible sorting for moose.showfields.
	Check with Subha about which of the documentation options is actually
	working at present. Should we do all our docs in markdown?

Reorganizing directory structure.
	moose
		/README
		/Copyright
		/src
			/basecode
			/benchmarks # this is for C++ benchmarks.
			/builtins
			...
		/Documentation
			/user
			/developer
			...
		/Demos
			/squid
			/tutorials
			...
		/gui
			...
		/benchmarks	# This is for MOOSE-wide benchmarks scripts.
			/Rallpacks
			...
	

Set up code for default ticks and their assignment during creation.
Lots of additional infrastructure code too.
I should assign default dts for these ticks, after making sure that
empty dts dont get invoked in the clock loop.
Also need to fix # of ticks.
	
Yet to compile.
=============================================================================
18 Aug 2014.

* Set up default dts.
* Fixed # of ticks. Not totally automatic, but mostly so.
* Check invocation of empty dts and correct spec of base dt.

I now have it working, need to retrofit some unit tests.
- Along the way found and fixed a bug with deleting child elements - it was
not getting all of them. But this needs to be revisited using a low-level
Element call because it is very inefficient.
- Need to write a snippet to show equivalent ways of scheduling something.
* Need to check zombification and automatic clearing/rebuilding of ticks.
	Clearing doesn't happen
- Would like some more intelligence in how to assign the Table tick for plots.
- Need to redo ReadKkit and ReadCspace to put objects appropriately, and to
	fit with the new scheduling.

=============================================================================
19 Aug
A documentation framework. The Use of Moose.

1. What is MOOSE and what is it good for?

2. Loading and running models
	2.1 Hello, MOOSE: Load, run and display existing models.
	2.2 Adding graphs
	2.2 Start, stop, and speed: Timing and scheduling
	2.3 Accessing and tweaking parameters
	2. Storing simulation output.

3. Chemical signaling models
	3.1 Loading, modifying, saving
	3.2 Running with different numerical methods
	3.3 Changing volumes
	3.4 Feeding tabulated input to a model
	3.5 Finding steady states
	3.6 Making a dose-response curve
	3.7 Building a chemical model from parts.
	3.8 Oscillation models
	3.8 Bistability models

4. Reaction-diffusion models
	4.1 In a cylinder
	4.2 In a neuron
	4.3 Reaction+diffusion+motor transport
	4.4 A Turing model
	4.5 A spatial bistable model.

5. Single cell models
	5.1 Loading, modifying, saving
	5.2 Explicit vs. implicit methods
	5.3 Integrate-and-fire models
	5.4 The HH model
	5.5 Analyzing spike trains

6. Network models
	6.1 Connecting two cells together
	6.2 Regular and plastic synapses
	6.3 Providing random input to a cell
	6.4 A recurrent integrate-and-fire network
	6.5 An integrate-and-fire network with plasticity
	6.6 A feed-forward network with random input
	6.7 Using compartmental models in networks.

7. Multiscale models
	7.1 A multiscale HH model with chemical feedback
	7.2 A multiscale compartmental model with chemical feedback

8. Graphics
	8.1 Displaying time-series plots 
	8.2 Animation of values along an axis.
	8.3 Using MOOGLI widgets to display a neuron




Looks like problem in unzombification is that upon stoich deletion the
objects dont get unzombified. Which is because of ordering issues.
If I remember, the Elements know if they are slated for destruction.

=============================================================================
21 Aug.
	Proceeding with documentation. Put in general chapter outline for
the MOOSE cookbook.

Stuff to fix still with solver and zombification scheduling.
=============================================================================
23 Aug.
I think that the solver delete is now safe. 
Now to figure out the best way to build on an existing model for merging and
for editing. Deleting solver is the obvious way.

After quite a bit of debugging, I have it working for solver clearing
and rebuilding. The test script is switchKineticSolvers.py

Next:
- Make a few good cross-reac and multiscale model examples.


For later on: There are some options for removing the fieldId in handling
synapses.

Basic requirement: 
- Messages to synapses needs to identify individual synapses.
- Want to have a single message to all equiv synapses in a network.

Current approach manages this by
- FieldElement index provides the identity of the synapse on each msg.
- All FieldElements share the same channel parent.

Alternate approaches
Wrapper class + regular child elements on each synchan. Message goes to
	common wrapper class. Wrapper class figures out mapping from its 
	internal field indexing to that of the actual synapses on each synchan.

Messages store extra index info + regular child elements on each synchan.
	The messages go to the synchan, not to the synapse, but identify it
	through their extra index.


=============================================================================
24 Aug
	Trying to find why the system crashes with multi2.g. May need to run
valgrind.

=============================================================================
25 Aug
Fixed problem with indexing Func pools in Dsolve. Hack to ignore.
Now problem with Ksolve nan. Possibly due to cross-compt reacs on non-
connected compts. 

Tracked it down. It was a silly oversight in the RateTerm calculations for
volume scaling. Came up for the first in this model because I haven't used
N-order reactions in the other tests.

Now both multi2 and multi3 run and give reasonable outputs.
Would like to compare multi2 with and without hsolve, but it isn't doing the
right thing.

Continuing documentation updates too.

=============================================================================
26 Aug.
Worked on why the multi1 and multi2 models don't work in EE mode. Seems that
there is an error in message setup for channels in ReadCell.cpp.

Testing hhcomp.py
I get good HH spiking with the original scheduling func that Subha has.
I don't get channel responses with the new scheduling.

Is it just that the functions don't get called, or is there something
nasty happening with the messaging?

Another clue: the channel process is called twice. Looks like some fairly
serious message confusion is happening. Possibly only to do with shared msgs.

27 Aug
I think I have it: the library channels are scheduled (should not be) but
the ones on the compartment are not (should be). Need to look at the copy
function to ensure that sensible things happen

=============================================================================
27 Aug
I think I have it: the library channels are scheduled (should not be) but
the ones on the compartment are not (should be). Need to look at the copy
function to ensure that sensible things happen.

Fixed. Works. The ee method at 0.2us slowly is converging to the hsolve@10usec,
still not there. Key thing is that the method works.

=============================================================================
28 Aug.
Try to understand why crossComptNeuroMesh.py has a slowly decaying conc
in first compt. Implemented foo.py based on this. will try:
- shfiting compts so they no longer touch eath other, see what happens. Do the
structures for junctions still remain and is there reaction stuff
across.

Did this. Fixed a minor bug with CubeMesh. Now I'm able to run with
CubeMesh compartments, a<===>b<===>c is the reac. The problem occurs even 
in this simple case. 

Is there a way a voxel can find out if it a given x-reaction does not occur
on it? There is already the hasXfer function, but this is for the voxel as a
whole.

Compiles, runs, but 
At this point it looks like all of the reacs have been turned off.

=============================================================================
29 Aug.
Looks like the proxyPoolsMap is never getting assigned.

=============================================================================
30 Aug.
Some progress on it, seems to be OK now but I need to design proper
unit tests.
=============================================================================
31 Aug.
Lots of documentation and code snippets. Implemented a rather nice one for
dose-response of a bistable.

=============================================================================

1 Sep. Working on demo neuroDiffusion.py.
=============================================================================
2 Sep. Still working on it, seems to be close.
=============================================================================
3/4 Sep. Now it looks OK. One issue to fix is that if I set concInit in the
original pool, it should persist through voxelization and zombification.

=============================================================================
7 Sep.
I need to do a gdb to find out why the Stoich::zombifyModel call (when
the model path is set), fails to iterate through all voxels assigning concInit.

The reac-diff models now work but the simple single compt models all seem
to start at initConc = 0.

Fixed this - the functions setting nInit were not setting PoolBase::initConc_.

Checking 5799.

Trying to fix the problem Nikon has with the cspace tests. I found and
fixed one error but it is still possible to hang the steady-state finder.
=============================================================================
8 Sep.
Tracked the problem with Nikon's model. Turns out that the routine for
generating starting points consistent with the stoichiometry rules was
also generating cases with negative concentration. Fixed. Now Nikon's 
models, even the original one with 26 pools, works.

Did lots of cleanup on the cylinderDiffusion snippet, with a nice animation.
I want to make another snippet to check that the system does sensible things
in a branching neuronal model, and also to illustrate bidirectional motor
transport. Need to do this anyway for my own benefit.

From FastMatrixElim.cpp:400, the motor equations are:
* n-1:    dt*(D+M)*adx(-)
* n:   1 -dt*[ D*adx(-) +D*adx(+) + M*adx(+) ]
* n+1:    dt*D*adx(+)

=============================================================================
9 Sep.
Making a display of a neuron to watch motor transport on it.
This reveals errors.
Further tests show that the errors happen only when there is branching.
It is OK with a tapering geometry without branches.

Tracked it (I think). Line 592 in FastMatrixElim.cpp shows that we are adding
all the mols to every twig. Need instead to get the total XA downstream,
and then weight the amount going to each twig by its XA.

Fixed.
This works very accurately for motor transport from soma toward dends.
There is a numerical error though going the other way:

mass consv =  86000366.962 86784536.0959
It is not a dt problem, checked with much smaller diffdt.
It is not that the terminal voxels have some bits left over. Checked with a
printout of final concs.

Fixed it: it was a problem with indexing.

I'll update the transport demo to show the diffusions of a and b 
in opposite directions, simultaneously. Done.

Subha has reported a bug in Clock.cpp, causing assertion errors when 
dt is not a clean multiple of the baseDt. Confirmed. Fixed.
Fixed a problem arising from this fix.

=============================================================================
11 Sep
Trying to build a decent snippet for multiscale models. I have one doing
a rather boring thing, will need to modify. The exercise turns out to
be nastier than expected. The Ca-dependent channels don't seem to do much.

Ran into puzzling errors on the HSolve. Turns out that the ZombieHHChannel
isn't done properly, and this is bound to cause issues if there is messaging
to the channel. Need to fix.

Managed to implement a decent single-compt multiscale model, but the
HHChannel issues will need to be fixed soon.

=============================================================================
12 Sep

Implemented a ChanCommon class to sit between ChanBase (virtual base class)
and most of the other channel classes. This is the first step in having 
a systematic channel inheritance framework to include the Zombie classes.

Something is wrong about SynChans. They don't support synapses!
Turns out the documentation was incorrect. SynChans need input from
SynHandlers. Fixed docs.

Tested out a couple of snippets. mgblock wasn't working. Did quite a bit
of fixing on it and now it works. Uncovered and fixed a bug in the copying 
of synapses.

Ground through the refactoring, now the HSolver is working again with
the refactored ZombieHHChannels. The crash with the ordering of plots
is gone, but it turns out that we still need to put in the Adaptors before
the Hsolve otherwise they don't receive input.

Planning final snippet multiscale model. Will do something very similar
to chanPhosphByCaMKII.g, but placed in spine/psd mostly and here the
chan will be AMPAR and the phospho will put it in the PSD. Just have Ca
in dends and it will diffuse into the spines.
=============================================================================
13 sep
getting close, working currently on adaptor areount 
multiscaleManyCompt.py:178
=============================================================================
14 Sep
Ran into a bug with the big multiscaleManyCompt.py snippet. 
It looked like the cross-compt reaction from the spine to the PSD was not
happening.
Actually the data for the cross-compt reaction was going across, but was 
promptly overwritten by data coming in for diffusion. There shouldn't have
been any pools connected by diffusion.

After a long investigation of the logic of the dsolve, turns out everything was
fine except that I should have had the Dsolve input come in before the 
kinetics input. I will need to check out the math for this but functionally
it seems to be OK.

The multi demo seems to be taking shape.
=============================================================================
19 Sep
Fixed GSSA bug reported by Sahil.
Reordered updates for X-compt reacs in Ksolve.

To do:
- Handle functions by solvers
- Test out the differential func object.

=============================================================================
25 Sep. Looking at functions. First, examined the FuncPool. It doesn't
really do anything that the BufPool cannot, so I'll eliminate FuncPool.
- Will first need to confirm that I can use the BufPool in its place.
- Then test the use of Subha's Func or Function object for doing sumtotals
	among other things..
=============================================================================
26 Sep 2014

Minor fixes checked in, 5864.
=============================================================================
27 Sep
Added in IntFireBase class and LIF class. Added snippet in 
Demos/snippets/recurreintLIF.py. This is almost identical to 
recurrentIntFire.py.

Checkin as rev 5874.

=============================================================================
29 Sep.
Planning for arbitrary functions in ksolver. This means that the RateTerms
must know how to use it.
Use cases:
	1. Arb funcs transforming mol concs to another mol conc.
		This would include sumtotals.
	2. Arb funcs altering reaction rates.
	3. Arb funcs directly specifying differential equations.
		This is used, for example, in the model by Tim O'Leary.

Discussed with Harsha, seems all of the above are used in SBML models.
Implementation.
1: Make a RateTerm that holds a Func, and directly does conc assignment.
2. Make a RateTerm implementing a reaction incorporating the Func. This
	would have as inputs both the reactants and also the terms whose
	values contribute to the rate scaling.
3. Similar to 2, except that we have a single reactant whose value is 
	governed by the RateTerm. Guarantees positive concs.
3a. Variant on 3, which permits negative values for variables.

This need not go into the current release.

The current implementation of FuncTerm handles 1, but the math part isn't
implemented yet.

* Will need a base class to encapsulate the func stuff. Subha has some rather
	elaborate code to do this.
- Will need to provide specialized version of RateTerm where the function can
	be set, for use cases 2 and 3.
- For Use case 1, the existing MathFunc framework will suffice.
	- Put a flag in there for using time as x0.
	- I would like to use Func or Function for this but both have
	problems. Func has only 3 args. Function requires msgs to the
	FieldElements. Could be handled in the gui, but not friendly otherwise.

The muparser docs have an example which I've copied here. It should be much
easier to base the code off this.
Compile with 

g++ muparser_example.cpp external/muparser/_muparser.o -Iexternal/muparser

Questions:
- What happens if I define extra (unused) args? No problem.
- What happens if I haven't defined a variable used in the expression? Bails.

Implemented FuncCore class. This can either fit inside a RateTerm, or
be the basis for a FuncTerm. Won't bother with the subdivisions into 
MathFuncTerm any more.

Separately, Aditya reports a compile problem. This is fixed.

=============================================================================
1 Oct 2014.

Cloned Table class so that one variant automagically uses the slower clock for
plotting chemical kinetics.
Updated Clock to handle this.

Went through and fixed many snippets.

Problem with scaleVolumes.py. Issue is at the time I swap concs.

Harsha:
	Need a way to access runtime from kkit reader.
	Deleting messages. Go to Neutral::destroy and call Msg::deleteMsg
	if it is a Msg ObjId.
=============================================================================
2. Oct. 
Fixed problem with non-integral poll Ns. This cures scaleVolumes.py. 
Unfortunately the issue with the stargazin model in bidirectionalPlasticity.py
remains. Defer.

=============================================================================
3 Oct
Massive rewrite in progress to remove FuncPool and the old FuncTerm stuff,
replacing it with code based on the muParser. Not pretty but will vastly
expand the kinds of models supported, and flexibility.

Now it compiles and clears unit tests, but fails to work in loadKineticModel.py

Now that mostly works but the sumtotal isn't computing its value at reinit.
Otherwise seems good.

=============================================================================
4 Oct 2014
Added in fix for stochastic calculations that use functions (sumtotals).
This fixes a problem with the bidirectionalPlasticity model.

Valgrind seems very unhappy about the MOOSE unit tests, will tackle next.

Cleaned up most of the mess with memory and other things. A couple of
small leaks remain.

Possibly should expand ~Clock. Check if the Clock id at is the root 
id of clock, and then delete the new-created objs.

=============================================================================
5 Oct 2014.
Further grinding through leaks. Now Valgrind is happy. Checkin 5901.

Trying to fix the problem Harsha reported with deleting messages. This
was far more involved than expected, including code in MooseModule etc which
was using Ids rather than ObjIds for deleting. I think it is OK but I'll
consult Subha.

=============================================================================
